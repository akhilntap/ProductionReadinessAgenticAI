{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the llm proxy\n",
    "# import os\n",
    "# # pem_path = \"/opt/homebrew/etc/openssl@3/certs/../cert.pem\"\n",
    "# pem_path = \"/opt/homebrew/etc/openssl@3/cert.pem\"\n",
    "# # Ensure the environment variables are set before making the API call\n",
    "# os.environ['REQUESTS_CA_BUNDLE'] = pem_path\n",
    "# os.environ['SSL_CERT_FILE'] = pem_path\n",
    "\n",
    "# # Proceed with the API call\n",
    "# import getpass\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url = os.getenv('proxyllmendpoint'),\n",
    "#     api_key  = os.getenv('proxyllmuserkey'), \n",
    "# )\n",
    "# completion =  client.chat.completions.create(\n",
    "#                     model    = \"gpt-4o-mini\",\n",
    "#                     messages = [{ \"role\"   : \"user\",\n",
    "#                                   \"content\": \"Write a function that prints n primes in python\"}],\n",
    "#                     user     = getpass.getuser() # DO NOT HARDCODE A USER HERE\n",
    "#                 )\n",
    "\n",
    "# import os\n",
    "# import getpass\n",
    "# pem_path = \"/opt/homebrew/etc/openssl@3/certs/../cert.pem\"\n",
    "# # Ensure the environment variables are set before making the API call\n",
    "# os.environ['REQUESTS_CA_BUNDLE'] = pem_path\n",
    "# # os.environ['SSL_CERT_FILE']      = pem_path\n",
    "\n",
    "# # Proceed with the API call\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url = os.getenv('proxyllmendpoint'),\n",
    "#     api_key  = os.getenv('proxyllmappkey'), # DO NOT HARDCODE YOUR KEY\n",
    "# )\n",
    "# completion =  client.chat.completions.create(\n",
    "#                     model    = \"gpt-4o-mini\",\n",
    "#                     messages = [{ \"role\"   : \"user\",\n",
    "#                                   \"content\": \"Write a function that prints n primes in python\"}],\n",
    "#                     # user     = \"prragenticworkflow\" # DO NOT HARDCODE A USER HERE\n",
    "#                     user = getpass.getuser()\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhilred/Library/Python/3.13/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import getpass # to get the current user\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain.chains import ConversationChain\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from typing import Optional, TypedDict, Annotated\n",
    "import operator\n",
    "from semantic_router.utils.function_call import FunctionSchema\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining file read tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_python_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"Error: File not found.\"\n",
    "    \n",
    "class codepath(BaseModel):\n",
    "  path: str = Field(description=\"code path to execute\")\n",
    "\n",
    "@tool(args_schema = codepath)\n",
    "def execute_query(path: str) -> str:\n",
    "  \"\"\"Returns the result of code path execution\"\"\"\n",
    "  return read_python_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# # Function to fetch repository contents from GitHub\n",
    "# def get_repo_contents(repo_owner, repo_name, access_token):\n",
    "#     \"\"\"\n",
    "#     Fetch the contents of a repository from GitHub.\n",
    "\n",
    "#     :param repo_owner: Owner of the repository\n",
    "#     :param repo_name: Name of the repository\n",
    "#     :param access_token: Personal access token for GitHub\n",
    "#     :return: Repository contents as a list of files and directories\n",
    "#     \"\"\"\n",
    "#     url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents\"\n",
    "\n",
    "#     headers = {\n",
    "#         \"Authorization\": f\"token {access_token}\",\n",
    "#         \"Accept\": \"application/vnd.github.v3+json\"\n",
    "#     }\n",
    "#     response = requests.get(url, headers=headers)\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         return response.json()\n",
    "#     else:\n",
    "#         return f\"Error: Unable to fetch repository contents (Status Code: {response.status_code})\"\n",
    "\n",
    "# # Example usage\n",
    "# repo_owner = \"akhilntap\"\n",
    "# repo_name = \"ProductionReadinessAgenticAI\"\n",
    "# access_token = os.environ.get(\"githubpat\")\n",
    "# repo_contents = get_repo_contents(repo_owner, repo_name, access_token)\n",
    "# print(repo_contents)\n",
    "# repo_table = pd.DataFrame(repo_contents)\n",
    "# repo_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_github_file_content(repo_contents, file_name):\n",
    "#     \"\"\"\n",
    "#     Fetch the content of a specific file from the GitHub repository.\n",
    "\n",
    "#     :param repo_contents: List of repository contents\n",
    "#     :param file_name: Name of the file to fetch\n",
    "#     :return: File content as a string or an error message\n",
    "#     \"\"\"\n",
    "#     # Find the file in the repository contents\n",
    "#     file_info = next((item for item in repo_contents if item['name'] == file_name), None)\n",
    "    \n",
    "#     if not file_info:\n",
    "#         return f\"Error: File '{file_name}' not found in the repository.\"\n",
    "    \n",
    "#     if not file_info.get('download_url'):\n",
    "#         return f\"Error: File '{file_name}' does not have a downloadable URL.\"\n",
    "    \n",
    "#     # Fetch the file content\n",
    "#     response = requests.get(file_info['download_url'])\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         return response.text\n",
    "#     else:\n",
    "#         return f\"Error: Unable to fetch file content (Status Code: {response.status_code})\"\n",
    "\n",
    "# # Example usage\n",
    "# file_name = \"40poc.ipynb\" \n",
    "# file_content = fetch_github_file_content(repo_contents, file_name)\n",
    "# print(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#when you write code for code diff pull based on pull request, modify this function with good way of representing parameters\n",
    "def fetch_file_from_github(file_name, repo_owner=\"akhilntap\", repo_name=\"ProductionReadinessAgenticAI\", access_token=os.environ.get(\"githubpat\")):\n",
    "    \"\"\"\n",
    "    Fetch the content of a specific file from a GitHub repository.\n",
    "\n",
    "    :param repo_owner: Owner of the repository\n",
    "    :param repo_name: Name of the repository\n",
    "    :param access_token: Personal access token for GitHub\n",
    "    :param file_name: Name of the file to fetch\n",
    "    :return: File content as a string or an error message\n",
    "    \"\"\"\n",
    "    # Fetch repository contents\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {access_token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error: Unable to fetch repository contents (Status Code: {response.status_code})\"\n",
    "    \n",
    "    repo_contents = response.json()\n",
    "\n",
    "    # Find the file in the repository contents\n",
    "    file_info = next((item for item in repo_contents if item['name'] == file_name), None)\n",
    "    \n",
    "    if not file_info:\n",
    "        return f\"Error: File '{file_name}' not found in the repository.\"\n",
    "    \n",
    "    if not file_info.get('download_url'):\n",
    "        return f\"Error: File '{file_name}' does not have a downloadable URL.\"\n",
    "    \n",
    "    # Fetch the file content\n",
    "    file_response = requests.get(file_info['download_url'])\n",
    "    \n",
    "    if file_response.status_code == 200:\n",
    "        return file_response.text\n",
    "    else:\n",
    "        return f\"Error: Unable to fetch file content (Status Code: {file_response.status_code})\"\n",
    "    \n",
    "class githubfile(BaseModel):\n",
    "  path: str = Field(description=\"github file to fetch\")\n",
    "\n",
    "@tool(args_schema = githubfile)\n",
    "def execute_githubpull(file_name: str) -> str:\n",
    "  \"\"\"Pull the code contents from github\"\"\"\n",
    "  return fetch_file_from_github(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "   messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'read_python_file',\n",
       "  'description': 'None',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'file_path': {'description': None, 'type': 'object'},\n",
       "    'description': None},\n",
       "   'required': []}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the function calling schema for ollama\n",
    "execute_query_schema = FunctionSchema(read_python_file).to_ollama()\n",
    "# execute_query_schema[\"function\"][\"parameters\"][\"properties\"][\"description\"] = None\n",
    "execute_query_schema[\"function\"][\"parameters\"][\"properties\"][\"description\"] = None\n",
    "execute_query_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 18:04:35 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST https://llm-proxy-api.ai.openeng.netapp.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-17 18:04:38 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST https://llm-proxy-api.ai.openeng.netapp.com/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "class codeAgent:\n",
    "  # initialising the object\n",
    "  def __init__(self, model, tools, system_prompt = \"\"):\n",
    "    self.system_prompt = system_prompt\n",
    "\n",
    "    # initialising graph with a state \n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    # adding nodes \n",
    "    graph.add_node(\"llm\", self.call_llm)\n",
    "    graph.add_node(\"function\", self.execute_function)\n",
    "    graph.add_conditional_edges(\n",
    "      \"llm\",\n",
    "      self.exists_function_calling,\n",
    "      {True: \"function\", False: END}\n",
    "    )\n",
    "    graph.add_edge(\"function\", \"llm\")\n",
    "\n",
    "    # setting starting point\n",
    "    graph.set_entry_point(\"llm\")\n",
    "\n",
    "    self.graph = graph.compile()\n",
    "    self.tools = {t.name: t for t in tools}\n",
    "    self.model = model.bind_tools(tools)\n",
    "\n",
    "  def call_llm(self, state: AgentState):\n",
    "    messages = state['messages']\n",
    "    # adding system prompt if it's defined\n",
    "    if self.system_prompt:\n",
    "        messages = [SystemMessage(content=self.system_prompt)] + messages\n",
    "\n",
    "    # calling LLM\n",
    "    message = self.model.invoke(messages)\n",
    "\n",
    "    return {'messages': [message]}\n",
    "  \n",
    "  def execute_function(self, state: AgentState):\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    results = []\n",
    "    for tool in tool_calls:\n",
    "      # checking whether tool name is correct\n",
    "      if not tool['name'] in self.tools:\n",
    "        # returning error to the agent \n",
    "        result = \"Error: There's no such tool, please, try again\" \n",
    "      else:\n",
    "        # getting result from the tool\n",
    "        result = self.tools[tool['name']].invoke(tool['args'])\n",
    "\n",
    "      results.append(\n",
    "        ToolMessage(\n",
    "          tool_call_id=tool['id'], \n",
    "          name=tool['name'], \n",
    "          content=str(result)\n",
    "        )\n",
    "    )\n",
    "    return {'messages': results}\n",
    "  \n",
    "  def exists_function_calling(self, state: AgentState):\n",
    "    result = state['messages'][-1]\n",
    "    return len(result.tool_calls) > 0\n",
    "\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "# model = ChatOllama(model=\"llama3.2:1b\")\n",
    "# # model = ChatOllama(model=\"codellama:latest\")\n",
    "\n",
    "# system prompt\n",
    "# prompt = '''You are a senior expert in reviewing python code. \n",
    "# So, you can help the team to review the code and provide feedback. \n",
    "# You are very accurate and take into account all the nuances in code.\n",
    "# Your goal is to provide the detailed documentation for any security issues in the code that will help users.'''\n",
    "\n",
    "# prompt = '''You are a senior expert in reviewing code for observability. The best one that exists.\n",
    "# Your goal is to analyze the code on the following questions \n",
    "# 1. Are there actionable alerts identified for the feature? Are there Runbooks for the actionable alerts? Do we have TSGs attached to the alert?\n",
    "# 2. Add metrics to monitor dependencies and exception handling on components, infrastructure and features so that SRE can create alerts to reduce TTD?\n",
    "# 3. Are there CorrelationIDs established in logs to derive error lineage across various components?\n",
    "# 4. Can the feature/service support changing log levels for troubleshooting purposes?\n",
    "# 5. Are there critical log lines that we need to get alerted upon?\n",
    "# Provide response in the format as follows: {question: response}\n",
    "# '''\n",
    "\n",
    "prompt = '''You are a senior expert in reviewing code for resiliency. The best one that exists.\n",
    "Your goal is to analyze the code on the following questions \n",
    "1. Can the service/feature sustain a single node/pod failure?\n",
    "2. Can the feature/service recover gracefully without requiring a restart?\n",
    "3. Are there conditions that will require a human intervention to start the pods? if so, what are they?\n",
    "4. What will be the customer impact(if any) due to lack of high availability?\n",
    "Provide response in the format as follows: {question: response}\n",
    "'''\n",
    "\n",
    "#  getting the required ssl certificates\n",
    "pem_path = \"/opt/homebrew/etc/openssl@3/cert.pem\"\n",
    "# Ensure the environment variables are set before making the API call\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = pem_path\n",
    "os.environ['SSL_CERT_FILE'] = pem_path\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model_name      = \"gpt-4o-mini\",\n",
    "                 openai_api_base = os.getenv('proxyllmendpoint'),\n",
    "                 openai_api_key  = os.getenv('proxyllmuserkey'),\n",
    "                 model_kwargs    = {'user': getpass.getuser() })\n",
    "\n",
    "doc_agent = codeAgent(model, [execute_query], system_prompt=prompt)\n",
    "\n",
    "# messages = [HumanMessage(content=\"the code is in the path '/Users/akhilred/Desktop/Billing Pyton Script/Billing_Usage_Extraction.py'. Analyze the code for resiliency requirements mentioned in the prompt\")]\n",
    "messages = [HumanMessage(content=\"Analyze the code with name 40poc.ipynb from github for resiliency requirements mentioned in the prompt\")]\n",
    "result = doc_agent.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the analysis of the code from `40poc.ipynb`, here are the responses to the resiliency requirements:\n",
      "\n",
      "1. **Can the service/feature sustain a single node/pod failure?** \n",
      "   - **Response:** The code does not contain any mechanisms for handling failures or redistributing tasks in the event of a single node or pod failure. In a production environment, this could lead to downtime or degraded service, indicating a lack of built-in resilience.\n",
      "\n",
      "2. **Can the feature/service recover gracefully without requiring a restart?**\n",
      "   - **Response:** The code lacks error-handling provisions that would allow it to recover gracefully after encountering errors. Currently, it returns errors without any logic to retry or handle them, which means it requires either human intervention or a restart to recover from failures.\n",
      "\n",
      "3. **Are there conditions that will require human intervention to start the pods? if so, what are they?**\n",
      "   - **Response:** Conditions that might require human intervention include uncaught exceptions, inability to read the specified file, or issues related to the Azure AI service calls. Given that there are no auto-recovery mechanisms or logging for alerts, it would necessitate human review to diagnose and restart the services.\n",
      "\n",
      "4. **What will be the customer impact (if any) due to lack of high availability?**\n",
      "   - **Response:** The lack of high availability and resiliency features could lead to significant customer impact, including downtime and loss of service functionality. Users would experience delays or inability to access services, which could damage their trust in the system and lead to loss of revenue or customers.\n",
      "\n",
      "In summary, the code demonstrates limited resiliency capabilities and would benefit significantly from improvements in error handling, monitoring, and high availability mechanisms to allow for graceful recovery and continued operation in the face of failures.\n"
     ]
    }
   ],
   "source": [
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
