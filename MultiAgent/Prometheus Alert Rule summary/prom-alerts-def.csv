Name,Location,Description,record,expression,labels,alert,enabled,severity,for,actions,resolveConfiguration,annotations,translation
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance:node_num_cpu:sum,"count without (cpu, mode) (  node_cpu_seconds_total{job=""node"",mode=""idle""})",,,,,,,,,"Count the total number of nodes that are reporting CPU idle time, ignoring the differences in CPU core and mode."
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance:node_cpu_utilisation:rate5m,"1 - avg without (cpu) (  sum without (mode) (rate(node_cpu_seconds_total{job=""node"", mode=~""idle|iowait|steal""}[5m])))",,,,,,,,,"The average CPU utilization across all CPUs for each instance over the last 5 minutes, calculated as the percentage of time spent not in idle, iowait, or steal modes."
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance:node_load1_per_cpu:ratio,"(  node_load1{job=""node""}/  instance:node_num_cpu:sum{job=""node""})",,,,,,,,,"""The average load on a single CPU for each instance, calculated by dividing the 1-minute load average by the total number of CPUs."""
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance:node_memory_utilisation:ratio,"1 - (  (    node_memory_MemAvailable_bytes{job=""node""}    or    (      node_memory_Buffers_bytes{job=""node""}      +      node_memory_Cached_bytes{job=""node""}      +      node_memory_MemFree_bytes{job=""node""}      +      node_memory_Slab_bytes{job=""node""}    )  )/  node_memory_MemTotal_bytes{job=""node""})",,,,,,,,,"This rule calculates the ratio of used memory to total memory for a node, taking into account available memory and memory used by buffers, cache, free memory, and slab. It subtracts the sum of available memory (or the sum of buffers, cached, free, and slab memory if available memory is not reported) from the total memory, then divides by the total memory to get the utilization ratio. The result is 1 minus this ratio, which represents the proportion of memory that is actually used by the node."
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance:node_vmstat_pgmajfault:rate5m,"rate(node_vmstat_pgmajfault{job=""node""}[5m])",,,,,,,,,"""The rate of major page faults per second for a node, averaged over the last 5 minutes."""
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance_device:node_disk_io_time_seconds:rate5m,"rate(node_disk_io_time_seconds_total{job=""node"", device!=""""}[5m])",,,,,,,,,"""The average rate of disk I/O time per second for each node over the last 5 minutes, excluding devices with no name."""
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance_device:node_disk_io_time_weighted_seconds:rate5m,"rate(node_disk_io_time_weighted_seconds_total{job=""node"", device!=""""}[5m])",,,,,,,,,"""The average rate of disk I/O time per second for each node over the last 5 minutes, excluding devices with no name."""
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance:node_network_receive_bytes_excluding_lo:rate5m,"sum without (device) (  rate(node_network_receive_bytes_total{job=""node"", device!=""lo""}[5m]))",,,,,,,,,"""The total amount of data received by each node on all non-loopback network devices, averaged over a 5-minute period."""
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance:node_network_transmit_bytes_excluding_lo:rate5m,"sum without (device) (  rate(node_network_transmit_bytes_total{job=""node"", device!=""lo""}[5m]))",,,,,,,,,"The average rate of bytes transmitted by each network device (excluding the loopback interface) over the last 5 minutes, summed across all devices on a node."
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance:node_network_receive_drop_excluding_lo:rate5m,"sum without (device) (  rate(node_network_receive_drop_total{job=""node"", device!=""lo""}[5m]))",,,,,,,,,"""The average rate of packets dropped while receiving network traffic, excluding the loopback interface, over the last 5 minutes, summed across all devices on a node."""
NodeRecordingRulesRuleGroup-eastus-stage-arc,eastus,Node Recording Rules RuleGroup - 0.1,instance:node_network_transmit_drop_excluding_lo:rate5m,"sum without (device) (  rate(node_network_transmit_drop_total{job=""node"", device!=""lo""}[5m]))",,,,,,,,,"""The average rate of packets dropped while transmitting data from all network devices (excluding the loopback interface) over the last 5 minutes, summed across all instances."""
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate,"sum by (cluster, namespace, pod, container) (  irate(container_cpu_usage_seconds_total{job=""cadvisor"", image!=""""}[5m])) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (  1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"Show the top container CPU usage for each cluster, namespace, and pod, calculated over the last 5 minutes, grouped by cluster, namespace, pod, and container, and filtered to only include pods with a known node, considering only the top result per pod."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,node_namespace_pod_container:container_memory_working_set_bytes,"container_memory_working_set_bytes{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container memory usage for each namespace and pod, considering only containers with a specified image, and return the maximum value for each combination of namespace, pod, and node, limited to the top result."""
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,node_namespace_pod_container:container_memory_rss,"container_memory_rss{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container with the highest memory usage (RSS) for each namespace and pod, considering only containers from the 'cadvisor' job with a non-empty image name, and group the results by namespace and pod, limited to 1 result per group."""
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,node_namespace_pod_container:container_memory_cache,"container_memory_cache{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container using the most memory cache for each namespace and pod, considering only containers with a non-empty image name, and filter the results to only include pods that are running on a node with available information."""
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,node_namespace_pod_container:container_memory_swap,"container_memory_swap{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container using swap memory for each namespace and pod, considering only containers with a non-empty image name, and filter the results to only include pods that are running on a node with available information."""
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,cluster:namespace:pod_memory:active:kube_pod_container_resource_requests,"kube_pod_container_resource_requests{resource=""memory"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) (  (kube_pod_status_phase{phase=~""Pending|Running""} == 1))",,,,,,,,,"Show the maximum memory requested by containers in pods that are either pending or running, grouped by namespace, pod, and cluster."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,namespace_memory:kube_pod_container_resource_requests:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_requests{resource=""memory"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"Calculate the total requested memory for all containers in each namespace and cluster, but only consider containers that are part of pods which are either pending or running. The result is the sum of the maximum requested memory for each pod in each namespace and cluster."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests,"kube_pod_container_resource_requests{resource=""cpu"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) (  (kube_pod_status_phase{phase=~""Pending|Running""} == 1))",,,,,,,,,"""Show the maximum CPU requests for running or pending pods in each namespace and cluster, based on the resource requests of their containers."""
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,namespace_cpu:kube_pod_container_resource_requests:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_requests{resource=""cpu"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"Calculate the total requested CPU resources for all containers in each namespace and cluster, but only consider containers that are either pending or running. The result is the sum of the maximum requested CPU resources for each pod in each namespace and cluster."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,cluster:namespace:pod_memory:active:kube_pod_container_resource_limits,"kube_pod_container_resource_limits{resource=""memory"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) (  (kube_pod_status_phase{phase=~""Pending|Running""} == 1))",,,,,,,,,"Show the maximum memory limits for containers in pods that are either pending or running, grouped by namespace, pod, and cluster."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,namespace_memory:kube_pod_container_resource_limits:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_limits{resource=""memory"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"The total memory limits of all running or pending containers in each namespace across clusters, calculated by summing up the maximum memory limits of containers within each pod and then aggregating those totals by namespace and cluster."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits,"kube_pod_container_resource_limits{resource=""cpu"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) ( (kube_pod_status_phase{phase=~""Pending|Running""} == 1) )",,,,,,,,,"Show the maximum CPU limit for containers in pods that are either pending or running, grouped by namespace, pod, and cluster."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,namespace_cpu:kube_pod_container_resource_limits:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_limits{resource=""cpu"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"The total CPU limits of all running or pending containers in each namespace across clusters, calculated by summing up the maximum CPU limits of each container within a pod and then aggregating those values by namespace and cluster."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    label_replace(      kube_pod_owner{job=""kube-state-metrics"", owner_kind=""ReplicaSet""},      ""replicaset"", ""$1"", ""owner_name"", ""(.*)""    ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (      1, max by (replicaset, namespace, owner_name) (        kube_replicaset_owner{job=""kube-state-metrics""}      )    ),    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'deployment'},,,,,,,,"This Prometheus rule calculates the maximum number of pods in a ReplicaSet for each workload, across all clusters and namespaces. It does this by:

- Finding the owner (ReplicaSet) of each pod
- Identifying the top ReplicaSet with the most pods in each namespace
- Then, finding the workload that owns these top ReplicaSets

In simpler terms, it's trying to identify which workloads have the most pods running in their ReplicaSets across the cluster."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    kube_pod_owner{job=""kube-state-metrics"", owner_kind=""DaemonSet""},    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'daemonset'},,,,,,,,"Get the maximum number of pods owned by a DaemonSet in each cluster, namespace, and workload, where the workload name is extracted from the DaemonSet's name."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    kube_pod_owner{job=""kube-state-metrics"", owner_kind=""StatefulSet""},    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'statefulset'},,,,,,,,"Get the maximum number of pods owned by a StatefulSet in each cluster, namespace, and workload, where the workload name is extracted from the StatefulSet's owner name."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    kube_pod_owner{job=""kube-state-metrics"", owner_kind=""Job""},    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'job'},,,,,,,,"Get the maximum number of pods owned by a Job in each cluster, namespace, and workload, where the workload name is extracted from the Job's owner name."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,:node_memory_MemAvailable_bytes:sum,"sum(  node_memory_MemAvailable_bytes{job=""node""} or  (    node_memory_Buffers_bytes{job=""node""} +    node_memory_Cached_bytes{job=""node""} +    node_memory_MemFree_bytes{job=""node""} +    node_memory_Slab_bytes{job=""node""}  )) by (cluster)",,,,,,,,,"The total amount of available memory across all nodes in each cluster, calculated as either the directly reported available memory or the sum of buffers, cached, free, and slab memory."
KubernetesRecordingRulesRuleGroup-eastus-stage-arc,eastus,,cluster:node_cpu:ratio_rate5m,"sum(rate(node_cpu_seconds_total{job=""node"",mode!=""idle"",mode!=""iowait"",mode!=""steal""}[5m])) by (cluster) /count(sum(node_cpu_seconds_total{job=""node""}) by (cluster, instance, cpu)) by (cluster)",,,,,,,,,"Calculate the average CPU usage ratio for each cluster over the last 5 minutes, excluding idle, iowait, and steal modes, by summing up the total CPU time spent in other modes and dividing it by the total number of CPU cores across all instances in the cluster."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate,"sum by (cluster, namespace, pod, container) (  irate(container_cpu_usage_seconds_total{job=""cadvisor"", image!=""""}[5m])) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (  1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"Show the top CPU usage for containers in each pod across all clusters and namespaces over the last 5 minutes, considering only pods with a non-empty image name, and prioritize results based on the maximum number of pods per node."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,node_namespace_pod_container:container_memory_working_set_bytes,"container_memory_working_set_bytes{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container memory usage for each namespace and pod, considering only containers with a non-empty image name, grouped by namespace and pod, and limited to the maximum value for each pod across all nodes."""
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,node_namespace_pod_container:container_memory_rss,"container_memory_rss{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container with the highest memory usage (RSS) for each pod in a namespace, considering only containers from pods running on a specific node and ignoring containers without an image, limited to 1 result per pod."""
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,node_namespace_pod_container:container_memory_cache,"container_memory_cache{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container using the most memory cache for each namespace and pod, considering only containers with a non-empty image name, and filter the results to only include pods that are running on a node with available information."""
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,node_namespace_pod_container:container_memory_swap,"container_memory_swap{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container using swap memory for each namespace and pod, considering only containers with a non-empty image name, and filter the results to only include pods that are running on a node with available information."""
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,cluster:namespace:pod_memory:active:kube_pod_container_resource_requests,"kube_pod_container_resource_requests{resource=""memory"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) (  (kube_pod_status_phase{phase=~""Pending|Running""} == 1))",,,,,,,,,"Show the maximum memory requested by containers in pods that are either pending or running, grouped by namespace, pod, and cluster."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,namespace_memory:kube_pod_container_resource_requests:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_requests{resource=""memory"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"Calculate the total requested memory for all containers in each namespace and cluster, but only consider containers that are part of pods which are either pending or running. The result is the sum of the maximum requested memory for each pod in each namespace and cluster."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests,"kube_pod_container_resource_requests{resource=""cpu"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) (  (kube_pod_status_phase{phase=~""Pending|Running""} == 1))",,,,,,,,,"""Show the maximum CPU requests for running or pending pods in each namespace and cluster, based on the resource requests of their containers."""
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,namespace_cpu:kube_pod_container_resource_requests:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_requests{resource=""cpu"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"Calculate the total requested CPU resources for all containers in each namespace and cluster, but only consider containers that are either pending or running. The result is the sum of the maximum requested CPU resources for each pod in each namespace and cluster."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,cluster:namespace:pod_memory:active:kube_pod_container_resource_limits,"kube_pod_container_resource_limits{resource=""memory"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) (  (kube_pod_status_phase{phase=~""Pending|Running""} == 1))",,,,,,,,,"Show the maximum memory limits for containers in pods that are either pending or running, grouped by namespace, pod, and cluster."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,namespace_memory:kube_pod_container_resource_limits:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_limits{resource=""memory"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"The total memory limits of all running or pending containers in each namespace across clusters, calculated by summing up the maximum memory limits of containers within each pod and then aggregating those totals by namespace and cluster."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits,"kube_pod_container_resource_limits{resource=""cpu"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) ( (kube_pod_status_phase{phase=~""Pending|Running""} == 1) )",,,,,,,,,"Show the maximum CPU limit for containers in pods that are either pending or running, grouped by namespace, pod, and cluster."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,namespace_cpu:kube_pod_container_resource_limits:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_limits{resource=""cpu"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"This rule calculates the total CPU limits for all running or pending pods in each namespace across different clusters. It does this by first finding the maximum CPU limit for each container within a pod, then summing these limits for all containers in a pod, and finally summing these pod-level totals by namespace and cluster. The result is the total CPU limits for all running or pending pods in each namespace, aggregated by both namespace and cluster."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    label_replace(      kube_pod_owner{job=""kube-state-metrics"", owner_kind=""ReplicaSet""},      ""replicaset"", ""$1"", ""owner_name"", ""(.*)""    ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (      1, max by (replicaset, namespace, owner_name) (        kube_replicaset_owner{job=""kube-state-metrics""}      )    ),    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'deployment'},,,,,,,,"This Prometheus rule calculates the maximum number of pods in a ReplicaSet for each workload, across all clusters and namespaces. It does this by:

* Retrieving the owners of pods from the `kube_pod_owner` metric
* Replacing the owner name with the ReplicaSet name
* Joining this data with the `topk` function to get the top 1 ReplicaSet with the most pods for each namespace
* Then replacing the owner name with the workload name (e.g. Deployment, StatefulSet, etc.)
* Finally, it returns the maximum number of pods in a ReplicaSet for each workload, grouped by cluster, namespace, workload, and pod.

In simpler terms: ""What is the maximum number of pods running for each type of workload (like Deployments or StatefulSets) across all clusters and namespaces?"""
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    kube_pod_owner{job=""kube-state-metrics"", owner_kind=""DaemonSet""},    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'daemonset'},,,,,,,,"Get the maximum number of pods owned by a DaemonSet in each cluster, namespace, and workload, where the workload name is extracted from the DaemonSet's name."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    kube_pod_owner{job=""kube-state-metrics"", owner_kind=""StatefulSet""},    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'statefulset'},,,,,,,,"Get the maximum number of pods in a StatefulSet, grouped by cluster, namespace, workload, and pod, where the workload name is extracted from the owner name of the StatefulSet."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    kube_pod_owner{job=""kube-state-metrics"", owner_kind=""Job""},    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'job'},,,,,,,,"Get the maximum number of pods for each combination of cluster, namespace, workload, and pod, where the workload is determined by replacing the ""owner_name"" label with a new ""workload"" label in the kube_pod_owner metric, but only consider metrics from the ""kube-state-metrics"" job and where the owner kind is a Job."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,:node_memory_MemAvailable_bytes:sum,"sum(  node_memory_MemAvailable_bytes{job=""node""} or  (    node_memory_Buffers_bytes{job=""node""} +    node_memory_Cached_bytes{job=""node""} +    node_memory_MemFree_bytes{job=""node""} +    node_memory_Slab_bytes{job=""node""}  )) by (cluster)",,,,,,,,,"The total amount of available memory across all nodes in each cluster, calculated as either the directly reported available memory or the sum of buffers, cached, free, and slab memory."
KubernetesReccordingRulesRuleGroup-eastus-stage-arc,eastus,Kubernetes Recording Rules RuleGroup - 0.1,cluster:node_cpu:ratio_rate5m,"sum(rate(node_cpu_seconds_total{job=""node"",mode!=""idle"",mode!=""iowait"",mode!=""steal""}[5m])) by (cluster) /count(sum(node_cpu_seconds_total{job=""node""}) by (cluster, instance, cpu)) by (cluster)",,,,,,,,,"Calculate the average CPU usage ratio for each cluster over the last 5 minutes, excluding idle, iowait, and steal modes, by summing up the total CPU time spent in other modes and dividing it by the total number of CPU cores across all instances in the cluster."
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance:node_num_cpu:sum,"count without (cpu, mode) (  node_cpu_seconds_total{job=""node"",mode=""idle""})",,,,,,,,,"Count the total number of nodes that are reporting CPU idle time, ignoring the differences in CPU core and mode."
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance:node_cpu_utilisation:rate5m,"1 - avg without (cpu) (  sum without (mode) (rate(node_cpu_seconds_total{job=""node"", mode=~""idle|iowait|steal""}[5m])))",,,,,,,,,"The average CPU utilization across all CPUs for each instance over the last 5 minutes, calculated as the percentage of time spent not in idle, iowait, or steal modes."
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance:node_load1_per_cpu:ratio,"(  node_load1{job=""node""}/  instance:node_num_cpu:sum{job=""node""})",,,,,,,,,"The average load on a single CPU for each instance, calculated by dividing the 1-minute load average by the total number of CPUs."
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance:node_memory_utilisation:ratio,"1 - (  (    node_memory_MemAvailable_bytes{job=""node""}    or    (      node_memory_Buffers_bytes{job=""node""}      +      node_memory_Cached_bytes{job=""node""}      +      node_memory_MemFree_bytes{job=""node""}      +      node_memory_Slab_bytes{job=""node""}    )  )/  node_memory_MemTotal_bytes{job=""node""})",,,,,,,,,"This rule calculates the ratio of used memory to total memory for a node, taking into account available memory, buffers, cached memory, free memory, and slab memory. It subtracts the sum of these available memory components from the total memory and divides by the total memory, then subtracts this result from 1 to get the utilization ratio. In simpler terms, it shows what percentage of a node's memory is being used."
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance:node_vmstat_pgmajfault:rate5m,"rate(node_vmstat_pgmajfault{job=""node""}[5m])",,,,,,,,,"""The rate of major page faults per second for a node, averaged over the last 5 minutes."""
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance_device:node_disk_io_time_seconds:rate5m,"rate(node_disk_io_time_seconds_total{job=""node"", device!=""""}[5m])",,,,,,,,,"""The average rate of disk I/O time per second for each node over the last 5 minutes, excluding devices with no name."""
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance_device:node_disk_io_time_weighted_seconds:rate5m,"rate(node_disk_io_time_weighted_seconds_total{job=""node"", device!=""""}[5m])",,,,,,,,,"""The average rate of disk I/O time per second for each node over the last 5 minutes, excluding devices with no name."""
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance:node_network_receive_bytes_excluding_lo:rate5m,"sum without (device) (  rate(node_network_receive_bytes_total{job=""node"", device!=""lo""}[5m]))",,,,,,,,,"""The total amount of data received by each node on all non-loopback network devices, averaged over a 5-minute period."""
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance:node_network_transmit_bytes_excluding_lo:rate5m,"sum without (device) (  rate(node_network_transmit_bytes_total{job=""node"", device!=""lo""}[5m]))",,,,,,,,,"The average rate of bytes transmitted by each network device (excluding the loopback interface) over the last 5 minutes, summed across all devices on a node."
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance:node_network_receive_drop_excluding_lo:rate5m,"sum without (device) (  rate(node_network_receive_drop_total{job=""node"", device!=""lo""}[5m]))",,,,,,,,,"""The average rate of packets dropped while receiving network traffic, excluding the loopback interface, over a 5-minute period, summed across all devices on each node."""
NodeRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,instance:node_network_transmit_drop_excluding_lo:rate5m,"sum without (device) (  rate(node_network_transmit_drop_total{job=""node"", device!=""lo""}[5m]))",,,,,,,,,"""The average rate of packets dropped while transmitting data from all network devices (excluding the loopback interface) across all nodes over a 5-minute period."""
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate,"sum by (cluster, namespace, pod, container) (  irate(container_cpu_usage_seconds_total{job=""cadvisor"", image!=""""}[5m])) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (  1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"Show the top CPU usage for containers in each pod across all clusters and namespaces over the last 5 minutes, considering only pods with a non-empty image name and cadvisor job, and group the results by cluster, namespace, and pod, while also including the node information for each pod."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,node_namespace_pod_container:container_memory_working_set_bytes,"container_memory_working_set_bytes{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container memory usage for each namespace and pod, considering only containers with a specified image, and return the maximum value for each combination of namespace, pod, and node, limited to the top result."""
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,node_namespace_pod_container:container_memory_rss,"container_memory_rss{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container with the highest memory usage (RSS) for each pod in a namespace, considering only containers from pods running on nodes and excluding those without an image, limited to the top result per pod."""
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,node_namespace_pod_container:container_memory_cache,"container_memory_cache{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container using the most memory cache for each namespace and pod, considering only containers with a non-empty image name, and filter the results to only include pods that are running on a node with available information."""
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,node_namespace_pod_container:container_memory_swap,"container_memory_swap{job=""cadvisor"", image!=""""}* on (namespace, pod) group_left(node) topk by(namespace, pod) (1,  max by(namespace, pod, node) (kube_pod_info{node!=""""}))",,,,,,,,,"""Show the top container using swap memory for each namespace and pod, considering only containers with a non-empty image name, and filter the results to only include pods that are running on a node with available information."""
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,cluster:namespace:pod_memory:active:kube_pod_container_resource_requests,"kube_pod_container_resource_requests{resource=""memory"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) (  (kube_pod_status_phase{phase=~""Pending|Running""} == 1))",,,,,,,,,"Show the maximum memory requested by containers in pods that are either pending or running, grouped by namespace, pod, and cluster."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,namespace_memory:kube_pod_container_resource_requests:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_requests{resource=""memory"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"Calculate the total requested memory for all containers in each namespace and cluster, but only consider containers that are part of pods which are either pending or running. The result is the sum of the maximum requested memory for each pod in each namespace and cluster."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests,"kube_pod_container_resource_requests{resource=""cpu"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) (  (kube_pod_status_phase{phase=~""Pending|Running""} == 1))",,,,,,,,,"""Show the maximum CPU requests for running or pending pods in each namespace and cluster, based on the resource requests of their containers."""
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,namespace_cpu:kube_pod_container_resource_requests:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_requests{resource=""cpu"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"Calculate the total requested CPU resources for all containers in each namespace and cluster, but only consider containers that are either pending or running. The result is the sum of the maximum requested CPU resources for each pod in each namespace and cluster."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,cluster:namespace:pod_memory:active:kube_pod_container_resource_limits,"kube_pod_container_resource_limits{resource=""memory"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) (  (kube_pod_status_phase{phase=~""Pending|Running""} == 1))",,,,,,,,,"Show the maximum memory limits for containers in pods that are either pending or running, grouped by namespace, pod, and cluster."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,namespace_memory:kube_pod_container_resource_limits:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_limits{resource=""memory"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"The total memory limits of all running or pending containers in each namespace across clusters, calculated by summing up the maximum memory limits of containers within each pod and then aggregating those values by namespace and cluster."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits,"kube_pod_container_resource_limits{resource=""cpu"",job=""kube-state-metrics""}  * on (namespace, pod, cluster)group_left() max by (namespace, pod, cluster) ( (kube_pod_status_phase{phase=~""Pending|Running""} == 1) )",,,,,,,,,"Show the maximum CPU limit for containers in pods that are either pending or running, grouped by namespace, pod, and cluster."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,namespace_cpu:kube_pod_container_resource_limits:sum,"sum by (namespace, cluster) (    sum by (namespace, pod, cluster) (        max by (namespace, pod, container, cluster) (          kube_pod_container_resource_limits{resource=""cpu"",job=""kube-state-metrics""}        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (          kube_pod_status_phase{phase=~""Pending|Running""} == 1        )    ))",,,,,,,,,"This rule calculates the total CPU limits for all running or pending pods in each namespace across different clusters. It does this by first finding the maximum CPU limit for each container within a pod, then summing these limits for all containers in a pod, and finally summing these pod-level totals by namespace and cluster. The result is the total CPU limits for all running or pending pods in each namespace, aggregated by both namespace and cluster."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    label_replace(      kube_pod_owner{job=""kube-state-metrics"", owner_kind=""ReplicaSet""},      ""replicaset"", ""$1"", ""owner_name"", ""(.*)""    ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (      1, max by (replicaset, namespace, owner_name) (        kube_replicaset_owner{job=""kube-state-metrics""}      )    ),    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'deployment'},,,,,,,,"This Prometheus rule calculates the maximum number of pods in a ReplicaSet for each workload in a namespace, across all clusters. It does this by:

- Finding the owner (ReplicaSet) of each pod
- Identifying the top ReplicaSet with the most pods in each namespace
- Then, finding the workload that owns these top ReplicaSets

In simpler terms, it's trying to identify which workloads have the most pods running in their ReplicaSets across different clusters and namespaces."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    kube_pod_owner{job=""kube-state-metrics"", owner_kind=""DaemonSet""},    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'daemonset'},,,,,,,,"Get the maximum number of pods owned by a DaemonSet in each cluster, namespace, and workload, where the workload name is extracted from the DaemonSet's name."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    kube_pod_owner{job=""kube-state-metrics"", owner_kind=""StatefulSet""},    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'statefulset'},,,,,,,,"Get the maximum number of pods in a StatefulSet, grouped by cluster, namespace, workload, and pod, where the workload name is extracted from the owner name of the StatefulSet."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,namespace_workload_pod:kube_pod_owner:relabel,"max by (cluster, namespace, workload, pod) (  label_replace(    kube_pod_owner{job=""kube-state-metrics"", owner_kind=""Job""},    ""workload"", ""$1"", ""owner_name"", ""(.*)""  ))",{'workload_type': 'job'},,,,,,,,"Get the maximum number of pods for each combination of cluster, namespace, workload, and pod, where the workload is determined by replacing the ""owner_name"" label with a new ""workload"" label in the kube_pod_owner metric, but only consider metrics from the ""kube-state-metrics"" job and where the owner kind is a Job."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,:node_memory_MemAvailable_bytes:sum,"sum(  node_memory_MemAvailable_bytes{job=""node""} or  (    node_memory_Buffers_bytes{job=""node""} +    node_memory_Cached_bytes{job=""node""} +    node_memory_MemFree_bytes{job=""node""} +    node_memory_Slab_bytes{job=""node""}  )) by (cluster)",,,,,,,,,"The total amount of available memory across all nodes in each cluster, calculated as either the directly reported available memory or the sum of buffers, cached, free, and slab memory."
KubernetesRecordingRulesRuleGroup-aks-sde-eastus-stage,eastus,,cluster:node_cpu:ratio_rate5m,"sum(rate(node_cpu_seconds_total{job=""node"",mode!=""idle"",mode!=""iowait"",mode!=""steal""}[5m])) by (cluster) /count(sum(node_cpu_seconds_total{job=""node""}) by (cluster, instance, cpu)) by (cluster)",,,,,,,,,"Calculate the average CPU usage ratio for each cluster over the last 5 minutes, excluding idle, iowait, and steal modes, by summing up the total CPU time used by all nodes in a cluster and dividing it by the total number of CPU cores available across all instances in that cluster."
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node:sum,"count (windows_system_system_up_time{job=""windows-exporter""})",,,,,,,,,The total number of Windows machines that are currently reporting their system uptime via the windows-exporter job.
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_num_cpu:sum,"count by (instance) (sum by (instance, core) (windows_cpu_time_total{job=""windows-exporter""}))",,,,,,,,,"Count the total number of CPU cores across all Windows instances, grouped by instance."
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,:windows_node_cpu_utilisation:avg5m,"1 - avg(rate(windows_cpu_time_total{job=""windows-exporter"",mode=""idle""}[5m]))",,,,,,,,,"The average CPU idle time for Windows nodes over the last 5 minutes, subtracted from 1 to represent the average CPU utilization."
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_cpu_utilisation:avg5m,"1 - avg by (instance) (rate(windows_cpu_time_total{job=""windows-exporter"",mode=""idle""}[5m]))",,,,,,,,,"""The average CPU idle time for Windows nodes over the last 5 minutes, subtracted from 1 to get the average CPU utilization."""
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,:windows_node_memory_utilisation:,"1 -sum(windows_memory_available_bytes{job=""windows-exporter""})/sum(windows_os_visible_memory_bytes{job=""windows-exporter""})",,,,,,,,,"""The percentage of used memory on Windows nodes, calculated by subtracting the available memory from the total visible memory and expressing it as a proportion of the total."""
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,:windows_node_memory_MemFreeCached_bytes:sum,"sum(windows_memory_available_bytes{job=""windows-exporter""} + windows_memory_cache_bytes{job=""windows-exporter""})",,,,,,,,,"The total amount of free and cached memory in bytes across all Windows nodes, calculated by adding the available memory and cache memory from the Windows exporter job."
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_memory_totalCached_bytes:sum,"(windows_memory_cache_bytes{job=""windows-exporter""} + windows_memory_modified_page_list_bytes{job=""windows-exporter""} + windows_memory_standby_cache_core_bytes{job=""windows-exporter""} + windows_memory_standby_cache_normal_priority_bytes{job=""windows-exporter""} + windows_memory_standby_cache_reserve_bytes{job=""windows-exporter""})",,,,,,,,,"The total amount of cached memory on a Windows node, calculated by summing up the following components: 
- the amount of memory used for caching files 
- the amount of memory used for modified pages waiting to be written to disk 
- the amount of memory used for standby cache with core priority 
- the amount of memory used for standby cache with normal priority 
- the amount of memory reserved for standby cache."
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,:windows_node_memory_MemTotal_bytes:sum,"sum(windows_os_visible_memory_bytes{job=""windows-exporter""})",,,,,,,,,The total amount of visible memory in bytes across all Windows nodes being monitored by the windows-exporter job.
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_memory_bytes_available:sum,"sum by (instance) ((windows_memory_available_bytes{job=""windows-exporter""}))",,,,,,,,,"Show the total amount of available memory in bytes for each Windows instance, as reported by the windows-exporter job."
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_memory_bytes_total:sum,"sum by (instance) (windows_os_visible_memory_bytes{job=""windows-exporter""})",,,,,,,,,"Calculate the total visible memory for each Windows instance, summing up the values from the windows-exporter job."
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_memory_utilisation:ratio,(node:windows_node_memory_bytes_total:sum - node:windows_node_memory_bytes_available:sum) / scalar(sum(node:windows_node_memory_bytes_total:sum)),,,,,,,,,"The percentage of used memory on Windows nodes, calculated by subtracting available memory from total memory and dividing by the total memory."
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_memory_utilisation:,1 - (node:windows_node_memory_bytes_available:sum / node:windows_node_memory_bytes_total:sum),,,,,,,,,"""The percentage of memory used on Windows nodes, calculated by subtracting the available memory from the total memory and expressing it as a proportion of the total."""
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_memory_swap_io_pages:irate,"irate(windows_memory_swap_page_operations_total{job=""windows-exporter""}[5m])",,,,,,,,,The rate of Windows swap page operations per second over the last 5 minutes for nodes running the windows-exporter job.
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,:windows_node_disk_utilisation:avg_irate,"avg(irate(windows_logical_disk_read_seconds_total{job=""windows-exporter""}[5m]) + irate(windows_logical_disk_write_seconds_total{job=""windows-exporter""}[5m]))",,,,,,,,,"The average rate of disk read and write operations per second over the last 5 minutes for Windows nodes, as reported by the Windows Exporter job."
NodeRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_disk_utilisation:avg_irate,"avg by (instance) ((irate(windows_logical_disk_read_seconds_total{job=""windows-exporter""}[5m]) + irate(windows_logical_disk_write_seconds_total{job=""windows-exporter""}[5m])))",,,,,,,,,"Calculate the average rate of disk utilization for each Windows instance over the last 5 minutes, by adding the rates of read and write operations on logical disks."
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_filesystem_usage:,"max by (instance,volume)((windows_logical_disk_size_bytes{job=""windows-exporter""} - windows_logical_disk_free_bytes{job=""windows-exporter""}) / windows_logical_disk_size_bytes{job=""windows-exporter""})",,,,,,,,,"""The percentage of used disk space on Windows nodes, calculated by subtracting the free disk space from the total disk size and dividing by the total disk size, with the result showing the maximum usage across all volumes for each instance."""
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_filesystem_avail:,"max by (instance, volume) (windows_logical_disk_free_bytes{job=""windows-exporter""} / windows_logical_disk_size_bytes{job=""windows-exporter""})",,,,,,,,,"""The percentage of available disk space on each Windows node and volume, calculated as the maximum free bytes divided by the total size."""
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,:windows_node_net_utilisation:sum_irate,"sum(irate(windows_net_bytes_total{job=""windows-exporter""}[5m]))",,,,,,,,,"Show the total rate of network bytes used by all Windows nodes over the last 5 minutes, calculated as the sum of the increase in network bytes for each node during that time period."
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_net_utilisation:sum_irate,"sum by (instance) ((irate(windows_net_bytes_total{job=""windows-exporter""}[5m])))",,,,,,,,,"Show the total rate of network bytes used by each Windows instance over the last 5 minutes, calculated as the sum of the increase in total network bytes for each instance."
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,:windows_node_net_saturation:sum_irate,"sum(irate(windows_net_packets_received_discarded_total{job=""windows-exporter""}[5m])) + sum(irate(windows_net_packets_outbound_discarded_total{job=""windows-exporter""}[5m]))",,,,,,,,,"The total rate of discarded network packets on Windows nodes over the last 5 minutes, including both incoming and outgoing packets."
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,node:windows_node_net_saturation:sum_irate,"sum by (instance) ((irate(windows_net_packets_received_discarded_total{job=""windows-exporter""}[5m]) + irate(windows_net_packets_outbound_discarded_total{job=""windows-exporter""}[5m])))",,,,,,,,,"The total rate of discarded network packets (both incoming and outgoing) per Windows instance over the last 5 minutes, summed up across all instances."
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,windows_pod_container_available,"windows_container_available{job=""windows-exporter"", container_id != """"} * on(container_id) group_left(container, pod, namespace) max(kube_pod_container_info{job=""kube-state-metrics"", container_id != """"}) by(container, container_id, pod, namespace)",,,,,,,,,"""Show the maximum value of available containers for each Windows pod container, where the container ID is not empty, grouped by container, container ID, pod, and namespace, and joined with Kubernetes pod container information from the kube-state-metrics job."""
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,windows_container_total_runtime,"windows_container_cpu_usage_seconds_total{job=""windows-exporter"", container_id != """"} * on(container_id) group_left(container, pod, namespace) max(kube_pod_container_info{job=""kube-state-metrics"", container_id != """"}) by(container, container_id, pod, namespace)",,,,,,,,,"""Calculate the total runtime for each Windows container by multiplying the total CPU usage in seconds for each container with the maximum value of container information from Kubernetes, grouping the results by container, container ID, pod, and namespace, while ignoring containers without an ID."""
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,windows_container_memory_usage,"windows_container_memory_usage_commit_bytes{job=""windows-exporter"", container_id != """"} * on(container_id) group_left(container, pod, namespace) max(kube_pod_container_info{job=""kube-state-metrics"", container_id != """"}) by(container, container_id, pod, namespace)",,,,,,,,,"""Show the maximum memory usage for each Windows container, considering only containers with a valid ID, and provide additional information about the container, pod, and namespace it belongs to."""
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,windows_container_private_working_set_usage,"windows_container_memory_usage_private_working_set_bytes{job=""windows-exporter"", container_id != """"} * on(container_id) group_left(container, pod, namespace) max(kube_pod_container_info{job=""kube-state-metrics"", container_id != """"}) by(container, container_id, pod, namespace)",,,,,,,,,"""Show the maximum private working set memory usage for each Windows container, grouped by container, container ID, pod, and namespace, where the container has a non-empty ID and is associated with a Kubernetes pod."""
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,windows_container_network_received_bytes_total,"windows_container_network_receive_bytes_total{job=""windows-exporter"", container_id != """"} * on(container_id) group_left(container, pod, namespace) max(kube_pod_container_info{job=""kube-state-metrics"", container_id != """"}) by(container, container_id, pod, namespace)",,,,,,,,,"""Show the total number of bytes received by each Windows container's network interface, but only for containers that have a non-empty ID and are associated with a Kubernetes pod, and return the maximum value for each unique combination of container, container ID, pod, and namespace."""
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,windows_container_network_transmitted_bytes_total,"windows_container_network_transmit_bytes_total{job=""windows-exporter"", container_id != """"} * on(container_id) group_left(container, pod, namespace) max(kube_pod_container_info{job=""kube-state-metrics"", container_id != """"}) by(container, container_id, pod, namespace)",,,,,,,,,"""Show the total number of bytes transmitted by each Windows container, grouped by container, container ID, pod, and namespace, where the container has a non-empty ID, and also show the maximum value of the kube_pod_container_info metric for each group, but only consider pods that have a non-empty container ID."""
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,kube_pod_windows_container_resource_memory_request,"max by (namespace, pod, container) (kube_pod_container_resource_requests{resource=""memory"",job=""kube-state-metrics""}) * on(container,pod,namespace) (windows_pod_container_available)",,,,,,,,,"The maximum memory requested by containers in Windows pods, calculated for each namespace, pod, and container, and scaled by the available resources of the corresponding Windows pod container."
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,kube_pod_windows_container_resource_memory_limit,"kube_pod_container_resource_limits{resource=""memory"",job=""kube-state-metrics""} * on(container,pod,namespace) (windows_pod_container_available)",,,,,,,,,"The amount of memory limit set for Windows containers in Kubernetes pods, calculated by multiplying the memory limits of each container with the availability of Windows containers in the same pod and namespace."
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,kube_pod_windows_container_resource_cpu_cores_request,"max by (namespace, pod, container) ( kube_pod_container_resource_requests{resource=""cpu"",job=""kube-state-metrics""}) * on(container,pod,namespace) (windows_pod_container_available)",,,,,,,,,"""Get the maximum CPU cores requested by containers in Windows pods, for each namespace, pod, and container, where the available resources are also considered."""
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,kube_pod_windows_container_resource_cpu_cores_limit,"kube_pod_container_resource_limits{resource=""cpu"",job=""kube-state-metrics""} * on(container,pod,namespace) (windows_pod_container_available)",,,,,,,,,"The total number of CPU cores allocated as a limit to Windows containers in Kubernetes pods, based on the resource limits and availability of those containers."
NodeAndKubernetesRecordingRulesRuleGroup-Win-aks-sde-eastus-stage,eastus,,namespace_pod_container:windows_container_cpu_usage_seconds_total:sum_rate,"sum by (namespace, pod, container) (rate(windows_container_total_runtime{}[5m]))",,,,,,,,,"Show the total CPU usage rate over 5 minutes for Windows containers, grouped by namespace, pod, and container."
K8sAlerts,eastus,,,"kube_node_status_condition{condition='Ready',status='true'} == 0",{'team': 'Argus'},K8sNodeNotReady,True,3.0,PT15M,[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourceGroups/anf.automation-stage.rg/providers/microsoft.insights/actiongroups/Azure-Monitor-IcM-Stage'}],"{'autoResolved': True, 'timeToResolve': 'PT15M'}",,The Kubernetes node is not ready.
K8sAlerts,eastus,,,kube_pod_container_status_terminated == 1,{'Team': 'Argus'},K8sPodTerminated,False,3.0,PT5M,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actiongroups/azure-monitor-icm-stage', 'actionProperties': {'IcM.Title': 'Prometheus Alert [#$.labels.cluster#] - [#$.labels.pod#] is getting terminated'}}]","{'autoResolved': False, 'timeToResolve': 'PT5M'}",{},There is at least one terminated container in a Kubernetes pod.
UXRecordingRulesRuleGroup-Win - aks-sde-eastus-stage,eastus,UX Recording Rules for Windows,ux:pod_cpu_usage_windows:sum_irate,"sum by (cluster, pod, namespace, node, created_by_kind, created_by_name, microsoft_resourceid) (
	(
		max by (instance, container_id, cluster, microsoft_resourceid) (
			irate(windows_container_cpu_usage_seconds_total{ container_id != """", job = ""windows-exporter""}[5m])
		) * on (container_id, cluster, microsoft_resourceid) group_left (container, pod, namespace) (
			max by (container, container_id, pod, namespace, cluster, microsoft_resourceid) (
				kube_pod_container_info{container != """", pod != """", container_id != """", job = ""kube-state-metrics""}
			)
		)
	) * on (pod, namespace, cluster, microsoft_resourceid) group_left (node, created_by_name, created_by_kind)
	(
		max by (node, created_by_name, created_by_kind, pod, namespace, cluster, microsoft_resourceid) (
		  kube_pod_info{ pod != """", job = ""kube-state-metrics""}
		)
	)
)",,,,,,,,,"Calculate the sum of the average CPU usage rate over 5 minutes for Windows containers in each pod, grouped by cluster, pod, namespace, node, and other identifying labels. To do this, first find the maximum CPU usage rate for each container, then match it with the corresponding container and pod information, and finally combine it with the pod's node and creation information."
UXRecordingRulesRuleGroup-Win - aks-sde-eastus-stage,eastus,UX Recording Rules for Windows,ux:controller_cpu_usage_windows:sum_irate,"sum by (namespace, node, cluster, created_by_name, created_by_kind, microsoft_resourceid) (
ux:pod_cpu_usage_windows:sum_irate
)
",,,,,,,,,"Calculate the total CPU usage rate for all Windows controllers, grouped by namespace, node, cluster, and other identifying labels, based on the CPU usage rates of individual pods."
UXRecordingRulesRuleGroup-Win - aks-sde-eastus-stage,eastus,UX Recording Rules for Windows,ux:pod_workingset_memory_windows:sum,"sum by (cluster, pod, namespace, node, created_by_kind, created_by_name, microsoft_resourceid) (
	(
		max by (instance, container_id, cluster, microsoft_resourceid) (
			windows_container_memory_usage_private_working_set_bytes{ container_id != """", job = ""windows-exporter""}
		) * on (container_id, cluster, microsoft_resourceid) group_left (container, pod, namespace) (
			max by (container, container_id, pod, namespace, cluster, microsoft_resourceid) (
				kube_pod_container_info{container != """", pod != """", container_id != """", job = ""kube-state-metrics""}
			)
		)
	) * on (pod, namespace, cluster, microsoft_resourceid) group_left (node, created_by_name, created_by_kind)
	(
		max by (node, created_by_name, created_by_kind, pod, namespace, cluster, microsoft_resourceid) (
		  kube_pod_info{ pod != """", job = ""kube-state-metrics""}
		)
	)
)",,,,,,,,,"Calculate the total private working set memory used by containers in Windows pods, aggregated by cluster, pod, namespace, node, and other identifying labels, by multiplying the maximum private working set memory usage of each container with its corresponding pod and node information."
UXRecordingRulesRuleGroup-Win - aks-sde-eastus-stage,eastus,UX Recording Rules for Windows,ux:controller_workingset_memory_windows:sum,"sum by (namespace, node, cluster, created_by_name, created_by_kind, microsoft_resourceid) (
ux:pod_workingset_memory_windows:sum
)",,,,,,,,,"Calculate the total working set memory used by controllers in Windows, grouped by namespace, node, cluster, and other identifying labels."
UXRecordingRulesRuleGroup-Win - aks-sde-eastus-stage,eastus,UX Recording Rules for Windows,ux:node_cpu_usage_windows:sum_irate,"sum by (instance, cluster, microsoft_resourceid) (
(1 - irate(windows_cpu_time_total{job=""windows-exporter"", mode=""idle""}[5m]))
)",,,,,,,,,"Show the total CPU usage for Windows nodes, calculated as the average rate of change over 5 minutes, excluding idle time, and grouped by instance, cluster, and Microsoft resource ID."
UXRecordingRulesRuleGroup-Win - aks-sde-eastus-stage,eastus,UX Recording Rules for Windows,ux:node_memory_usage_windows:sum,"sum by (instance, cluster, microsoft_resourceid) ((
windows_os_visible_memory_bytes{job = ""windows-exporter""}
- windows_memory_available_bytes{job = ""windows-exporter""}
))",,,,,,,,,"Calculate the total used memory for each Windows instance, cluster, and resource ID by subtracting the available memory from the visible memory."
UXRecordingRulesRuleGroup-Win - aks-sde-eastus-stage,eastus,UX Recording Rules for Windows,ux:node_network_packets_received_drop_total_windows:sum_irate,"sum by (instance, cluster, microsoft_resourceid) (irate(windows_net_packets_received_discarded_total{job=""windows-exporter"", device!=""lo""}[5m]))",,,,,,,,,"""The total rate of dropped incoming network packets on Windows nodes over the last 5 minutes, grouped by instance, cluster, and Microsoft resource ID, excluding the loopback device."""
UXRecordingRulesRuleGroup-Win - aks-sde-eastus-stage,eastus,UX Recording Rules for Windows,ux:node_network_packets_outbound_drop_total_windows:sum_irate,"sum by (instance, cluster, microsoft_resourceid) (irate(windows_net_packets_outbound_discarded_total{job=""windows-exporter"", device!=""lo""}[5m]))",,,,,,,,,"""The total rate of outbound network packets dropped on Windows nodes over the last 5 minutes, grouped by instance, cluster, and Microsoft resource ID, excluding the loopback device."""
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:pod_cpu_usage:sum_irate,"(sum by (namespace, pod, cluster, microsoft_resourceid) (
	irate(container_cpu_usage_seconds_total{container != """", pod != """", job = ""cadvisor""}[5m])
)) * on (pod, namespace, cluster, microsoft_resourceid) group_left (node, created_by_name, created_by_kind)
(max by (node, created_by_name, created_by_kind, pod, namespace, cluster, microsoft_resourceid) (kube_pod_info{pod != """", job = ""kube-state-metrics""}))",,,,,,,,,"Calculate the total CPU usage rate for each pod over the last 5 minutes, grouped by namespace, pod, cluster, and Microsoft resource ID, and then multiply it by the maximum number of pods with matching node, creator name, creator kind, pod, namespace, cluster, and Microsoft resource ID."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:controller_cpu_usage:sum_irate,"sum by (namespace, node, cluster, created_by_name, created_by_kind, microsoft_resourceid) (
ux:pod_cpu_usage:sum_irate
)
",,,,,,,,,"Calculate the total CPU usage rate for all controllers, grouped by namespace, node, cluster, and other identifying labels, based on the CPU usage rates of individual pods."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:pod_workingset_memory:sum,"(
	    sum by (namespace, pod, cluster, microsoft_resourceid) (
		container_memory_working_set_bytes{container != """", pod != """", job = ""cadvisor""}
	    )
	) * on (pod, namespace, cluster, microsoft_resourceid) group_left (node, created_by_name, created_by_kind)
(max by (node, created_by_name, created_by_kind, pod, namespace, cluster, microsoft_resourceid) (kube_pod_info{pod != """", job = ""kube-state-metrics""}))",,,,,,,,,"The total working set memory used by containers in each pod, summed across all pods with matching labels, and then multiplied by the maximum number of pods with matching node, creator, and other labels, effectively giving the total working set memory used by containers in each pod, scaled by the maximum number of pods on each node."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:controller_workingset_memory:sum,"sum by (namespace, node, cluster, created_by_name, created_by_kind, microsoft_resourceid) (
ux:pod_workingset_memory:sum
)",,,,,,,,,"Calculate the total working set memory used by all controllers in each unique combination of namespace, node, cluster, creator name, creator kind, and Microsoft resource ID, by summing up the working set memory used by individual pods."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:pod_rss_memory:sum,"(
	    sum by (namespace, pod, cluster, microsoft_resourceid) (
		container_memory_rss{container != """", pod != """", job = ""cadvisor""}
	    )
	) * on (pod, namespace, cluster, microsoft_resourceid) group_left (node, created_by_name, created_by_kind)
(max by (node, created_by_name, created_by_kind, pod, namespace, cluster, microsoft_resourceid) (kube_pod_info{pod != """", job = ""kube-state-metrics""}))",,,,,,,,,"Calculate the total resident set size (RSS) memory used by containers in each pod, and then multiply it by the maximum value of a metric that provides information about the pod, grouped by node, creator, and other identifiers, effectively scaling the memory usage by a factor related to the pod's properties."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:controller_rss_memory:sum,"sum by (namespace, node, cluster, created_by_name, created_by_kind, microsoft_resourceid) (
ux:pod_rss_memory:sum
)",,,,,,,,,"Calculate the total RSS memory used by all containers in a controller, grouped by namespace, node, cluster, and other identifying labels."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:pod_container_count:sum,"sum by (node, created_by_name, created_by_kind, namespace, cluster, pod, microsoft_resourceid) (
(
(
sum by (container, pod, namespace, cluster, microsoft_resourceid) (kube_pod_container_info{container != """", pod != """", container_id != """", job = ""kube-state-metrics""})
or sum by (container, pod, namespace, cluster, microsoft_resourceid) (kube_pod_init_container_info{container != """", pod != """", container_id != """", job = ""kube-state-metrics""})
)
* on (pod, namespace, cluster, microsoft_resourceid) group_left (node, created_by_name, created_by_kind)
(
max by (node, created_by_name, created_by_kind, pod, namespace, cluster, microsoft_resourceid) (
	kube_pod_info{pod != """", job = ""kube-state-metrics""}
)
)
)

)",,,,,,,,,"Count the total number of containers in each pod, considering both regular and init containers, and group the results by node, creator, namespace, cluster, pod, and Microsoft resource ID. This is done by combining information from kube-state-metrics about pods and their containers, and then linking this data with information about the nodes and creators of these pods."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:controller_container_count:sum,"sum by (node, created_by_name, created_by_kind, namespace, cluster, microsoft_resourceid) (
ux:pod_container_count:sum
)",,,,,,,,,"Calculate the total number of containers in all controller pods, grouped by node, creator name, creator kind, namespace, cluster, and Microsoft resource ID."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:pod_container_restarts:max,"max by (node, created_by_name, created_by_kind, namespace, cluster, pod, microsoft_resourceid) (
(
(
max by (container, pod, namespace, cluster, microsoft_resourceid) (kube_pod_container_status_restarts_total{container != """", pod != """", job = ""kube-state-metrics""})
or sum by (container, pod, namespace, cluster, microsoft_resourceid) (kube_pod_init_status_restarts_total{container != """", pod != """", job = ""kube-state-metrics""})
)
* on (pod, namespace, cluster, microsoft_resourceid) group_left (node, created_by_name, created_by_kind)
(
max by (node, created_by_name, created_by_kind, pod, namespace, cluster, microsoft_resourceid) (
	kube_pod_info{pod != """", job = ""kube-state-metrics""}
)
)
)

)",,,,,,,,,"This Prometheus rule calculates the maximum number of container restarts for each pod, considering both regular containers and init containers, and groups the results by node, creator, namespace, cluster, pod, and Microsoft resource ID. It essentially answers the question: ""What is the maximum number of times any container in a pod has been restarted, grouped by various pod and node identifiers?"""
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:controller_container_restarts:max,"max by (node, created_by_name, created_by_kind, namespace, cluster, microsoft_resourceid) (
ux:pod_container_restarts:max
)",,,,,,,,,"Show the maximum number of container restarts for each unique combination of node, creator, namespace, cluster, and Microsoft resource ID across all pods."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:pod_resource_limit:sum,"(sum by (cluster, pod, namespace, resource, microsoft_resourceid) (
(
	max by (cluster, microsoft_resourceid, pod, container, namespace, resource)
	 (kube_pod_container_resource_limits{container != """", pod != """", job = ""kube-state-metrics""})
)
)unless (count by (pod, namespace, cluster, resource, microsoft_resourceid)
	(kube_pod_container_resource_limits{container != """", pod != """", job = ""kube-state-metrics""})
!= on (pod, namespace, cluster, microsoft_resourceid) group_left()
 sum by (pod, namespace, cluster, microsoft_resourceid)
 (kube_pod_container_info{container != """", pod != """", job = ""kube-state-metrics""}) 
)

)* on (namespace, pod, cluster, microsoft_resourceid) group_left (node, created_by_kind, created_by_name)
(
	kube_pod_info{pod != """", job = ""kube-state-metrics""}
)",,,,,,,,,"This Prometheus rule calculates the total resource limits for each pod in a cluster, but only if there is exactly one container per pod. It considers pods with multiple containers as invalid and excludes them from the calculation. The result is then linked to additional information about each pod, such as its node, creation kind, and creation name.

In simpler terms: ""For each pod, sum up the resource limits of its containers, but only if the pod has exactly one container, and provide additional details like the node where the pod is running."""
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:controller_resource_limit:sum,"sum by (cluster, namespace, created_by_name, created_by_kind, node, resource, microsoft_resourceid) (
ux:pod_resource_limit:sum
)",,,,,,,,,"Calculate the total resource limits for all pods in each unique combination of cluster, namespace, creator, node, resource type, and Microsoft resource ID."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:controller_pod_phase_count:sum,"sum by (cluster, phase, node, created_by_kind, created_by_name, namespace, microsoft_resourceid) ( (
(kube_pod_status_phase{job=""kube-state-metrics"",pod!=""""})
 or (label_replace((count(kube_pod_deletion_timestamp{job=""kube-state-metrics"",pod!=""""}) by (namespace, pod, cluster, microsoft_resourceid) * count(kube_pod_status_reason{reason=""NodeLost"", job=""kube-state-metrics""} == 0) by (namespace, pod, cluster, microsoft_resourceid)), ""phase"", ""terminating"", """", """"))) * on (pod, namespace, cluster, microsoft_resourceid) group_left (node, created_by_name, created_by_kind)
(
max by (node, created_by_name, created_by_kind, pod, namespace, cluster, microsoft_resourceid) (
kube_pod_info{job=""kube-state-metrics"",pod!=""""}
)
)
)",,,,,,,,,"Count the total number of pods in each phase (e.g., running, pending, succeeded, failed, etc.) across all clusters, nodes, namespaces, and Microsoft resource IDs, considering both existing pods and terminated pods that were lost by a node, while also taking into account the pod's creation metadata."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:cluster_pod_phase_count:sum,"sum by (cluster, phase, node, namespace, microsoft_resourceid) (
ux:controller_pod_phase_count:sum
)",,,,,,,,,"Show the total count of pods in each phase, grouped by cluster, phase, node, namespace, and Microsoft resource ID, based on the controller pod phase counts."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:node_cpu_usage:sum_irate,"sum by (instance, cluster, microsoft_resourceid) (
(1 - irate(node_cpu_seconds_total{job=""node"", mode=""idle""}[5m]))
)",,,,,,,,,"Calculate the average CPU usage rate over the last 5 minutes for each instance, cluster, and Microsoft resource ID, by subtracting the idle CPU time from the total CPU time and summing the results."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:node_memory_usage:sum,"sum by (instance, cluster, microsoft_resourceid) ((
node_memory_MemTotal_bytes{job = ""node""}
- node_memory_MemFree_bytes{job = ""node""} 
- node_memory_cached_bytes{job = ""node""}
- node_memory_buffers_bytes{job = ""node""}
))",,,,,,,,,"The total used memory for each instance, cluster, and Microsoft resource ID, calculated by subtracting the free, cached, and buffer memory from the total memory."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:node_network_receive_drop_total:sum_irate,"sum by (instance, cluster, microsoft_resourceid) (irate(node_network_receive_drop_total{job=""node"", device!=""lo""}[5m]))",,,,,,,,,"Show the total rate of dropped incoming network packets for each node over the last 5 minutes, grouped by instance, cluster, and Microsoft resource ID, but exclude the loopback interface."
UXRecordingRulesRuleGroup - aks-sde-eastus-stage,eastus,UX Recording Rules for Linux,ux:node_network_transmit_drop_total:sum_irate,"sum by (instance, cluster, microsoft_resourceid) (irate(node_network_transmit_drop_total{job=""node"", device!=""lo""}[5m]))",,,,,,,,,"Show the total rate of dropped outgoing network packets for each node over the last 5 minutes, grouped by instance, cluster, and Microsoft resource ID, but exclude the loopback interface (lo)."
CBSRuleGroup,eastus,CBS Alerts,,"sum(increase(cbs_http_requests_total{job=""cloud-backup-service"",code!~""2..|4..""}[5m])) by (cluster) > 0","{'metric': 'cbs_http_requests_total', 'service': 'cbs', 'pod': 'cbs'}",CBSHttp5XXErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSHttp5XXErrorCritical has fired on #$.labels.cluster#', 'IcM.Description': 'HTTP 5XX errors #$.labels.error# in CBS - region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSHttp5XXErrorCritical', 'description': 'There were #$.labels.value# HTTP 5XX errors for in CBS in the last 5 minutes, please investigate', 'summary': 'High amount of CBS HTTP 5XX Failures.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}","Trigger an alert when there are any non-successful HTTP requests (status codes other than 200 or 400 range) to the cloud backup service within a 5-minute window, grouped by cluster."
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_storage_account_key_rotation_error_gauge[5m]) > 0,"{'metric': 'cbs_storage_account_key_rotation_error_gauge', 'service': 'cbs', 'pod': 'cbs'}",CBSKeyRotationErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSKeyRotationErrorCritical has fired on #$.labels.cluster#', 'IcM.Description': 'Storage Account Key Rotation failed #$.labels.error# - region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSKeyRotationErrorCritical', 'description': 'Storage Account Key Rotation failed, please investigate', 'summary': 'Storage Account Key Rotation failed.All Backup operations would fail now.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",There has been an error rotating a storage account key in the last 5 minutes.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_ontap_key_rotation_error_gauge[5m]) > 0,"{'metric': 'cbs_ontap_key_rotation_error_gauge', 'service': 'cbs', 'pod': 'cbs'}",CBSOntapKeyRotationErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSOntapKeyRotationErrorCritical has fired on #$.labels.cluster#', 'IcM.Description': 'Storage Ontap Key Rotation failed #$.labels.error# - region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSOntapKeyRotationErrorCritical', 'description': 'Storage Ontap Key Rotation failed, please investigate', 'summary': 'Storage Ontap Key Rotation failed.All Ontap calls would fail now.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",There has been an increase in key rotation errors for the CBS Ontap system within the last 5 minutes.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_ontap_errors[5m]) > 1,"{'metric': 'cbs_ontap_errors', 'service': 'cbs', 'pod': 'cbs'}",CBSOntapErrors,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSOntapErrors has fired on #$.labels.cluster#', 'IcM.Description': 'Ontap Connectivity Errors #$.labels.error# - region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod# (node #$.labels.node#)', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSOntapErrors', 'description': 'There were #$.labels.value# Ontap Errors occoured, please investigate', 'summary': 'Ontap Connectivity Errors', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",Trigger an alert if there is more than one increase in NetApp Ontap errors within a 5-minute time frame.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_sde_msg_publish_failure_error_gauge[60m]) > 100,"{'metric': 'cbs_sde_msg_publish_failure_error_gauge', 'service': 'cbs', 'pod': 'cbs'}",CBSSdeMsgFailureError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSSdeMsgFailureError has fired on #$.labels.cluster#', 'IcM.Description': 'Alert on CBS external msg publish failures #$.labels.error# - region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSSdeMsgFailureError', 'description': 'Number of times error occurred while publishing a msg to rabbitmq by metrics pod to SDE queue for a particular operation', 'summary': 'Alert on CBS external msg publish failures', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",The number of times the message publishing failure error occurred has increased by more than 100 instances over the last 60 minutes.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_hyperscaler_connectivity_error_gauge[24h]) > 5,"{'metric': 'cbs_hyperscaler_connectivity_error_gauge', 'service': 'cbs', 'pod': 'cbs'}",CBSHypercalerError,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSHypercalerError has fired on #$.labels.cluster#', 'IcM.Description': 'Hyperscaler connectivity error Alert #$.labels.error# -  region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSHypercalerError', 'description': 'User operations might fail.This may be a config issue, please investigate', 'summary': 'Hyperscaler connectivity error Alert', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",There has been an increase of more than 5 hyperscaler connectivity errors in the last 24 hours.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_gs_connectivity_error_gauge[24h]) > 50,"{'metric': 'cbs_gs_connectivity_error_gauge', 'service': 'cbs', 'pod': 'cbs'}",CBSGsError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSGsError has fired on #$.labels.cluster#', 'IcM.Description': 'Connectivity Issues with global scheduler #$.labels.error# region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSGsError', 'description': 'Connectivity Issues with global scheduler, please investigate', 'summary': 'Alert on GS connectivity issues', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",The number of times the connectivity error occurred has increased by more than 50 instances over the last 24 hours.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_adc_connectivity_error_gauge[60m]) > 20,"{'metric': 'cbs_adc_connectivity_error_gauge', 'service': 'cbs', 'pod': 'cbs'}",CBSAdcError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSAdcError has fired on #$.labels.cluster#', 'IcM.Description': 'ADC calls are failing with request timeout, connectivity failed #$.labels.error# - region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSAdcError', 'description': 'ADC calls are failing with request timeout, connectivity failed, please investigate', 'summary': 'Alert on ADC connectivity issues', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",The number of times the ADC connectivity error occurred has increased by more than 20 instances over the last 60 minutes.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_cvi_connectivity_error_gauge[60m]) > 50,"{'metric': 'cbs_cvi_connectivity_error_gauge', 'service': 'cbs', 'pod': 'cbs'}",CBSCviError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSCviError has fired on #$.labels.cluster#', 'IcM.Description': 'CVI calls may fail with request timeout, connectivity failed, too many requests failure #$.labels.error# region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSCviError', 'description': 'CVI calls may fail with request timeout, connectivity failed, too many requests failure, please investigate', 'summary': 'Alert on CVI connectivity issues', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",The number of new connectivity errors in the last 60 minutes has increased by more than 50.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_postgres_db_connection_error[60m]) > 50,"{'metric': 'cbs_postgres_db_connection_error', 'service': 'cbs', 'pod': 'cbs'}",CBSPostgresError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSPostgresError has fired on #$.labels.cluster#', 'IcM.Description': 'Connectivity Issue with Postgres #$.labels.error# - region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSPostgresError', 'description': 'Connectivity Issue with Postgres, please investigate', 'summary': 'Alert on Postgres connectivity issues', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",There has been an increase of more than 50 PostgreSQL database connection errors in the last 60 minutes.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_blob_store_connectivity_error_gauge[24h]) > 100,"{'metric': 'cbs_blob_store_connectivity_error_gauge', 'service': 'cbs', 'pod': 'cbs'}",CBSBlobStoreError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSBlobStoreError has fired on #$.labels.cluster#', 'IcM.Description': 'Worker fails to connect to blob store, backup creation is failing.Metrics are not able to connect to blob store, it might lead to loss of billing #$.labels.error# region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSBlobStoreError', 'description': 'Worker fails to connect to blob store, backup creation is failing.Metrics are not able to connect to blob store, it might lead to loss of billing, please investigate', 'summary': 'Alerts on blob store failure', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",There has been an increase of more than 100 connectivity errors to the CBS blob store in the last 24 hours.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_internal_msg_publish_failure_error_gauge[24h]) > 50,"{'metric': 'cbs_internal_msg_publish_failure_error_gauge', 'service': 'cbs', 'pod': 'cbs'}",CBSInternalMsgPublishError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSInternalMsgPublishError has fired on #$.labels.cluster#', 'IcM.Description': 'CBS is publishing error msgs for each backup request/policy assignment/restore/delete backup etc #$.labels.error# - region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSInternalMsgPublishError', 'description': 'CBS is publishing error msgs for each backup request/policy assignment/restore/delete backup etc, please investigate', 'summary': 'Alert on CBS internal msg publish failures', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",The number of times internal message publishing failed due to errors has increased by more than 50 instances over the last 24 hours.
CBSRuleGroup,eastus,CBS Alerts,,increase(cbs_get_secret_from_keyvault_error_gauge[5m]) > 0,"{'metric': 'cbs_get_secret_from_keyvault_error_gauge', 'service': 'cbs', 'pod': 'cbs'}",CBSGetSecretFromKvError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CBSGetSecretFromKvError has fired on #$.labels.cluster#', 'IcM.Description': 'CBS encountered authentication issues while fetching secrets from Key Vault #$.labels.error# - region #$.labels.cluster# | node #$.labels.node# | pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cbs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CBSOntapKeyRotationErrorCritical', 'description': 'CBS encountered authentication issues while fetching secrets from Key Vault, please investigate', 'summary': 'CBS encountered authentication issues while fetching secrets from Key Vault', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/upKMXl34k/cloud-backup-service'}",There has been an increase in the number of errors when trying to retrieve a secret from Key Vault within the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_reader_worker_file_parsing_failure[5m]) > 0,"{'metric': 'fal_s3_reader_worker_file_parsing_failure', 'service': 'fal', 'pod': 'fal'}",FalS3ParsingFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3ParsingFailure has fired on #$.labels.region#', 'IcM.Description': 'Connection failure between S3 reader pod and azure service bus', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3ParsingFailure', 'description': 'Connection failure between S3 reader pod and azure service bus', 'summary': 'Connection failure between S3 reader pod and azure service bus'}",There has been an increase in file parsing failures by the S3 reader worker within the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_secret_or_certificate_failure_count[5m]) > 2,"{'metric': 'fal_s3_secret_or_certificate_failure_count', 'service': 'fal', 'pod': 'fal'}",FalS3CertFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3CertFailure has fired on #$.labels.region#', 'IcM.Description': 'Failure has occurred in secret or certificate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3CertFailure', 'description': 'Failure has occurred in secret or certificate', 'summary': 'Failure has occurred in secret or certificate'}",There has been an increase of more than 2 failures in fetching S3 secrets or certificates in the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_k8s_api_failure_count[5m]) > 3,"{'metric': 'fal_s3_k8s_api_failure_count', 'service': 'fal', 'pod': 'fal'}",FalS3ApiFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3ApiFailure has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered when attempting a K8s API Failure', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3ApiFailure', 'description': 'Failure encountered when attempting a K8s API Failure', 'summary': 'Failure encountered when attempting a K8s API Failure'}",The number of Kubernetes API failures in the last 5 minutes has increased by more than 3.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_set_key_on_redis_error[5m]) > 4,"{'metric': 'fal_s3_set_key_on_redis_error', 'service': 'fal', 'pod': 'fal'}",FalS3SetKeyRedisError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3SetKeyRedisError has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while setting key on redis', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3SetKeyRedisError', 'description': 'Failure encountered while setting key on redis', 'summary': 'Failure encountered while setting key on redis'}",The number of times the S3 set key operation failed and was retried on Redis within the last 5 minutes has increased by more than 4 occurrences.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_get_value_from_redis_error[5m]) > 0,"{'metric': 'fal_s3_get_value_from_redis_error', 'service': 'fal', 'pod': 'fal'}",FalS3GetValueFromRedisError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3GetValueFromRedisError has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while getting value from redis', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3GetValueFromRedisError', 'description': 'Failure encountered while getting value from redis', 'summary': 'Failure encountered while getting value from redis'}",There has been an increase in the number of times the S3 get value from Redis operation failed in the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,"increase(fal_s3_set_key_on_redis_error{operationName=""s3_reader_supervisor_set_key_on_redis_while_publish_event_to_asb_error""}[5m]) > 0","{'metric': 's3_reader_supervisor_set_key_on_redis_while_publish_event_to_asb_error', 'service': 'fal', 'pod': 'fal'}",FalS3SetKeyRedisPubAsbError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3SetKeyRedisPubAsbError has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while setting supervisor key on redis while publishing event to ASB', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3SetKeyRedisPubAsbError', 'description': 'Failure encountered while setting supervisor key on redis while publishing event to ASB', 'summary': 'Failure encountered while setting supervisor key on redis while publishing event to ASB'}",There has been an increase in the number of times the S3 reader supervisor failed to set a key on Redis while publishing an event to ASB in the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,"increase(fal_s3_set_key_on_redis_error{operationName=""s3_reader_supervisor_set_key_on_redis_for_volume_details""}[5m]) > 0","{'metric': 'fal_s3_set_key_on_redis_error', 'service': 'fal', 'pod': 'fal'}",FalS3SetKeyRedisVolumeError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3SetKeyRedisVolumeError has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while setting supervisor key on redis for volume details', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3SetKeyRedisVolumeError', 'description': 'Failure encountered while setting supervisor key on redis for volume details', 'summary': 'Failure encountered while setting supervisor key on redis for volume details'}","There has been an increase in the number of times the ""s3_reader_supervisor_set_key_on_redis_for_volume_details"" operation encountered a Redis error and set a key, within the last 5 minutes."
FalRuleGroup,eastus,FAL Alerts,,"increase(fal_s3_set_key_on_redis_error{operationName=""s3_reader_worker_set_key_redis_failure_while_updating_processed_key_count""}[5m]) > 100","{'metric': 'fal_s3_set_key_on_redis_error', 'service': 'fal', 'pod': 'fal'}",FalS3SetKeyRedisUpdateCountFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3SetKeyRedisUpdateCountFailure has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while setting worker key on redis while updating processed key count', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3SetKeyRedisUpdateCountFailure', 'description': 'Failure encountered while setting worker key on redis while updating processed key count', 'summary': 'Failure encountered while setting worker key on redis while updating processed key count'}",There has been an increase of more than 100 occurrences in the last 5 minutes where the s3 reader worker failed to update the processed key count in Redis.
FalRuleGroup,eastus,FAL Alerts,,"increase(fal_s3_set_key_on_redis_error{operationName=""s3_reader_worker_set_key_redis_failure_index""}[5m]) > 7","{'metric': 'fal_s3_set_key_on_redis_error', 'service': 'fal', 'pod': 'fal'}",FalS3SetKeyRedisIndexFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3SetKeyRedisIndexFailure has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while setting worker key on redis index', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3SetKeyRedisIndexFailure', 'description': 'Failure encountered while setting worker key on redis index', 'summary': 'Failure encountered while setting worker key on redis index'}","There has been an increase of more than 7 occurrences in the last 5 minutes of errors when trying to set a key on Redis from the S3 reader worker, specifically for the operation of setting a key for a Redis failure index."
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_fetch_s3_details_from_cvi_error[5m]) > 0,"{'metric': 'fal_s3_fetch_s3_details_from_cvi_error', 'service': 'fal', 'pod': 'fal'}",FalS3FetchDetailsCvi,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3FetchDetailsCvi has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered during fetching S3 details from CVI from supervisor', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3FetchDetailsCvi', 'description': 'Failure encountered during fetching S3 details from CVI from supervisor', 'summary': 'Failure encountered during fetching S3 details from CVI from supervisor'}",There has been an increase in errors when fetching S3 details from CVI in the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_volume_refresh_job_failure_count[10m]) > 0,"{'metric': 'fal_s3_volume_refresh_job_failure_count', 'service': 'fal', 'pod': 'fal'}",FalS3ApiVolRefreshFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3ApiVolRefreshFailure has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered when attempting a volume refresh job', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3ApiVolRefreshFailure', 'description': 'Failure encountered when attempting a volume refresh job', 'summary': 'Failure encountered when attempting a volume refresh job'}",There has been an increase in the number of times the S3 volume refresh job has failed in the last 10 minutes.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_host_to_bucket_uri_failure_count[5m]) > 0,"{'metric': 'fal_s3_host_to_bucket_uri_failure_count', 'service': 'fal', 'pod': 'fal'}",FalS3HostBucketFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3HostBucketFailure has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while mapping uri from host to bucket', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3HostBucketFailure', 'description': 'Failure encountered while mapping uri from host to bucket', 'summary': 'Failure encountered while mapping uri from host to bucket'}",There has been an increase in the number of failures for S3 host to bucket URI requests over the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_fetch_file_list_from_s3_failure_count[5m]) > 3,"{'metric': 'fal_s3_fetch_file_list_from_s3_failure_count', 'service': 'fal', 'pod': 'fal'}",FalS3FetchFileList,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3FetchFileList has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while fetch file list from S3', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3FetchFileList', 'description': 'Failure encountered while fetch file list from S3', 'summary': 'Failure encountered while fetch file list from S3'}",There has been more than 3 increase in failures to fetch file lists from S3 in the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_fetch_file_content_from_s3_failure_count[5m]) > 3,"{'metric': 'fal_s3_fetch_file_content_from_s3_failure_count', 'service': 'fal', 'pod': 'fal'}",FalS3FetchFileContent,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3FetchFileContent has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while fetch file content from S3', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3FetchFileContent', 'description': 'Failure encountered while fetch file content from S3', 'summary': 'Failure encountered while fetch file content from S3'}",There have been more than 3 increases in failures to fetch file content from S3 in the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_delete_file_from_s3_reader_worker_failure_count[5m]) > 2,"{'metric': 'fal_s3_delete_file_from_s3_reader_worker_failure_count', 'service': 'fal', 'pod': 'fal'}",FalS3DeleteFileS3,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3DeleteFileS3 has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while deleting file from S3 in reader worker', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3DeleteFileS3', 'description': 'Failure encountered while deleting file from S3 in reader worker', 'summary': 'Failure encountered while deleting file from S3 in reader worker'}",There has been more than 2 increase in failures of the S3 delete file operation from the reader worker within the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_decrement_key_on_redis_reader_worker_failure_count[5m]) > 9,"{'metric': 'fal_s3_decrement_key_on_redis_reader_worker_failure_count', 'service': 'fal', 'pod': 'fal'}",FalS3DecrementKeyRedis,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3DecrementKeyRedis has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while decrementing key on redis in reader worker', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3DecrementKeyRedis', 'description': 'Failure encountered while decrementing key on redis in reader worker', 'summary': 'Failure encountered while decrementing key on redis in reader worker'}",The number of times a Redis reader worker failed and decremented a key in S3 has increased by more than 9 instances within the last 5 minutes.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_delete_key_on_redis_reader_worker_failure_count[5m]) > 9,"{'metric': 'fal_s3_delete_key_on_redis_reader_worker_failure_count', 'service': 'fal', 'pod': 'fal'}",FalS3DeleteKeyRedis,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3DeleteKeyRedis has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered while deleting key on redis in reader worker', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3DeleteKeyRedis', 'description': 'Failure encountered while deleting key on redis in reader worker', 'summary': 'Failure encountered while deleting key on redis in reader worker'}",The number of times the S3 delete key operation failed on the Redis reader worker in the last 5 minutes has increased by more than 9 occurrences.
FalRuleGroup,eastus,FAL Alerts,,increase(fal_s3_receive_message_from_asb_error[5m]) > 9,"{'metric': 'fal_s3_receive_message_from_asb_error', 'service': 'fal', 'pod': 'fal'}",FalS3ReceiveMsgAsbError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3ReceiveMsgAsbError has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered during receiving message from azure service bus', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3ReceiveMsgAsbError', 'description': 'Failure encountered during receiving message from azure service bus', 'summary': 'Failure encountered during receiving message from azure service bus'}",The number of times an error occurred while receiving a message from Azure Service Bus to S3 in the last 5 minutes has increased by more than 9.
FalRuleGroup,eastus,FAL Alerts,,sum(increase(fal_s3_reader_worker_mdsd_send_failure[5m])) > 30,"{'metric': 'fal_s3_reader_worker_mdsd_send_failure', 'service': 'fal', 'pod': 'fal'}",FalS3ReaderWorkerMdsdSendFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3ReaderWorkerMdsdSendFailure has fired on #$.labels.region#', 'IcM.Description': 'Failure encountered when mdsd send failed during file processing', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3ReaderWorkerMdsdSendFailure', 'description': 'Failure encountered when mdsd send failed during file processing', 'summary': 'Failure encountered when mdsd send failed during file processing'}","In the last 5 minutes, the total number of times the S3 reader worker failed to send data to MDSD is more than 30."
FalRuleGroup,eastus,FAL Alerts,,"sum by(region, ownerid) (increase(fal_s3_num_files_generated_per_customer[10m]) > 0) and sum by(region, ownerid) (increase(fal_s3_num_logs_processed_per_customer[10m]) == 0)","{'metric': 'fal_s3_num_files_generated_per_customer', 'service': 'fal', 'pod': 'fal'}",FalS3LogsGeneratingButNotProcessing,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FalS3LogsGeneratingButNotProcessing has fired on #$.labels.region#', 'IcM.Description': 'Logs are generating but not processing for FAL service #$.labels.region# | pod #$.labels.pod# | ownerid #$.labels.ownerid#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/fal-s3-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'FalS3LogsGeneratingButNotProcessing', 'description': 'Alert for FAL S3 logs generating but not processing', 'summary': 'Logs are generating but not processing for FAL service'}","""Alert when a customer has generated new files in the last 10 minutes, but hasn't processed any logs in the same time period, grouped by region and owner ID."""
ONTAPRuleGroup,eastus,ONTAP alerts,,"(cluster_new_status == 0) * on(ontap_cluster) group_left(upgrade_status) image_upgrade_labels{upgrade_status!=""in_progress""}","{'metric': 'cluster_new_status', 'service': 'Ontap'}",ClusterDegraded,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert ClusterDegraded has fired on #$.labels.region# - #$.labels.ontap_cluster#', 'IcM.Description': 'Cluster is in degraded state from last 10 mins (region #$.labels.region#) (cluster #$.labels.ontap_cluster#) (namespace #$.labels.namespace#)'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}",{'description': 'Cluster is in degraded state from last 10 mins (region #$.labels.region#) (cluster #$.labels.ontap_cluster#)'},"""Show me clusters that have a new status of 0 (meaning they are not currently being upgraded), but also show me their current upgrade status, and only include clusters where an image upgrade is not currently in progress."""
ONTAPRuleGroup,eastus,ONTAP alerts,,"(node_new_status == 0) * on(ontap_cluster) group_left(upgrade_status) image_upgrade_labels{upgrade_status!=""in_progress""}","{'metric': 'node_new_status', 'service': 'Ontap'}",NodeDegraded,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NodeDegraded has fired on #$.labels.region# - #$.labels.ontap_cluster#', 'IcM.Description': 'Node is in degraded state from last 10 mins (region #$.labels.region#) (node #$.labels.node#)'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}",{'description': 'Node is in degraded state from last 10 mins (region #$.labels.region#) (node #$.labels.node#) (namespace #$.labels.namespace#)'},"""Create a new metric when the node status is unknown (0) for an Ontap cluster, and there's an image upgrade that is not currently in progress, grouping the results by upgrade labels."""
ONTAPRuleGroup,eastus,ONTAP alerts,,nic_new_status == 0,"{'metric': 'nic_new_status', 'service': 'Ontap'}",PortDown,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert PortDown has fired on #$.labels.region# - #$.labels.ontap_cluster# #$.labels.node# #$.labels.nic#', 'IcM.Description': 'Ethernet Port (nic #$.labels.nic#) is down on (region #$.labels.region#) (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Ethernet Port (nic #$.labels.nic#) is down on (region #$.labels.region#) (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#)'},The network interface is currently down.
ONTAPRuleGroup,eastus,ONTAP alerts,,"count(disk_labels{failed=""true""}) by (node,ontap_cluster,region,kubernetes_pod_name) > 1","{'metric': 'disk_labels', 'service': 'Ontap'}",MultipleDiskFailure,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert MultipleDiskFailure has fired on #$.labels.region# - #$.labels.ontap_cluster#', 'IcM.Description': 'Multiple disk failure on (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Multiple disk failure on (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'},"Alert when there is more than one failed disk on any node, Ontap cluster, region, or Kubernetes pod."
ONTAPRuleGroup,eastus,ONTAP alerts,,aggr_new_status != 1,"{'metric': 'aggr_new_status', 'service': 'Ontap'}",AggregateNotOnline,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert AggregateNotOnline has fired on #$.labels.region# - #$.labels.ontap_cluster# - #$.labels.aggr# ', 'IcM.Description': 'Aggregate is not online on (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Aggregate is not online on (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'},"""Trigger an alert when the aggregated new status is not okay (i.e., not equal to 1), indicating a potential issue or error."""
ONTAPRuleGroup,eastus,ONTAP alerts,,aggr_space_used_percent > 70,"{'metric': 'aggr_space_used_percent', 'service': 'Ontap'}",AggregateSpaceUsageHigh,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert AggregateSpaceUsageHigh has fired on #$.labels.region# - #$.labels.ontap_cluster# - #$.labels.aggr#', 'IcM.Description': 'High space usage on aggregate (aggr #$.labels.aggr#) (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#) (kubernetes_pod_name #$.labels.kubernetes_pod_name#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'High space usage on aggregate (aggr #$.labels.aggr#) (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#) (kubernetes_pod_name #$.labels.kubernetes_pod_name#)'},The aggregated used space percentage is greater than 70%.
ONTAPRuleGroup,eastus,ONTAP alerts,,aggr_space_physical_used_percent > 70,"{'metric': 'aggr_space_used_percent', 'service': 'Ontap'}",AggregatePhysicalSpaceUsageHigh,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert AggregatePhysicalSpaceUsageHigh has fired on #$.labels.region# - #$.labels.ontap_cluster# #$.labels.aggr#', 'IcM.Description': 'High physical space usage on aggregate (aggr #$.labels.aggr#) (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#) (kubernetes_pod_name #$.labels.kubernetes_pod_name#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'High physical space usage on aggregate (aggr #$.labels.aggr#) (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#) (kubernetes_pod_name #$.labels.kubernetes_pod_name#)'},The physical space used on the system exceeds 70%.
ONTAPRuleGroup,eastus,ONTAP alerts,,"count by (node,ontap_cluster,region) (ems_events{message=""callhome.nvram.cecc""} or ems_events{message=""nvdimm.excessive.cecc.errs""}) == 2","{'metric': 'ems_events', 'service': 'Ontap'}",NVRAMDegraded,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NVRAMDegraded has fired on #$.labels.region# - #$.labels.ontap_cluster# ', 'IcM.Description': 'Degraded NVRAM detected on (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Degraded NVRAM detected on (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'},"Trigger an alert when there are exactly 2 error events related to either ""callhome.nvram.cecc"" or ""nvdimm.excessive.cecc.errs"" messages, grouped by node, Ontap cluster, and region."
ONTAPRuleGroup,eastus,ONTAP alerts,,"count by (node,ontap_cluster,region) (ems_events{message=""cf.multidisk.fatalproblem""}) > 0","{'metric': 'ems_events', 'service': 'Ontap'}",ChelsioT6,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert ChelsioT6 Failure has fired on  #$.labels.region# - #$.labels.ontap_cluster#', 'IcM.Description': 'T6 port failure detected on (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'T6 port failure detected on (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'},"Trigger an alert when there is at least one EMS event with a message indicating a fatal problem on multiple disks, grouped by node, NetApp OntAP cluster, and region."
ONTAPRuleGroup,eastus,ONTAP alerts,,"count by (node,ontap_cluster,region) (ems_events{message=""object.store.full""} or ems_events{message=""objstore.host.unresolvable""}) == 2","{'metric': 'ems_events', 'service': 'Ontap'}",CoolTierObject,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CoolTierObject Failure has fired on #$.labels.region# - #$.labels.ontap_cluster#', 'IcM.Description': 'Cool Tier Object store issue detected on (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Cool Tier Object store issue detected on (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'},"Trigger an alert when there are exactly 2 occurrences of either ""object store full"" or ""object store host unresolvable"" events on the same node, Ontap cluster, and region."
ONTAPRuleGroup,eastus,ONTAP alerts,,node_failed_fan > 0,"{'metric': 'node_failed_fan', 'service': 'Ontap'}",OntapChassisFan,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert OntapChassisFan Failure has fired #$.labels.region# - #$.labels.ontap_cluster# #$.labels.serial#', 'IcM.Description': 'A Chassis fan warning has triggered for (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'A Chassis fan warning has triggered for (node #$.labels.node#) (ontap_cluster #$.labels.ontap_cluster#) (region #$.labels.region#)'},"The number of failed fans on a node is greater than zero, indicating at least one fan has failed."
ONTAPRuleGroup,eastus,ONTAP alerts,,"ems_evemts{message=""Nblade.vldb.Timeout""} and ignoring(upgrade_status) (image_upgrade_labels{upgrade_status!=""completed""}) > 0","{'metric': 'ems_events', 'service': 'Ontap'}",OntapVLDBLookupFailure,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VLDB Timeout has fired in region #$.labels.region# on cluster #$.labels.ontap_cluster#', 'IcM.TsgId': 'TSG link here 3', 'IcM.Description': 'VLDB Lookup failure detected in #$.labels.ontap_cluster#. This results in a NFS/CIFS Giveup on an operation. Visit [this](https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/de3ui3ctluxvka/ontap3a-data-plane-slos?orgId=1) dashboard for more information'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'VLDB Lookup failure detected in #$.labels.ontap_cluster#. This results in a NFS/CIFS Giveup on an operation. Visit [this](https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/de3ui3ctluxvka/ontap3a-data-plane-slos?orgId=1) dashboard for more information', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/de3ui3ctluxvka/ontap3a-data-plane-slos?orgId=1'}","""Alert when there are Nblade.vldb.Timeout events and there is at least one image that has not completed an upgrade, ignoring the current upgrade status."""
ONTAPRuleGroup,eastus,ONTAP alerts,,"sum(nic_rx_crc_errors) by (ontap_cluster, nic, node, region) > 5000","{'metric': 'nic_rx_crc_errors', 'service': 'Harvest'}",OntapHighCRCErrorRate,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Ontap high CRC error rate for Cluster (cluster #$.labels.ontap_cluster#) in region (region #$.labels.region#)', 'IcM.TsgId': 'https://eng.ms/cid/e2d1871b-35d1-4f3d-ac34-8b2d3cc79531/fid/88049a92c4dc2b3bc90d368ab0534a5d69a9e33b60b6e0b63b977f7b0a145d71', 'IcM.Description': 'High CRC count on node (node #$.labels.node#) and port (port #$.labels.nic#)  detected in Harvest on (region #$.labels.region#) (ontap_cluster #$.labels.ontap_cluster#). Refer to [this](https://eng.ms/cid/e2d1871b-35d1-4f3d-ac34-8b2d3cc79531/fid/88049a92c4dc2b3bc90d368ab0534a5d69a9e33b60b6e0b63b977f7b0a145d71) to begin troubleshooting the issue'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'High CRC count on node (node #$.labels.node#) detected in Harvest on (region #$.labels.region#) (ontap_cluster #$.labels.ontap_cluster#) port (port #$.labels.nic#).'},"The total number of CRC errors received on network interfaces is greater than 5,000 for any combination of cluster, network card, node, and region."
ONTAPRuleGroup,eastus,ONTAP alerts,,"sum(nic_link_up_to_downs) by (ontap_cluster, nic, node, region) > 20","{'metric': 'nic_link_up_to_downs', 'service': 'Harvest'}",OntapHighUpDownEvents,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Ontap high number of UP DOWN events for Cluster (cluster #$.labels.ontap_cluster#) in region (region #$.labels.region#)', 'IcM.TsgId': 'https://eng.ms/cid/e2d1871b-35d1-4f3d-ac34-8b2d3cc79531/fid/88049a92c4dc2b3bc90d368ab0534a5d69a9e33b60b6e0b63b977f7b0a145d71', 'IcM.Description': 'High UP DOWN events on node (node #$.labels.node#) and port (port #$.labels.nic#)  detected in Harvest on (region #$.labels.region#) (ontap_cluster #$.labels.ontap_cluster#). Refer to [this](https://eng.ms/cid/e2d1871b-35d1-4f3d-ac34-8b2d3cc79531/fid/88049a92c4dc2b3bc90d368ab0534a5d69a9e33b60b6e0b63b977f7b0a145d71) to begin troubleshooting the issue'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'High number of UP DOWN events on node (node #$.labels.node#) detected in Harvest on (region #$.labels.region#) (ontap_cluster #$.labels.ontap_cluster#) port (port #$.labels.nic#).'},"The total number of times a network interface has transitioned from an up to a down state, grouped by storage cluster, network interface, node, and region, exceeds 20 occurrences."
ONTAPRuleGroup,eastus,ONTAP alerts,,"sum(nic_rx_bytes{nic=~""e2a|e4a""}) by (node, ontap_cluster, region) * on(region, node, ontap_cluster) group_left(model) sum by (node, ontap_cluster, model, region) (node_labels{model=""aff-a700s""}) > 5368709120","{'metric': 'nic_rx_bytes', 'service': 'Ontap'}",OntapNicRxUsageA700s,True,4.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Nic Rx is high on node #$.labels.node# on LAG port (e2a/e4a) in region #$.labels.region#', 'IcM.TsgId': 'https://eng.ms/cid/e2d1871b-35d1-4f3d-ac34-8b2d3cc79531/fid/44c0ac08bcf124d7b85a0a0f4c6f7331bb382fef789c64fc5490370b9709a324', 'IcM.Description': 'A NIC RX usage warning has triggered for (node #$.labels.node#) on LAG port (e2a/e4a) on (ontap_cluster #$.labels.ontap_cluster#) in (region #$.labels.region#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'A NIC RX usage warning has triggered for (node #$.labels.node#) on LAG port (e2a/e4a) on (ontap_cluster #$.labels.ontap_cluster#) in (region #$.labels.region#)'},"Alert when the total received bytes on network interfaces 'e2a' or 'e4a' for a node in a specific region and Ontap cluster, that also has an 'aff-a700s' model label, exceeds 5 GB (5368709120 bytes)."
ONTAPRuleGroup,eastus,ONTAP alerts,,"sum(nic_tx_bytes{nic=~""e2a|e4a""}) by (node, ontap_cluster, region) * on(region, node, ontap_cluster) group_left(model) sum by (node, ontap_cluster, model, region) (node_labels{model=""aff-a700s""}) > 5368709120","{'metric': 'nic_tx_bytes', 'service': 'Ontap'}",OntapNicTxUsageA700s,True,4.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Nic Tx is high on node #$.labels.node# on LAG port (e2a/e4a) in region #$.labels.region#', 'IcM.TsgId': 'https://eng.ms/cid/e2d1871b-35d1-4f3d-ac34-8b2d3cc79531/fid/44c0ac08bcf124d7b85a0a0f4c6f7331bb382fef789c64fc5490370b9709a324', 'IcM.Description': 'A NIC TX usage warning has triggered for (node #$.labels.node#) on LAG port (e3a/e5a) on (ontap_cluster #$.labels.ontap_cluster#) in (region #$.labels.region#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'A NIC TX usage warning has triggered for (node #$.labels.node#) on LAG port (e2a/e4a) on (ontap_cluster #$.labels.ontap_cluster#) in (region #$.labels.region#)'},"Alert when the total transmitted bytes from network interfaces 'e2a' or 'e4a' on an 'aff-a700s' model node exceeds 5 GB, grouped by node, cluster, region, and model."
ONTAPRuleGroup,eastus,ONTAP alerts,,"sum(nic_rx_bytes{nic=~""e3a|e5a""}) by (node, ontap_cluster, region) * on(region, node, ontap_cluster) group_left(model) sum by (node, ontap_cluster, model, region) (node_labels{model=""aff-a800""}) > 13421772800","{'metric': 'nic_rx_bytes', 'service': 'Ontap'}",OntapNicRxUsageA800,True,4.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Nic Rx is high on node #$.labels.node# on LAG ports (e3a/e5a) in region #$.labels.region#', 'IcM.TsgId': 'https://eng.ms/cid/e2d1871b-35d1-4f3d-ac34-8b2d3cc79531/fid/44c0ac08bcf124d7b85a0a0f4c6f7331bb382fef789c64fc5490370b9709a324', 'IcM.Description': 'A NIC RX usage warning has triggered for (node #$.labels.node#) on the LAG ports (e3a/e5a) on (ontap_cluster #$.labels.ontap_cluster#) in (region #$.labels.region#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'A NIC RX usage warning has triggered for (node #$.labels.node#) on the LAG ports (e3a/e5a) on (ontap_cluster #$.labels.ontap_cluster#) in (region #$.labels.region#)'},"Alert when the total received bytes on network interfaces e3a or e5a, grouped by node, Ontap cluster, and region, is greater than 12 GB for nodes with model aff-a800."
ONTAPRuleGroup,eastus,ONTAP alerts,,"sum(nic_tx_bytes{nic=~""e3a|e5a""}) by (node, ontap_cluster, region) * on(region, node, ontap_cluster) group_left(model) sum by (node, ontap_cluster, model, region) (node_labels{model=""aff-a800""}) > 13421772800","{'metric': 'nic_tx_bytes', 'service': 'Ontap'}",OntapNicTxUsageA800,True,4.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Nic Tx is high on node #$.labels.node# on the LAG (e3a/e5a) port in region #$.labels.region#', 'IcM.TsgId': 'https://eng.ms/cid/e2d1871b-35d1-4f3d-ac34-8b2d3cc79531/fid/44c0ac08bcf124d7b85a0a0f4c6f7331bb382fef789c64fc5490370b9709a324', 'IcM.Description': 'A NIC TX usage warning has triggered for (node #$.labels.node#) on the LAG ports (e3a/e5a) on (ontap_cluster #$.labels.ontap_cluster#) in (region #$.labels.region#)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'A NIC TX usage warning has triggered for (node #$.labels.node#) on the LAG port (e3a/e5a) on (ontap_cluster #$.labels.ontap_cluster#) in (region #$.labels.region#)'},"Alert when the total transmitted bytes from network interfaces e3a or e5a on any node exceeds 12 GB, but only for nodes with a model of aff-a800. The alert is grouped by node, Ontap cluster, and region."
ONTAPRuleGroup,eastus,ONTAP alerts,,"(avg_over_time(node_total_ops[30m] offset 10m) > 1000 and avg_over_time(node_total_ops[10m]) == 0) * on(ontap_cluster) group_left(upgrade_status) image_upgrade_labels{upgrade_status!=""in_progress""}","{'metric': 'node_total_ops', 'service': 'Ontap'}",OntapIOPDropWarning,True,4.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'IOPS drop found on node #$.labels.node# in region #$.labels.region#', 'IcM.TsgId': 'https://eng.ms/cid/e2d1871b-35d1-4f3d-ac34-8b2d3cc79531/fid/8b31f10e07d36d66b28cb52700adaa3ce02c6d1917150b6f1a84336b5a3fdb3b', 'IcM.Description': 'An IOPS drop warning has triggered for (node #$.labels.node#) on (ontap_cluster #$.labels.ontap_cluster#) in (region #$.labels.region#)'}}]",{'autoResolved': False},{'description': 'An IOPS drop warning has triggered for (node #$.labels.node#) on (ontap_cluster #$.labels.ontap_cluster#) in (region #$.labels.region#)'},"""Trigger an alert when the average total operations over the last 30 minutes, offset by 10 minutes, is greater than 1000 and the average total operations over the last 10 minutes is exactly 0, but only for nodes that are part of a specific cluster and have not started an image upgrade, and also consider the current upgrade status."""
ONTAPRuleGroup,eastus,ONTAP alerts,,"(node_cpu_busy) * on (ontap_cluster) group_left(upgrade_status) image_upgrade_labels{upgrade_status!=""in_progress""} > 99","{'metric': 'node_cpu_usage', 'service': 'ONTAP'}",HighNodeCPUUsage,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Node High CPU Usage #$.labels.region# has fired on #$.labels.region#', 'IcM.TsgId': 'TSG link here 3', 'IcM.Description': 'High Node CPU usage (node_cpu_usage #$.labels.node_cpu_usage#) detected for node (node #$.labels.node#) on (region #$.labels.region#) (ontap_cluster #$.labels.ontap_cluster#). Refer to [this](https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/fduwxzn5a3chsd/harvest-diagnostics?orgId=1) dashboard for insight into Harvest pod health'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'High CPU usage (node_cpu_usage #$.labels.node_cpu_usage#) detected in Harvest for node (node #$.labels.node#) on (region #$.labels.region#) (ontap_cluster #$.labels.ontap_cluster#)'},"""Alert when the CPU usage of a node is greater than 99% and an image upgrade is not currently in progress, considering the cluster and upgrade status."""
HarvestRuleGroup,eastus,Harvest alerts,,"metadata_component_status{target!=""rwctx""} == 2","{'metric': 'metadata_componet_status', 'service': 'Harvest', 'team': 'Team-Argus'}",HarvestCollectorFailure,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Harvest Collector Failure Alert has fired on #$.labels.region#', 'IcM.TsgId': 'TSG link here', 'IcM.Description': 'Status of the Harvest Collector (target #$.labels.target#) is in failed state on (region #$.labels.region#) (ontap_cluster #$.labels.ontap_cluster#). Refer to [this](https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/fduwxzn5a3chsd/harvest-diagnostics?orgId=1) dashboard for insight into Harvest pod health'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Status of the Harvest Collector (target #$.labels.target#) is in failed state on (region #$.labels.region#) (ontap_cluster #$.labels.ontap_cluster#)'},The status of any metadata component (excluding rwctx) is in an error state.
HarvestRuleGroup,eastus,Harvest alerts,,metadata_target_status == 1,"{'metric': 'metadata_target_status', 'service': 'Harvest'}",HarvestTargetUnreachable,False,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Harvest Target Unreachable on #$.labels.region# has fired on #$.labels.region#', 'IcM.TsgId': 'TSG link here 2', 'IcM.Description': 'The target system monitored by Harvest is unreachable on (region #$.labels.region#) (ontap_cluster #$.labels.ontap_cluster#) (addr #$.labels.addr#) (instance #$.labels.instance#). Refer to [this](https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/fduwxzn5a3chsd/harvest-diagnostics?orgId=1) dashboard for insight into Harvest pod health'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'The target system monitored by Harvest is unreachable on (region #$.labels.region#) (ontap_cluster #$.labels.ontap_cluster#) (addr #$.labels.addr#) (instance #$.labels.instance#)'},"""Trigger an alert when the target status in metadata is set to 'unhealthy' (indicated by a value of 1)."""
HarvestRuleGroup,eastus,Harvest alerts,,"sum by (cluster, region) (kube_pod_container_status_last_terminated_reason{namespace=""ontap-monitoring"", reason=""OOMKilled""}) > 0","{'metric': 'kube_pod_container_status_last_terminated_reason', 'service': 'Harvest'}",HarvestPodOOMKilled,True,4.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Harvest Pods OOMKilled in #$.labels.cluster# has fired on #$.labels.region#', 'IcM.TsgId': 'TSG link here 4', 'IcM.Description': 'Harvest Pods with last terminated status of ""OOMKilled"" is greater than 0 in Cluster #$.labels.cluster#. This will result in a gap of metrics collected from ONTAP for the affected Pods. Refer to [this](https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/fduwxzn5a3chsd/harvest-diagnostics?orgId=1) dashboard for insight into Harvest pod health'}}]","{'autoResolved': True, 'timeToResolve': 'PT1M'}","{'description': 'Harvest Pods with last terminated status of ""OOMKilled"" is greater than 0 in Cluster #$.labels.cluster#. Refer to [this](https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/fduwxzn5a3chsd/harvest-diagnostics?orgId=1) dashboard for insight into Harvest pod health', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/fduwxzn5a3chsd/harvest-diagnostics?orgId=1'}","There have been one or more container terminations due to out-of-memory errors in the ""ontap-monitoring"" namespace, grouped by cluster and region."
HarvestRuleGroup,eastus,Harvest alerts,,"count(metadata_component_status{type=""collector""} == 0) by (ontap_cluster, region) == 1","{'metric': 'metadata_component_status', 'service': 'Harvest'}",HarvestUnableToCollect,True,4.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Harvest unable to collect from #$.labels.ontap_cluster# has fired on #$.labels.region#', 'IcM.TsgId': 'TSG link here 4', 'IcM.Description': 'Harvest is unable to collect metrics from ONTAP cluster #$.labels.ontap_cluster# in region #$.labels.region#. This will result in a gap of metrics collected from ONTAP for the affected cluster. Refer to [this](https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/fduwxzn5a3chsd/harvest-diagnostics?orgId=1) dashboard for insight into Harvest poller health'}}]","{'autoResolved': False, 'timeToResolve': 'PT1M'}","{'description': 'Harvest is unable to collect metrics from ONTAP cluster #$.labels.ontap_cluster# in region #$.labels.region#. This will result in a gap of metrics collected from ONTAP for the affected cluster. Refer to [this](https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/fduwxzn5a3chsd/harvest-diagnostics?orgId=1) dashboard for insight into Harvest poller health', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/fduwxzn5a3chsd/harvest-diagnostics?orgId=1'}",There is exactly one collector component in a healthy status for each combination of ONTAP cluster and region.
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"kube_node_status_condition{condition=""Ready"",status=""true""} == 0","{'metric': 'kube_node_status_condition', 'service': 'K8sNode'}",K8SNodeNotReady,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert K8SNodeNotReady has fired on #$.labels.cluster#', 'IcM.Description': 'Node status is NotReady, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/k8snodenotready'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Node status is NotReady, please investigate'}",The Kubernetes node is not ready.
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"count by (cluster) (kube_node_status_condition{condition=""Ready"",status=""true""} == 0) > 1 and (count by (cluster) (kube_node_status_condition{condition=""Ready"",status=""true""} == 0) / count by (cluster) (kube_node_status_condition{condition=""Ready"",status=""true""})) > 0.2","{'metric': 'kube_node_status_condition', 'service': 'K8sNode'}",K8SManyNodesNotReady,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert K8SManyNodesNotReady has fired on #$.labels.cluster#', 'IcM.Description': 'More than 1 nodes in the cluster are not ready, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/k8smanynodesnotready'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'More than 1 Kubernetes nodes are not ready, please investigate'}","""Alert when more than one node in a cluster is not ready and over 20% of nodes in the cluster are not ready."""
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"kube_pod_container_status_waiting_reason{reason=""CrashLoopBackOff"",container=""gs-core""} == 1","{'metric': 'kube_pod_container_status_waiting_reason', 'service': 'K8sPod'}",GsCorePodCrashing,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GsCorePodCrashing has fired on #$.labels.cluster#', 'IcM.Description': 'GS pod is crashing, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/gscore-crashing'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'GS pod is crashing, please investigate'}","There is at least one Kubernetes pod container named ""gs-core"" that has a waiting reason of ""CrashLoopBackOff"", indicating it is continuously crashing and restarting."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"kube_pod_container_status_waiting_reason{reason=~""CrashLoopBackOff|ContainerCreating|Error|ImagePullBackOff|ErrImagePull|CreateContainerConfigError"", container=~""rabbitmq"", namespace=""rabbitmq""} == 1","{'metric': 'kube_pod_container_status_waiting_reason', 'service': 'K8sPod'}",RabbitMQPodsCrashing,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitMQPodsCrashing has fired on #$.labels.cluster#', 'IcM.Description': 'Rabbitmq pod is crashing, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/rabbitmq-prometheus-crashing'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Rabbitmq pod is crashing, please investigate'}","There is at least one RabbitMQ container in the 'rabbitmq' namespace that is waiting due to a problem, such as crashing and restarting repeatedly, still being created, encountering an error, failing to pull its image, having an error pulling its image, or having a configuration error creating the container."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"sum(up{job!~""resource-provider.*""}) by (cluster,job) == 0","{'metric': 'up', 'service': 'K8sPod'}",K8sEndpointDown,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert K8sEndpointDown has fired on #$.labels.cluster# for job #$.labels.job#', 'IcM.Description': 'kubernetes endpoint is down, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/kube-endpoint-not-available'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'kubernetes endpoint is down, please investigate'}","Show me clusters and jobs where all instances are down, excluding any job names that contain ""resource-provider"", by checking if the total number of up instances is zero."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"kube_pod_container_status_waiting_reason{reason=""CrashLoopBackOff"",container=""prometheus-server"",namespace=""rabbitmq""} == 1","{'metric': 'kube_pod_container_status_waiting_reason', 'service': 'K8sPod'}",RabbitmqPrometheusServerCrashing,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitmqPrometheusServerCrashing has fired on #$.labels.cluster#', 'IcM.Description': 'Rabbimtmq prometheus is down, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/rabbitmq-prometheus-crashing'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Rabbimtmq prometheus is down, please investigate'}","There is a container named ""prometheus-server"" in the ""rabbitmq"" namespace that has entered a crash loop and is waiting to be retried, indicating that it has crashed repeatedly and is currently backing off before attempting to restart again."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"sum by (cluster) (kube_pod_container_status_ready{container=""rabbitmq"", namespace=""rabbitmq""}) == 1","{'metric': 'kube_pod_container_status_waiting_reason', 'service': 'K8sPod'}",RabbitmqQuorumDown,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitmqQuorumDown has fired on #$.labels.cluster#', 'IcM.Description': 'Rabbimtmq Quorum is down, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/rabbitmq-quorum-down'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Rabbimtmq Quorum is down, please investigate'}",The number of ready RabbitMQ containers in each cluster is exactly 1.
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"kube_pod_container_status_waiting_reason{reason=""CrashLoopBackOff"",container=""heimdall""} == 1","{'metric': 'kube_pod_container_status_waiting_reason', 'service': 'K8sPod'}",HeimdallCrashing,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HeimdallCrashing has fired on #$.labels.cluster#', 'IcM.Description': 'Heimdall pod is down, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/heimdall-crashing'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Heimdall pod is down, please investigate'}","There is a container named ""heimdall"" in a pod that has entered a ""CrashLoopBackOff"" state, indicating it keeps crashing and restarting, with this condition currently being true for at least one instance."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,kube_node_spec_unschedulable == 1,"{'metric': 'kube_node_spec_unschedulable', 'service': 'K8sNode'}",K8SNodeIsCordoned,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert K8SNodeIsCordoned has fired on #$.labels.cluster#', 'IcM.Description': 'Scheduling has been disabled on the node, no new pods are scheduled on this node', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/observability/monitors/kubernetes/k8snodeiscordoned'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Scheduling has been disabled on the node, no new pods are scheduled on this node'}","The node is marked as unschedulable, meaning it cannot be used to run new pods."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"count by (cluster) (up{job=""kube-state-metrics""} == 0)","{'metric': 'up', 'service': 'K8sStateMetrics'}",K8SKubeStateMetricsExporterDown,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert K8SKubeStateMetricsExporterDown has fired on #$.labels.cluster#', 'IcM.Description': 'Kubernetes State Metrics Exporter is down', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/observability/monitors/kubernetes/k8skubestatemetricsexporterdown'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Kubernetes State Metrics Exporter is down'},"Count the number of clusters where the kube-state-metrics job is not running (i.e., its ""up"" status is 0), grouped by cluster."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"(kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 30m >= 1) and ignoring(reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=""OOMKilled""}[30m]) == 1","{'metric': 'kube_pod_container_status_restarts_total', 'service': 'K8sPod'}",KubePodOOMKilled,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert KubePodOOMKilled has fired on #$.labels.cluster# for Pod #$.labels.pod#', 'IcM.Description': 'Pod is hitting OOMKilled', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/kube-endpoint-not-available'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Pod is hitting OOMKilled'},"""Alert when a container has restarted at least once in the last 30 minutes and the reason for the last termination was 'Out of Memory' (OOMKilled), ignoring any other reasons."""
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"kube_pod_container_status_waiting_reason{reason=~""CrashLoopBackOff|ContainerCreating|Error|ImagePullBackOff|ErrImagePull|CreateContainerConfigError"", container=~""file-logs-azure-pusher|file-logs-reader-worker|file-logs-reader-supervisor""} == 1","{'metric': 'kube_pod_container_status_waiting_reason', 'service': 'K8sPod'}",FALPodsCrashing,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FALPodsCrashing has fired on #$.labels.cluster#', 'IcM.Description': 'FAL pods are crashing', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/heimdall-crashing'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'FAL pods are crashing'},"The Kubernetes pod contains a container that is either waiting to be created, has crashed and is waiting to restart, or has encountered an error while pulling its image, specifically for containers named ""file-logs-azure-pusher"", ""file-logs-reader-worker"", or ""file-logs-reader-supervisor""."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"kube_pod_container_status_waiting_reason{reason=~""CrashLoopBackOff|ContainerCreating|Error|ImagePullBackOff|ErrImagePull|CreateContainerConfigError"", container=~""cloud-volumes-service|cloud-volumes-infrastructure""} == 1","{'metric': 'kube_pod_container_status_waiting_reason', 'service': 'K8sPod'}",CloudVolumesPodsCrashing,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CloudVolumesPodsCrashing has fired on #$.labels.cluster#', 'IcM.Description': 'Cloud Volumes pods are crashing', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/kube-state-metrics-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Cloud Volumes pods are crashing'},"Trigger an alert when a Kubernetes pod container is in a waiting state due to one of the following reasons: CrashLoopBackOff, ContainerCreating, Error, ImagePullBackOff, ErrImagePull, or CreateContainerConfigError, and the container name contains either ""cloud-volumes-service"" or ""cloud-volumes-infrastructure""."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"sum by (cluster, namespace, pod) (container_memory_working_set_bytes{container!="""", container!=""pod"", container!=""sde-prometheus-server"", container!=""prometheus-postgres-exporter"", container!~""fluent-bit.*"", container!~""fluentd.*"", container!=""prometheus-alertmanager"", namespace!=""kube-system"", namespace!=""ontap-monitoring"", namespace!=""monitoring""}) / max by (cluster, namespace, pod) (kube_pod_container_resource_limits{resource=""memory""}) * 100 > 95","{'metric': 'container_memory_working_set_bytes', 'service': 'K8sPod'}",HighPodMemoryUsage,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighPodMemoryUsage has fired on #$.labels.cluster# for Pod #$.labels.pod#', 'IcM.Description': 'Pod memory usage exceeded 95% for Pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/kube-state-metrics-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Pod memory usage exceeded 95%'},"""Alert when the average memory usage of containers in a pod exceeds 95% of the maximum allowed memory limit, excluding certain containers and namespaces like prometheus-server, postgres-exporter, fluent-bit, fluentd, alertmanager, kube-system, ontap-monitoring, and monitoring."""
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"rate(container_cpu_usage_seconds_total{container!="""", container!=""POD"", container=~""file-access-logs-azure-pusher|file-access-logs-subworker|file-access-logs-supervisor|file-access-logs-worker|faal-rabbitmq|faal-mongodb|cloud-backup-metrics|cloud-backup-service|cloud-backup-worker|gs-session-mgr|gs-dispatcher|gs-core|rabbitmq""}[10m]) > 0.9","{'metric': 'container_cpu_usage_seconds_total', 'service': 'K8sPod'}",HighCpuUsage,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighCpuUsage has fired on #$.labels.cluster# for Pod #$.labels.pod#', 'IcM.Description': 'High CPU usage detected for Pod #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/kube-state-metrics-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'High CPU usage detected'},"The CPU usage rate of specific containers (including file-access-logs, cloud-backup, and gs services) over the last 10 minutes is greater than 90%."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"kube_pod_container_status_waiting_reason{reason=~""CrashLoopBackOff|ContainerCreating|Error|ImagePullBackOff|ErrImagePull|CreateContainerConfigError"", container=~""prometheus-server"", namespace=""faal-rabbitmq""} == 1","{'metric': 'kube_pod_container_status_waiting_reason', 'service': 'K8sPod'}",FALRabbitMQPrometheusServerCrashing,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FALRabbitMQPrometheusServerCrashing has fired on #$.labels.cluster#', 'IcM.Description': 'Container prometheus-server in namespace faal-rabbitmq went into CrashLoopBackOff', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/kube-state-metrics-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Container prometheus-server in namespace faal-rabbitmq went into CrashLoopBackOff'},"Trigger an alert when a Prometheus Server container in the faal-rabbitmq namespace is waiting due to one of the following reasons: CrashLoopBackOff, ContainerCreating, Error, ImagePullBackOff, ErrImagePull, or CreateContainerConfigError, and there is exactly 1 occurrence of this condition."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"absent(up{job=""kube-apiserver""} == 1)","{'metric': 'up', 'service': 'K8sAPIServer'}",K8SApiserverDown,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert K8SApiserverDown has fired on prometheusmdmeastus-stage', 'IcM.Description': 'No API servers are reachable', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/observability/monitors/kubernetes/k8sapiserverdown'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'No API servers are reachable'},"The Kubernetes API server is down or not responding, as indicated by the absence of a healthy status (up) for at least one instance of the job ""kube-apiserver""."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"sum(rate(kube_pod_container_status_restarts_total[1m])) by (namespace, cluster, pod) > 1 / (5 * 60)","{'metric': 'kube_pod_container_status_restarts_total', 'service': 'K8sPod'}",PodRestartingTooMuch,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert PodRestartingTooMuch has fired on #$.labels.cluster# for pod #$.labels.pod#', 'IcM.Description': 'No API servers are reachable', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/kube-state-metrics-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Pod has restarted too many times in the last hour'},"The number of restarts for containers in pods is higher than 1 restart every 5 minutes, grouped by namespace, cluster, and pod."
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"sum by (cluster, namespace, pod) (kube_pod_status_phase{phase=""Pending""} * on (cluster, namespace, pod) group_left() max_over_time(kube_pod_status_phase{phase=""Pending""}[5m]))  > 0","{'metric': 'kube_pod_status_phase', 'service': 'K8sPod'}",K8SPodInPendingState,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert K8SPodInPendingState has fired on #$.labels.cluster# for Pod #$.labels.pod#', 'IcM.Description': '#$.labels.pod# Pod is in pending state since last 5 mins'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Pod is in pending state since last 5 mins'},"""Alert when there is at least one pending pod in a cluster, namespace, and pod combination where the pod has been pending for more than 5 minutes."""
K8sRuleGroup,eastus,Alerts on health of K8s cluster,,"node_filesystem_avail_bytes{mountpoint=""/"",fstype!=""tmpfs""} / node_filesystem_size_bytes{mountpoint=""/"",fstype!=""tmpfs""} * 100 < 10","{'metric': 'node_filesystem_avail_bytes', 'service': 'K8sNode'}",NodeDiskRunningFull,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NodeDiskRunningFull has fired on #$.labels.cluster#', 'IcM.Description': 'Node disk is running full', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/kube-state-metrics-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'Node disk is running full'},The available disk space on the root partition is less than 10% of the total disk space.
CvsRuleGroup,eastus,Cloud Volumes Service Alerts,,"sum by (region) (rate(http_requests_total{job='cloud-volumes-service',code!~'2..|4..'}[10m])) > 0.1","{'severity': 'critical', 'metric': 'http_requests_total', 'service': 'cloud-volumes-service', 'pod': 'cvs'}",CVSHttp5XXErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVSHttp5XXErrorCritical has fired on #$.labels.region# | #$.labels.service# | #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 5XX errors for in CVS in the last 10 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CVSHttp5XXErrorCritical', 'description': 'There were #$.labels.value# HTTP 5XX errors for in CVS in the last 10 minutes, please investigate', 'summary': 'High amount of CVS HTTP 5XX Failures', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/e6278aad-0b5e-43b4-b78f-c1bc2651a5a1/cloud-volumes'}","Trigger an alert when the average rate of non-200 and non-400 HTTP requests per minute from the cloud-volumes-service job exceeds 0.1 requests per second over a 10-minute period, grouped by region."
CvsRuleGroup,eastus,Cloud Volumes Service Alerts,,sum(increase(cvs_snapshots_hydration_errors_count[30m])) > 10,"{'severity': 'critical', 'metric': 'cvs_snapshots_hydration_errors_count', 'service': 'cloud-volumes-service', 'pod': 'cvs'}",CVSSnapshotHydrationError,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVSSnapshotHydrationError has fired on #$.labels.region# | #$.labels.service# | #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# snapshots hydration errors in CVS in the last 30 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CVSSnapshotHydrationError', 'description': 'There were #$.labels.value# snapshots hydration errors in CVS in the last 30 minutes, please investigate', 'summary': 'High amount of CVS snapshot hydration errors', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/e6278aad-0b5e-43b4-b78f-c1bc2651a5a1/cloud-volumes'}","In the last 30 minutes, the total number of new hydration errors from CVS snapshots has exceeded 10."
CvsRuleGroup,eastus,Cloud Volumes Service Alerts,,sum(increase(volume_clone_split_failure_count[60m])) > 1,"{'severity': 'critical', 'metric': 'volume_clone_split_failure_count', 'service': 'cloud-volumes-service', 'pod': 'cvs'}",CVSVolumeCloneSplitFailure,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVSVolumeCloneSplitFailure has fired on #$.labels.region# | #$.labels.service# | #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# volume clone split failures in CVS in the last 60 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CVSVolumeCloneSplitFailure', 'description': 'There were #$.labels.value# volume clone split failures in CVS in the last 60 minutes, please investigate', 'summary': 'A volume clone split failed in CVS. The ontap cluster needs to be investigated', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/e6278aad-0b5e-43b4-b78f-c1bc2651a5a1/cloud-volumes'}",The total number of volume clone split failures that occurred in the last 60 minutes is more than 1.
SfrRuleGroup,eastus,Alerts on SFR,,sum by (region) (increase(cbs_adc_node_labelling_error_gauge[1h])) > 0,"{'metric': 'cbs_adc_node_labelling_error_gauge', 'service': 'sfr', 'severity': 'warning', 'pod': 'cbs'}",CbsAdcNodelabelError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CbsAdcNodelabelError has fired on #$.labels.region#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for in CBS in the last 5 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/sfr-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for in CBS in the last 5 minutes, please investigate', 'summary': 'Failure in validating the ADC node label'}",There is an increase in labelling errors on ADC nodes in at least one region over the last hour.
SfrRuleGroup,eastus,Alerts on SFR,,sum by (region) (increase(cbs_sfr_poll_pod_status_error_gauge[1h])) > 0,"{'metric': 'cbs_sfr_poll_pod_status_error_gauge', 'service': 'sfr', 'severity': 'warning', 'pod': 'cbs'}",CbsSfrPollPodError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CbsSfrPollPodError has fired on #$.labels.region#', 'IcM.Description': 'Storage Ontap Key Rotation failed, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/sfr-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Storage Ontap Key Rotation failed, please investigate', 'summary': 'Failure in polling for the ADC Browse or config pod'}",There has been at least one increase in pod status errors in any region over the last hour.
SfrRuleGroup,eastus,Alerts on SFR,,sum by (region) (increase(cbs_sfr_status_error_gauge[1h])) > 0,"{'metric': 'cbs_sfr_status_error_gauge', 'service': 'sfr', 'severity': 'warning', 'pod': 'cbs'}",CbsSfrStatusError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CbsSfrStatusError has fired on #$.labels.region#', 'IcM.Description': 'ADC calls are failing with request timeout, connectivity failed, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/sfr-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'ADC calls are failing with request timeout, connectivity failed, please investigate', 'summary': 'Alert on ADC connectivity issues'}","There has been at least one error in the CBS SFR status within the last hour, grouped by region."
SfrRuleGroup,eastus,Alerts on SFR,,sum by (region) (increase(cbs_subscription_priority_with_less_than_or_equal_to_zero_gauge[1h])) > 0,"{'metric': 'cbs_subscription_priority_with_less_than_or_equal_to_zero_gauge', 'service': 'sfr', 'severity': 'warning', 'pod': 'cbs'}",CbsSubPriorityError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CbsSubPriorityError has fired on #$.labels.region#', 'IcM.Description': 'CVI calls may fail with request timeout, connectivity failed, too many requests failure, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/sfr-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'CVI calls may fail with request timeout, connectivity failed, too many requests failure, please investigate', 'summary': 'Alert on CVI connectivity issues'}","There is an increase in the number of CBS subscription priorities with a value of less than or equal to zero within the last hour, grouped by region, and at least one region has seen such an increase."
SloRuleGroup,eastus,Alerts on SLO,,"(sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5.."", url=""/v2/volumes/<uuid>"",method=""put""}[1h])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/volumes/<uuid>"",method=""put""}[1h]))) > (14.4 * 0.0001) and (sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5..""}[5m])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/volumes/<uuid>"",method=""put""}[5m]))) > (14.4 * 0.0001)","{'metric': 'http_requests_total', 'service': 'cvs', 'pod': 'cvs'}",CVSHighServerErrorRateOnVolumeCreate,True,3.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVSHighServerErrorRateOnVolumeCreate has fired on #$.labels.region#', 'IcM.Description': 'CVS is experiencing high server error rate on volume create operation, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'CVS is experiencing high server error rate on volume create operation, please investigate', 'summary': 'CVS is experiencing high server error rate on volume create operation in last 1 hour'}","Trigger an alert when, for the cloud-volumes-service job, two conditions are met: 

1. The proportion of HTTP requests with a 5xx status code (indicating server errors) to all PUT requests to the /v2/volumes/<uuid> endpoint over the last hour exceeds 0.144% (or 14.4 per million). 

2. The proportion of all 5xx status code requests to all PUT requests to the /v2/volumes/<uuid> endpoint over the last 5 minutes also exceeds 0.144% (or 14.4 per million)."
SloRuleGroup,eastus,Alerts on SLO,,"(sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5.."", url=""/v2/volumes/<uuid>"",method=""delete""}[1h])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/volumes/<uuid>"",method=""delete""}[1h]))) > (14.4 * 0.0001) and (sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5..""}[5m])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/volumes/<uuid>"",method=""delete""}[5m]))) > (14.4 * 0.0001)","{'metric': 'http_requests_total', 'service': 'cvs', 'pod': 'cvs'}",CVSHighServerErrorRateOnVolumeDelete,True,3.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVSHighServerErrorRateOnVolumeDelete has fired on #$.labels.region#', 'IcM.Description': 'CVS is experiencing high server error rate on volume delete in last 1 hour, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'CVS is experiencing high server error rate on volume delete in last 1 hour, please investigate', 'summary': 'CVS is experiencing high server error rate on volume delete in last 1 hour'}","Trigger an alert when, for the cloud-volumes-service job, two conditions are met: 

1. The proportion of HTTP delete requests to /v2/volumes/<uuid> that return a 5xx error code over the last hour is more than 0.144% (or 14.4 per million). 

2. The proportion of all 5xx error codes over the last 5 minutes is more than 0.144% (or 14.4 per million) of all delete requests to /v2/volumes/<uuid>."
SloRuleGroup,eastus,Alerts on SLO,,"(sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5.."", url=""/v2/pools/<uuid>"",method=""put""}[1h])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/pools/<uuid>"",method=""put""}[1h]))) > (14.4 * 0.0001) and (sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5..""}[5m])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/pools/<uuid>"",method=""put""}[5m]))) > (14.4 * 0.0001)","{'metric': 'http_requests_total', 'service': 'cvs', 'pod': 'cvs'}",CVSHighServerErrorRateOnPoolCreate,True,3.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVSHighServerErrorRateOnPoolCreate has fired on #$.labels.region#', 'IcM.Description': 'CVS is experiencing high server error rate on pool create in last 1 hour, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'CVS is experiencing high server error rate on pool create in last 1 hour, please investigate', 'summary': 'CVS is experiencing high server error rate on pool create in last 1 hour'}","Trigger an alert when two conditions are met: 

1. More than 0.144% of PUT requests to ""/v2/pools/<uuid>"" on the ""cloud-volumes-service"" job return a 5xx error code over the last hour.
2. More than 0.144% of all requests with a 5xx error code on the ""cloud-volumes-service"" job occur within the last 5 minutes, compared to the total number of PUT requests to ""/v2/pools/<uuid>"" during that time."
SloRuleGroup,eastus,Alerts on SLO,,"(sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5.."", url=""/v2/pools/<uuid>"",method=""delete""}[1h])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/pools/<uuid>"",method=""delete""}[1h]))) > (14.4 * 0.0001) and (sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5..""}[5m])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/pools/<uuid>"",method=""delete""}[5m]))) > (14.4 * 0.0001)","{'metric': 'http_requests_total', 'service': 'cvs', 'pod': 'cvs'}",CVSHighServerErrorRateOnPoolDelete,True,3.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVSHighServerErrorRateOnPoolDelete has fired on #$.labels.region#', 'IcM.Description': 'CVS is experiencing high server error rate on pool delete in last 1 hour, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'CVS is experiencing high server error rate on pool delete in last 1 hour, please investigate', 'summary': 'CVS is experiencing high server error rate on pool delete in last 1 hour'}","Alert when more than 0.144% of delete requests to the ""/v2/pools/<uuid>"" endpoint on the ""cloud-volumes-service"" job return a 5xx error code over the last hour, and also when more than 0.144% of all requests with 5xx error codes are from this specific endpoint over the last 5 minutes."
SloRuleGroup,eastus,Alerts on SLO,,"(sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5.."", url=""/v2/volumes/<uuid>/backups/<uuid>"",method=""delete""}[1h])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/volumes/<uuid>/backups/<uuid>"",method=""delete""}[1h]))) > (14.4 * 0.0001) and (sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5..""}[5m])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/volumes/<uuid>/backups/<uuid>"",method=""delete""}[5m]))) > (14.4 * 0.0001)","{'metric': 'http_requests_total', 'service': 'cvs', 'pod': 'cvs'}",CVSHighServerErrorRateOnBackupDelete,True,3.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVSHighServerErrorRateOnBackupDelete has fired on #$.labels.region#', 'IcM.Description': 'CVS is experiencing high server error rate on backup delete in last 1 hour, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'CVS is experiencing high server error rate on backup delete in last 1 hour, please investigate', 'summary': 'CVS is experiencing high server error rate on backup delete in last 1 hour'}","The system is alerting when two conditions are met: 

1. More than 0.144% of DELETE requests to the ""/v2/volumes/<uuid>/backups/<uuid>"" endpoint on the ""cloud-volumes-service"" job in the last hour return a server error (5xx status code).

2. More than 0.144% of all requests with server errors (5xx status code) on the ""cloud-volumes-service"" job in the last 5 minutes are DELETE requests to the ""/v2/volumes/<uuid>/backups/<uuid>"" endpoint."
SloRuleGroup,eastus,Alerts on SLO,,"(sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5.."", url=""/v2/volumereplications/<uuid>"",method=""put""}[1h])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/volumereplications/<uuid>"",method=""put""}[1h]))) > (14.4 * 0.0001) and (sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5..""}[5m])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/volumereplications/<uuid>"",method=""put""}[5m]))) > (14.4 * 0.0001)","{'metric': 'http_requests_total', 'service': 'cvs', 'pod': 'cvs'}",CVSHighServerErrorRateOnReplicationCreate,True,3.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVSHighServerErrorRateOnReplicationCreate has fired on #$.labels.region#', 'IcM.Description': 'CVS is experiencing high server error rate on replication create in last 1 hour, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'CVS is experiencing high server error rate on replication create in last 1 hour, please investigate', 'summary': 'CVS is experiencing high server error rate on replication create in last 1 hour'}","The system is alerting when two conditions are met: 

1. More than 0.144% of PUT requests to ""/v2/volumereplications/<uuid>"" on the ""cloud-volumes-service"" job in the last hour return a server error (5xx status code).

2. More than 0.144% of all requests with server errors (5xx status code) on the ""cloud-volumes-service"" job in the last 5 minutes are PUT requests to ""/v2/volumereplications/<uuid>""."
SloRuleGroup,eastus,Alerts on SLO,,"(sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5.."", url=""/v2/volumereplications/<uuid>"",method=""delete""}[1h])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/volumereplications/<uuid>"",method=""delete""}[1h]))) > (14.4 * 0.0001) and (sum(rate(http_requests_total{job=""cloud-volumes-service"",code=~""5..""}[5m])) / sum(rate(http_requests_total{job=""cloud-volumes-service"",url=""/v2/volumereplications/<uuid>"",method=""delete""}[5m]))) > (14.4 * 0.0001)","{'metric': 'http_requests_total', 'service': 'cvs', 'pod': 'cvs'}",CVSHighServerErrorRateOnReplicationDelete,True,3.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVSHighServerErrorRateOnReplicationDelete has fired on #$.labels.region#', 'IcM.Description': 'CVS is experiencing high server error rate on replication delete in last 1 hour, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'CVS is experiencing high server error rate on replication delete in last 1 hour, please investigate', 'summary': 'CVS is experiencing high server error rate on replication delete in last 1 hour'}","The system is alerting when two conditions are met: 

1. More than 0.144% of DELETE requests to ""/v2/volumereplications/<uuid>"" endpoint on the ""cloud-volumes-service"" job in the last hour return a server error (5xx status code).

2. More than 0.144% of all requests with server errors (5xx status code) on the ""cloud-volumes-service"" job in the last 5 minutes are DELETE requests to ""/v2/volumereplications/<uuid>"" endpoint."
SloRuleGroup,eastus,Alerts on SLO,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""put"",url=""/v2/volumes/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnVolumeCreate,True,3.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnVolumeCreate has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on volume creation is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on volume creation is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on volume creation is above 1 minute in the last 10 minutes.'}",The 90th percentile of the average time it takes to handle HTTP PUT requests to /v2/volumes/<uuid> over the last 5 minutes is greater than 60 seconds for the cloud-volumes-service job.
SloRuleGroup,eastus,Alerts on SLO,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""delete"",url=""/v2/volumes/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnVolumeDelete,True,3.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnVolumeDelete has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on volume delete is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on volume delete is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on volume delete is above 1 minute in the last 10 minutes.'}",The 90th percentile of the average time it takes to delete a volume via the cloud-volumes-service over the last 5 minutes is greater than 60 seconds.
SloRuleGroup,eastus,Alerts on SLO,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""put"",url=""/v2/pools/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnPoolCreate,True,3.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnPoolCreate has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on pool create is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on pool create is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on pool create is above 1 minute in the last 10 minutes.'}",The 90th percentile of the time it takes to process 'PUT' requests to '/v2/pools/<uuid>' on the cloud-volumes-service over a 5-minute period is greater than 60 seconds.
SloRuleGroup,eastus,Alerts on SLO,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""delete"",url=""/v2/pools/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnPoolDelete,True,3.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnPoolDelete has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on pool delete is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on pool delete is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on pool delete is above 1 minute in the last 10 minutes.'}",The 90th percentile of the average time it takes to delete a pool in the cloud volumes service over the last 5 minutes is greater than 60 seconds.
SloRuleGroup,eastus,Alerts on SLO,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""delete"",url=""/v2/volumes/<uuid>/backups/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnBackupDelete,True,3.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnBackupDelete has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on backup delete is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on backup delete is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on backup delete is above 1 minute in the last 10 minutes.'}",The 90th percentile of the time it takes to delete a volume backup in the cloud-volumes-service is higher than 1 minute over the last 5 minutes.
SloRuleGroup,eastus,Alerts on SLO,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""put"",url=""/v2/volumereplications/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnReplicationCreate,True,3.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnReplicationCreate has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on replication create is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on replication create is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on replication create is above 1 minute in the last 10 minutes.'}",The 90th percentile of the time it takes to process 'PUT' requests to '/v2/volumereplications/<uuid>' on the cloud-volumes-service is higher than 1 minute over the last 5 minutes.
SloRuleGroup,eastus,Alerts on SLO,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""delete"",url=""/v2/volumereplications/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnReplicationDelete,True,3.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnReplicationDelete has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on replication delete is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on replication delete is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on replication delete is above 1 minute in the last 10 minutes.'}",The 90th percentile of the time it takes to delete a volume replication on the cloud-volumes-service is higher than 1 minute over the last 5 minutes.
SloRuleGroup,eastus,Alerts on SLO,,"(((max_over_time(volume_write_latency{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m]) or vector(0)) - (max_over_time(volume_write_latency{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 10m) or vector(0))) / (max_over_time(volume_write_latency{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 10m) or vector(0)) * 100 >= 10) and (((max_over_time(volume_write_latency{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 10m) or vector(0)) - (max_over_time(volume_write_latency{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 20m) or vector(0))) / (max_over_time(volume_write_latency{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 20m) or vector(0)) * 100 >= 10)",{'metric': 'volume_write_latency'},FIORunnersLatency,True,2.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FIORunnersVolume has fired on #$.labels.region#', 'IcM.Description': '10% Latency Deviation on Runners Volume in the Last 30 Minutes (10-Minute Iterations). (region #$.labels.region#) (volume #$.labels.volume#) (ontap_cluster #$.labels.ontap_cluster#) (node #$.labels.node#) (aggr #$.labels.aggr#)', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/runners-performance-deviation'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': '10% Latency Deviation on Runners Volume in the Last 30 Minutes (5-Minute Iterations). (region #$.labels.region#) (volume #$.labels.volume#) (ontap_cluster #$.labels.ontap_cluster#) (node #$.labels.node#) (aggr #$.labels.aggr#)', 'summary': '10% Latency Deviation on Runners Volume in the Last 30 Minutes (10-Minute Iterations).'}","The rule is triggering when two conditions are met:

1. The maximum write latency of volumes named ""vol_basic_volume"" or ""vol_standard_volume"" over the last 10 minutes has increased by at least 10% compared to the same metric 10 minutes ago.

2. The maximum write latency of the same volumes over the last 10 minutes (offset by 10 minutes) has also increased by at least 10% compared to the same metric 20 minutes ago.

In simpler terms, this rule is checking for a significant increase in write latency over two consecutive 10-minute periods."
SloRuleGroup,eastus,Alerts on SLO,,"(((max_over_time(volume_write_ops{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m]) or vector(0)) - (max_over_time(volume_write_ops{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 10m) or vector(0))) / (max_over_time(volume_write_ops{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 10m) or vector(0)) * 100 <= -10) and (((max_over_time(volume_write_ops{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 10m) or vector(0)) - (max_over_time(volume_write_ops{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 20m) or vector(0))) / (max_over_time(volume_write_ops{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 20m) or vector(0)) * 100 <= -10)",{'metric': 'volume_write_ops'},FIORunnersIOPS,True,3.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert for synthetics standard deviation in IOPS has fired on #$.labels.region#', 'IcM.Description': '10% IOPS Deviation on Runners Volume in the Last 30 Minutes (10-Minute Iterations). (region #$.labels.region#) (volume #$.labels.volume#) (ontap_cluster #$.labels.ontap_cluster#) (node #$.labels.node#) (aggr #$.labels.aggr#)', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/runners-performance-deviation'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': '10% IOPS Deviation on Runners Volume in the Last 30 Minutes (5-Minute Iterations). (region #$.labels.region#) (volume #$.labels.volume#) (ontap_cluster #$.labels.ontap_cluster#) (node #$.labels.node#) (aggr #$.labels.aggr#)', 'summary': '10% IOPS Deviation on Runners Volume in the Last 30 Minutes (10-Minute Iterations).'}","The volume write operations for volumes named ""vol_basic_volume"" or ""vol_standard_volume"" have decreased by more than 10% in the last 10 minutes compared to both the previous 10-20 minutes and the previous 20-30 minutes."
SloRuleGroup,eastus,Alerts on SLO,,"(((max_over_time(volume_write_data{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m]) or vector(0)) - (max_over_time(volume_write_data{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 10m) or vector(0))) / (max_over_time(volume_write_data{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 10m) or vector(0)) * 100 <= -10) and (((max_over_time(volume_write_data{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 10m) or vector(0)) - (max_over_time(volume_write_data{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 20m) or vector(0))) / (max_over_time(volume_write_data{volume=~""vol_basic_volume.*|vol_standard_volume.*""}[10m] offset 20m) or vector(0)) * 100 <= -10)",{'metric': 'volume_write_data'},FIORunnersThroughput,True,3.0,PT5M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert for synthetics standard deviation in Throughput has fired on #$.labels.region#', 'IcM.Description': '10% Throughput Deviation on Runners Volume in the Last 30 Minutes (10-Minute Iterations). (region #$.labels.region#) (volume #$.labels.volume#) (ontap_cluster #$.labels.ontap_cluster#) (node #$.labels.node#) (aggr #$.labels.aggr#)', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/runners-performance-deviation'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': '10% Throughput Deviation on Runners Volume in the Last 30 Minutes (5-Minute Iterations). (region #$.labels.region#) (volume #$.labels.volume#) (ontap_cluster #$.labels.ontap_cluster#) (node #$.labels.node#) (aggr #$.labels.aggr#)', 'summary': '10% Throughput Deviation on Runners Volume in the Last 30 Minutes (10-Minute Iterations).'}","The volume write data for volumes named ""vol_basic_volume"" or ""vol_standard_volume"" has decreased by more than 10% in the last 10 minutes compared to the previous 10 minutes, and also decreased by more than 10% in the last 20 minutes compared to the 10 minutes before that."
CshRuleGroup,eastus,Alerts on CSH,,"increase(csh_akv_failure_count[5m]) > 0 and csh_akv_failure_count{hostname=~""cvt-metric-receiver.*|qstack-usage-geneva-pusher.*|cvt-pipelines.*|statistics-qollector-usage.*|cloud-volumes-secd-proxy.*|fluentd-aggregator-geneva-logs.*|cvt-cvi-scraper.*""}","{'metric': 'csh_akv_failure_count', 'service': 'csh', 'pod': 'csh'}",CshAkvFailureCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CshAkvFailureCount has fired on #$.labels.region# | hostname #$.labels.hostname#', 'IcM.Description': 'The CshAkvFailureCount #$.labels.job# has failed', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/csh-akv-secret-retrieval-failed'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The CshAkvFailureCount #$.labels.schedule# has failed #$.labels.value# times', 'summary': 'Csh AKV Count Failure at least once in the last 5 minutes.', 'dashboard': 'to-be-added'}","Trigger an alert when there is an increase in the number of AKV failures over the last 5 minutes and the failure occurs on a host with a name matching one of the following patterns: cvt-metric-receiver, qstack-usage-geneva-pusher, cvt-pipelines, statistics-qollector-usage, cloud-volumes-secd-proxy, fluentd-aggregator-geneva-logs, or cvt-cvi-scraper."
CshRuleGroup,eastus,Alerts on CSH,,increase(csh_fallback_to_k8s_secret[5m]) > 0,"{'metric': 'csh_fallback_to_k8s_secret', 'service': 'csh', 'pod': 'csh'}",CshFallbackToK8Secret,False,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CshFallbackToK8Secret has fired on #$.labels.region# | hostname #$.labels.hostname#', 'IcM.Description': 'Csh Fallback To K8s Secret #$.labels.job# has occurred, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/csh-akv-secret-retrieval-failed'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Csh Fallback To K8s Secret #$.labels.schedule# has occurred, please investigate', 'summary': 'Csh Fallback to K8s Secret at least once in the last 5 minutes.', 'dashboard': 'to-be-added'}",There has been an increase in the number of times a fallback to a Kubernetes secret occurred within the last 5 minutes.
VeleroRuleGroup,eastus,Alerts on Velero,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""put"",url=""/v2/pools/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnPoolCreate,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnPoolCreate has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on pool create is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on pool create is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on pool create is above 1 minute in the last 10 minutes.'}",The 90th percentile of the time it takes to process 'PUT' requests to '/v2/pools/<uuid>' on the cloud-volumes-service over a 5-minute period is greater than 60 seconds.
VeleroRuleGroup,eastus,Alerts on Velero,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""delete"",url=""/v2/pools/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnPoolDelete,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnPoolDelete has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on pool delete is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on pool delete is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on pool delete is above 1 minute in the last 10 minutes.'}",The 90th percentile of the average time it takes to delete a pool in the cloud volumes service over the last 5 minutes is greater than 60 seconds.
VeleroRuleGroup,eastus,Alerts on Velero,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""delete"",url=""/v2/volumes/<uuid>/backups/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnBackupDelete,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnBackupDelete has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on backup delete is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on backup delete is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on backup delete is above 1 minute in the last 10 minutes.'}",The 90th percentile of the time it takes to delete a volume backup in the cloud-volumes-service is higher than 1 minute over the last 5 minutes.
VeleroRuleGroup,eastus,Alerts on Velero,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""put"",url=""/v2/volumereplications/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnReplicationCreate,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnReplicationCreate has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on replication create is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on replication create is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on replication create is above 1 minute in the last 10 minutes.'}",The 90th percentile of the time it takes to process 'PUT' requests to '/v2/volumereplications/<uuid>' on the cloud-volumes-service is higher than 1 minute over the last 5 minutes.
VeleroRuleGroup,eastus,Alerts on Velero,,"histogram_quantile(0.90, rate(http_request_duration_seconds_bucket{job=""cloud-volumes-service"",method=""delete"",url=""/v2/volumereplications/<uuid>""}[5m])) > 60","{'metric': 'http_request_duration_seconds_bucket', 'service': 'cvs', 'pod': 'cvs'}",HighP90LatenyOnReplicationDelete,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnReplicationDelete has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on replication delete is above 1 minute in the last 10 minutes., please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on replication delete is above 1 minute in the last 10 minutes., please investigate', 'summary': 'P90 latency on replication delete is above 1 minute in the last 10 minutes.'}",The 90th percentile of the time it takes to delete a volume replication on the cloud-volumes-service is higher than 1 minute over the last 5 minutes.
VeleroRuleGroup,eastus,Alerts on Velero,,increase(velero_backup_failure_total[5m]) > 0,"{'metric': 'velero_backup_failure_total', 'service': 'velero', 'pod': 'velero'}",VeleroBackupFailed,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VeleroBackupFailed has fired on #$.labels.region#', 'IcM.Description': 'The backup #$.labels.schedule# has failed #$.labels.value# times'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The backup #$.labels.schedule# has failed #$.labels.value# times', 'summary': 'Velero backup has failed at least once in the last 5 minutes.'}",There has been at least one Velero backup failure in the last 5 minutes.
VeleroRuleGroup,eastus,Alerts on Velero,,increase(velero_restore_failed_total[5m]) > 0,"{'metric': 'velero_restore_failed_total', 'service': 'velero', 'pod': 'velero'}",VeleroRestoreFailed,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VeleroRestoreFailed has fired on #$.labels.region#', 'IcM.Description': 'The restore #$.labels.schedule# has failed #$.labels.value# times'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The restore #$.labels.schedule# has failed #$.labels.value# times', 'summary': 'Velero restore has failed at least once in the last 5 minutes.'}",There has been at least one Velero restore failure in the last 5 minutes.
VeleroRuleGroup,eastus,Alerts on Velero,,increase(velero_volume_snapshot_failure_total[5m]) > 0,"{'metric': 'velero_volume_snapshot_failure_total', 'service': 'velero', 'pod': 'velero'}",VeleroVolumeSnapshotFailed,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VeleroVolumeSnapshotFailed has fired on #$.labels.region#', 'IcM.Description': 'The Volume Snapshot #$.labels.schedule# has failed #$.labels.value# times'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The Volume Snapshot #$.labels.schedule# has failed #$.labels.value# times', 'summary': 'Velero volume snapshot has failed at least once in the last 5 minutes.'}",There has been at least one Velero volume snapshot failure in the last 5 minutes.
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,"count by (namespace, rabbitmq_cluster) (erlang_vm_dist_node_state * on(instance) group_left(rabbitmq_cluster) rabbitmq_identity_info == 3) < count by (namespace, rabbitmq_cluster) (rabbitmq_build_info * on(instance) group_left(rabbitmq_cluster) rabbitmq_identity_info) * (count by (namespace, rabbitmq_cluster) (rabbitmq_build_info * on(instance) group_left(rabbitmq_cluster) rabbitmq_identity_info) -1 )","{'metric': 'erlang_vm_dist_node_state', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '3'}",InsufficientEstablishedErlangDistributionLinks,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert InsufficientEstablishedErlangDistributionLinks has fired on #$.labels.region#', 'IcM.Description': 'There are too few established Erlang distribution links in RabbitMQ cluster.'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'There are too few established Erlang distribution links in RabbitMQ cluster.', 'summary': ""RabbitMQ clusters have a full mesh topology. All RabbitMQ nodes connect to all other RabbitMQ nodes in both directions. The expected number of established Erlang distribution links is therefore `n*(n-1)` where `n` is the number of RabbitMQ nodes in the cluster. Therefore, the expected number of distribution links are `0` for a 1-node cluster, `6` for a 3-node cluster, and `20` for a 5-node cluster. This alert reports that the number of established distributions links is less than the expected number. Some reasons for this alert include failed network links, network partitions, failed clustering (i.e. nodes can't join the cluster). Check the panels `All distribution links`, `Established distribution links`, `Connecting distributions links`, `Waiting distribution links`, and `distribution links` of the Grafana dashboard `Erlang-Distribution`. Check the logs of the RabbitMQ nodes: `kubectl -n rabbitmq logs -l app.kubernetes.io/component=rabbitmq`.""}","The number of RabbitMQ nodes in a cluster that are not fully connected is greater than 0, indicating a potential issue with the cluster's stability. Specifically, this rule checks if there are fewer nodes reporting a fully connected state (erlang_vm_dist_node_state == 3) compared to the total number of nodes in the cluster, minus one, which allows for a single node to be disconnected without triggering the alert."
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,sum by (region) (rabbitmq_queue_messages >= 10000),"{'metric': 'rabbitmq_queue_messages', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '3'}",RabbitMQQueueMessagesHigh,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitMQQueueMessagesHigh has fired on #$.labels.region#', 'IcM.Description': 'The RabbitMQ queue has more than 10,000 messages.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/rabbitmq-queue-high'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'The RabbitMQ queue has more than 10,000 messages.', 'summary': 'High RabbitMQ Queue Messages'}","Show the total number of regions where there are at least 10,000 messages in a RabbitMQ queue."
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,sum by (region) (rabbitmq_connection_error) > 0,"{'metric': 'rabbitmq_connection_error', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '2'}",RabbitmqConnectionError,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitmqConnectionError has fired on #$.labels.region#', 'IcM.Description': 'There is an issue connecting to Rabbitmq', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/rabbitmq-connection-error'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'There is an issue connecting to Rabbitmq', 'summary': 'Consumer is unable to connect to Rabbitmq'}",There is at least one RabbitMQ connection error in each region.
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,"(predict_linear(rabbitmq_disk_space_available_bytes[24h], 60*60*24) * on (instance) group_left(rabbitmq_cluster, rabbitmq_node, pod) rabbitmq_identity_info < rabbitmq_disk_space_available_limit_bytes * on (instance) group_left(rabbitmq_cluster, rabbitmq_node, pod) rabbitmq_identity_info) and (count_over_time(rabbitmq_disk_space_available_limit_bytes[2h] offset 22h) * on (instance) group_left(rabbitmq_cluster, rabbitmq_node, pod) rabbitmq_identity_info > 0)","{'metric': 'rabbitmq_disk_space_available_bytes', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '3'}",LowDiskWatermarkPredicted,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert LowDiskWatermarkPredicted has fired on #$.labels.region#', 'IcM.Description': 'The predicted free disk space in 24 hours from now is running low'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'The predicted free disk space in 24 hours from now is running low', 'summary': ""Based on the trend of available disk space over the past 24 hours, it's predicted that, in 24 hours from now, a disk alarm will be triggered since the free disk space will drop below the free disk space limit. This alert is reported for the partition where the RabbitMQ data directory is stored. When the disk alarm will be triggered, all publishing connections across all cluster nodes will be blocked. See https://www.rabbitmq.com/alarms.html, https://www.rabbitmq.com/disk-alarms.html, https://www.rabbitmq.com/production-checklist.html#resource-limits-disk-space, https://www.rabbitmq.com/persistence-conf.html, https://www.rabbitmq.com/connection-blocked.html.""}","""Alert when the predicted available disk space for RabbitMQ in the next 24 hours is less than the defined limit and there has been at least one measurement of the disk space limit in the last 2 hours (looking back from 22 hours ago), indicating that the disk space is likely to run out soon."""
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,"(sum(rate(rabbitmq_connections_closed_total[5m]) * on(instance) group_left(rabbitmq_cluster, rabbitmq_node) rabbitmq_identity_info) by(namespace, rabbitmq_cluster) + sum(rate(rabbitmq_connections_opened_total[5m]) * on(instance) group_left(rabbitmq_cluster, rabbitmq_node) rabbitmq_identity_info) by(namespace, rabbitmq_cluster)) / sum (rabbitmq_connections * on(instance) group_left(rabbitmq_cluster) rabbitmq_identity_info) by (namespace, rabbitmq_cluster) > 0.1 unless sum (rabbitmq_connections * on(instance) group_left(rabbitmq_cluster) rabbitmq_identity_info) by (namespace, rabbitmq_cluster) < 100","{'metric': 'rabbitmq_connections_opened_total', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '3'}",HighConnectionChurn,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighConnectionChurn has fired on #$.labels.region#', 'IcM.Description': 'Over the last 5 minutes of total connections are closed or opened per second in RabbitMQ cluster'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'Over the last 5 minutes of total connections are closed or opened per second in RabbitMQ cluster', 'summary': 'More than 10% of total connections are churning. This means that client application connections are short-lived instead of long-lived. Read https://www.rabbitmq.com/connections.html#high-connection-churn to understand why this is an anti-pattern.'}","""Trigger an alert when the rate of closed and opened RabbitMQ connections over the last 5 minutes, averaged by namespace and cluster, exceeds 10% of the total number of connections, but only if there are at least 100 connections in the cluster."""
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,"sum(rabbitmq_build_info{kubernetes_pod_name=~""rabbitmq-.*""})<3","{'metric': 'rabbitmq_build_info', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '3'}",RabbitMqClusterNotAllNodesRunning,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitMqClusterNotAllNodesRunning has fired on prometheusmdmeastus-stage', 'IcM.Description': 'Some RabbitMQ Cluster Nodes Are Down'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'Some RabbitMQ Cluster Nodes Are Down', 'summary': 'Some RabbitMQ Cluster Nodes Are Down'}","The number of RabbitMQ pods with names starting with ""rabbitmq-"" is less than 3."
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,"kube_persistentvolumeclaim_status_phase{phase=""Bound""} * on (namespace, persistentvolumeclaim) group_left(label_app_kubernetes_io_name) kube_persistentvolumeclaim_labels{label_app_kubernetes_io_component=""rabbitmq""} == 0","{'metric': 'kube_persistentvolumeclaim_status_phase', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '3'}",PersistentVolumeMissing,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert PersistentVolumeMissing has fired on #$.labels.region#', 'IcM.Description': 'PersistentVolumeClaim in rabbitmq namespace is not bound.'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'PersistentVolumeClaim in rabbitmq namespace is not bound.', 'summary': 'This alert depends on kube-state-metrics to be installed in the k8s cluster RabbitMQ needs a PersistentVolume for its data. However, there is no PersistentVolume bound to the PersistentVolumeClaim. This means the requested storage could not be provisioned. Check the status of the PersistentVolumeClaim: `kubectl -n rabbitmq describe pvc `.'}","""Alert when there are no RabbitMQ Persistent Volume Claims in the 'Bound' phase, meaning that RabbitMQ is not properly attached to its required storage."""
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,"sum by(namespace, rabbitmq_cluster) (increase(rabbitmq_channel_messages_unroutable_dropped_total[5m]) * on(instance) group_left(rabbitmq_cluster) rabbitmq_identity_info) >= 1 or sum by(namespace, rabbitmq_cluster) (increase(rabbitmq_channel_messages_unroutable_returned_total[5m]) * on(instance) group_left(rabbitmq_cluster) rabbitmq_identity_info) >= 1","{'metric': 'rabbitmq_channel_messages_unroutable_dropped_total', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '3'}",UnroutableMessages,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert UnroutableMessages has fired on #$.labels.region#', 'IcM.Description': 'There were unroutable messages within the last 5 minutes in RabbitMQ cluster'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'There were unroutable messages within the last 5 minutes in RabbitMQ cluster', 'summary': 'There are messages published into an exchange which cannot be routed and are either dropped silently, or returned to publishers. Is your routing topology set up correctly? Check your application code and bindings between exchanges and queues. See https://www.rabbitmq.com/publishers.html#unroutable, https://www.rabbitmq.com/confirms.html#when-publishes-are-confirmed.'}","""Trigger an alert if, within a 5-minute window, there is at least one occurrence of either unroutable messages being dropped or unroutable messages being returned in any RabbitMQ cluster across all namespaces."""
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,"sum by(namespace, rabbitmq_cluster, pod, rabbitmq_node) (max_over_time(rabbitmq_process_open_fds[5m]) * on(instance) group_left(rabbitmq_cluster, rabbitmq_node, pod) rabbitmq_identity_info) / sum by(namespace, rabbitmq_cluster, pod, rabbitmq_node) (rabbitmq_process_max_tcp_sockets  * on(instance) group_left(rabbitmq_cluster, rabbitmq_node, pod) rabbitmq_identity_info) > 0.8","{'metric': 'rabbitmq_process_open_fds', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '3'}",FileDescriptorsNearLimit,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FileDescriptorsNearLimit has fired on #$.labels.region#', 'IcM.Description': 'file descriptors are nearing file descriptor limit in RabbitMQ node. Restart it please.'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'file descriptors are nearing file descriptor limit in RabbitMQ node. Restart it please.', 'summary': 'More than 80% of file descriptors are used on the RabbitMQ node. When this value reaches 100%, new connections will not be accepted and disk write operations may fail. Client libraries, peer nodes and CLI tools will not be able to connect when the node runs out of available file descriptors. See https://www.rabbitmq.com/production-checklist.html#resource-limits-file-handle-limit.'}","""Trigger an alert when the average number of open file descriptors for a RabbitMQ node exceeds 80% of its maximum allowed TCP sockets over the last 5 minutes, grouped by namespace, cluster, pod, and node."""
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,"sum by(namespace, rabbitmq_cluster, pod, rabbitmq_node) (max_over_time(rabbitmq_process_open_tcp_sockets[5m]) * on(instance) group_left(rabbitmq_cluster, rabbitmq_node, pod) rabbitmq_identity_info) / sum by(namespace, rabbitmq_cluster, pod, rabbitmq_node) (rabbitmq_process_max_tcp_sockets * on(instance) group_left(rabbitmq_cluster, rabbitmq_node, pod) rabbitmq_identity_info) > 0.8","{'metric': 'rabbitmq_process_open_tcp_sockets', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '3'}",TCPSocketsNearLimit,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert TCPSocketsNearLimit has fired on #$.labels.region#', 'IcM.Description': 'TCP sockets of TCP socket limit are open in RabbitMQ node.'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'TCP sockets of TCP socket limit are open in RabbitMQ node.', 'summary': 'More than 80% of TCP sockets are open on the RabbitMQ node. When this value reaches 100%, new connections will not be accepted. Client libraries, peer nodes and CLI tools will not be able to connect when the node runs out of available TCP sockets. See https://www.rabbitmq.com/networking.html.'}","""Trigger an alert when the average number of open TCP sockets for a RabbitMQ node exceeds 80% of its maximum allowed TCP sockets over the last 5 minutes, grouped by namespace, cluster, pod, and node."""
RabbitMQSaasRuleGroup,eastus,Alerts on RabbitMQ Saas,,rabbitmq_alarms_free_disk_space_watermark == 1,"{'metric': 'rabbitmq_alarms_free_disk_space_watermark', 'service': 'rabbitmq-saas', 'pod': 'rabbitmq', 'rulesgroup': 'rabbitmq', 'severity': '2'}",RabbitmqDiskAlarm,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitmqDiskAlarm has fired on #$.labels.region#', 'IcM.Description': 'The available disk space on the server is below 8 GB. Please take action to prevent issues.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/rabbitmq-disk-alarm'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'The available disk space on the server is below 8 GB. Please take action to prevent issues.', 'summary': 'The available disk space on the server is below 8 GB. Please take action to prevent issues.'}",Alert when RabbitMQ's free disk space falls below the watermark threshold.
RabbitMQRuleGroup,eastus,Alerts for RabbitMQ,,sum by (region) (rabbitmq_up) == 0,"{'metric': 'rabbitmq_up', 'service': 'rabbitmq', 'severity': 'medium', 'pod': 'rabbitmq'}",RabbitMQDown,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitMQDown has fired on #$.labels.region#', 'IcM.Description': 'RabbitMQ is down\n'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'RabbitMQ is down\n', 'summary': 'RabbitMQ is down', 'dashboard': 'to-be-added'}",There are no RabbitMQ instances running in any region.
RabbitMQRuleGroup,eastus,Alerts for RabbitMQ,,sum by (region) (rabbitmq_running) < 3,"{'metric': 'rabbitmq_running', 'service': 'rabbitmq', 'severity': 'medium', 'pod': 'rabbitmq'}",RabbitMQClusterDegraded,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitMQClusterDegraded has fired on #$.labels.region#', 'IcM.Description': 'Fewer than 3 nodes are running in RabbitMQ cluster'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Fewer than 3 nodes are running in RabbitMQ cluster', 'summary': 'RabbitMQ cluster is degraded (instance #$.labels.node#)', 'dashboard': 'to-be-added'}",There are fewer than 3 RabbitMQ instances running in each region.
RabbitMQRuleGroup,eastus,Alerts for RabbitMQ,,sum by (region) (rabbitmq_partitions) > 0,"{'metric': 'rabbitmq_partitions', 'service': 'rabbitmq', 'severity': 'medium', 'pod': 'rabbitmq'}",RabbitMQClusterPartition,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitMQClusterPartition has fired on #$.labels.region#', 'IcM.Description': 'Cluster partition'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Cluster partition', 'summary': 'RabbitMQ Cluster partition (pod #$.labels.kubernetes_pod_name#)', 'dashboard': 'to-be-added'}",There are RabbitMQ partitions in at least one region.
RabbitMQRuleGroup,eastus,Alerts for RabbitMQ,,sum by (region) (rabbitmq_queue_messages) >= 1000,"{'metric': 'rabbitmq_queue_messages', 'service': 'rabbitmq', 'severity': 'high', 'pagerduty': 'true', 'pod': 'rabbitmq'}",RabbitmqMessageQueueCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitmqMessageQueueCritical has fired on #$.labels.region#', 'IcM.Description': 'Number of messages in the queue #$.labels.queue# is far higher than it should be; check components supposed to be processing these messages', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/rabbitmq-queue-high'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Number of messages in the queue #$.labels.queue# is far higher than it should be; check components supposed to be processing these messages', 'summary': 'RabbitMQ message queue #$.labels.queue# is at #$.value# when it should be close to empty.', 'dashboard': 'to-be-added'}",The total number of messages in RabbitMQ queues is 1000 or more in at least one region.
RabbitMQRuleGroup,eastus,Alerts for RabbitMQ,,"sum by (region) (time() - rabbitmq_queue_head_message_timestamp{queue!=""aliveness-test""}) > 900","{'metric': 'rabbitmq_queue_head_message_timestamp', 'service': 'rabbitmq', 'severity': 'medium', 'pod': 'rabbitmq'}",RabbitmqSlowQueueConsumption,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitmqSlowQueueConsumption has fired on #$.labels.region#', 'IcM.Description': 'Messages are not being picked up within 15 minutes'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Messages are not being picked up within 15 minutes', 'summary': 'Rabbitmq slow queue consumption (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}","Show me the regions where the oldest message in a RabbitMQ queue (excluding the ""aliveness-test"" queue) has been waiting for more than 15 minutes."
RabbitMQRuleGroup,eastus,Alerts for RabbitMQ,,sum by (region) (rabbitmq_memory_alarm) > 0,"{'metric': 'rabbitmq_memory_alarm', 'service': 'rabbitmq', 'severity': 'medium', 'pagerduty': 'true', 'pod': 'rabbitmq'}",RabbitmqOutOfMemory,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitmqOutOfMemory has fired on #$.labels.region#', 'IcM.Description': 'Rabbitmq is out of memory'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Rabbitmq is out of memory', 'summary': 'Rabbitmq out of memory (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}",There is at least one RabbitMQ memory alarm triggered in each region.
RabbitMQRuleGroup,eastus,Alerts for RabbitMQ,,sum by (region) (rabbitmq_memory_used > 80),"{'metric': 'rabbitmq_memory_used', 'service': 'rabbitmq', 'severity': 'medium', 'pod': 'rabbitmq'}",RabbitmqHighMemoryUsage,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RabbitmqHighMemoryUsage has fired on #$.labels.region#', 'IcM.Description': 'Rabbitmq is using more than 80% of memory'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Rabbitmq is using more than 80% of memory', 'summary': 'Rabbitmq high memory usage (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}",Show the total count of regions where RabbitMQ memory usage exceeds 80%.
CvmpRuleGroup,eastus,Cloud Volumes Message Proxy Alerts,,sum(rate(amqp_messages_convert_error_total[5m])) > 0 or max_over_time(amqp_messages_convert_error_total[5m]) > 0,"{'severity': '2', 'metric': 'amqp_messages_convert_error_total', 'service': 'cloud-volumes-message-proxy', 'pod': 'cvmp'}",MessagesConvertErrorTotal,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert MessagesConvertErrorTotal has fired on #$.labels.cluster# | #$.labels.kubernetes_pod_name# ', 'IcM.Description': 'There was an increase of converting errors for #$.labels.kubernetes_pod_name#, fails to convert it to a message for RabbitMQ, Please investigate', 'IcM.TsgId': 'https://msazure.visualstudio.com/DefaultCollection/One/_git/AzureNetApp-Docs?path=/docs/runbooks/cvmp-alerts.md'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'MessagesConvertErrorTotal', 'description': 'There was an increase of converting errors for #$.labels.kubernetes_pod_name#, fails to convert it to a message for RabbitMQ, Please investigate', 'summary': 'CVMP receives a valid message but fails to convert it to a message for RabbitMQ.', 'dashboard': 'to-be-added'}","Trigger an alert if there has been at least one error converting AMQP messages in the last 5 minutes, either overall or at any point during that time."
CvmpRuleGroup,eastus,Cloud Volumes Message Proxy Alerts,,sum(rate(amqp_messages_send_error_total[5m])) > 10 or max_over_time(amqp_messages_send_error_total[5m]) > 0,"{'severity': '2', 'metric': 'amqp_messages_send_error_total', 'service': 'cloud-volumes-message-proxy', 'pod': 'cvmp'}",MessagesSendErrorTotal,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert MessagesSendErrorTotal has fired on #$.labels.cluster# | #$.labels.kubernetes_pod_name# ', 'IcM.Description': 'There was an increase of sending errors for #$.labels.kubernetes_pod_name#, fails to send it to RabbitMQ, Please investigate', 'IcM.TsgId': 'https://msazure.visualstudio.com/DefaultCollection/One/_git/AzureNetApp-Docs?path=/docs/runbooks/cvmp-alerts.md'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'MessagesSendErrorTotal', 'description': 'There was an increase of sending errors for #$.labels.kubernetes_pod_name#, fails to send it to RabbitMQ, Please investigate', 'summary': 'Whenever CVMP receives a valid message and successfully converts the message for RabbitMQ but fails to send it to RabbitMQ.', 'dashboard': 'to-be-added'}","""Trigger an alert if, over the last 5 minutes, the total rate of AMQP send error messages is more than 10 per second, or if there has been at least one AMQP send error message."""
CvmpRuleGroup,eastus,Cloud Volumes Message Proxy Alerts,,sum(rate(amqp_messages_invalid_error_total[5m])) > 0 or max_over_time(amqp_messages_invalid_error_total[5m]) > 0,"{'severity': '2', 'metric': 'amqp_messages_invalid_error_total', 'service': 'cloud-volumes-message-proxy', 'pod': 'cvmp'}",MessagesInvalidErrorTotal,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert MessagesInvalidErrorTotal has fired on #$.labels.cluster# | #$.labels.kubernetes_pod_name# ', 'IcM.Description': 'There was an increase of invalid errors for #$.labels.kubernetes_pod_name#, total number of received messages without a message body, please investigate', 'IcM.TsgId': 'https://msazure.visualstudio.com/DefaultCollection/One/_git/AzureNetApp-Docs?path=/docs/runbooks/cvmp-alerts.md'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'MessagesInvalidErrorTotal', 'description': 'There was an increase of invalid errors for #$.labels.kubernetes_pod_name#, total number of received messages without a message body, please investigate', 'summary': 'Whenever CVMP receives an invalid message from ONTAP.', 'dashboard': 'to-be-added'}","""Trigger an alert if there has been at least one invalid error in AMQP messages within the last 5 minutes, either overall or at any point during that time."""
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum(increase(cvt_metric_receiver_success_message_processing[30m])) by (region) == 0,"{'metric': 'cvt_metric_receiver_success_message_processing', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtMetricReceiverNoMessageProcessedSuccessfully,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtMetricReceiverNoMessageProcessedSuccessfully has fired on #$.labels.region# for app cvt_metric_receiver', 'IcM.Description': 'No message has been processed successfully by the Metric Receiver in the last 5 minutes', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No message has been processed successfully by the Metric Receiver in the last 5 minutes', 'summary': 'No message has been processed successfully by the Metric Receiver in the last 5 minutes', 'dashboard': 'to-be-added'}","""The total number of successfully processed messages by the metric receiver in each region has not increased over the last 30 minutes, resulting in no new successful message processing in any region."""
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum(rate(cbs_metrics_total_message_published_to_queue[15m])) by (region) > 0 and sum(rate(cvt_metric_receiver_received_messages[15m])) by (region) == 0,"{'metric': 'cvt_metric_receiver_received_messages', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtMetricReceiverNoMessageReceived,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtMetricReceiverNoMessageReceived has fired on #$.labels.region#', 'IcM.Description': 'No message has been received by the Metric Receiver during the last 5 minutes', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No message has been received by the Metric Receiver during the last 5 minutes', 'summary': 'No message has been received by the Metric Receiver during the last 5 minutes', 'dashboard': 'to-be-added'}","There is a discrepancy in message processing between publishing and receiving in at least one region, where messages have been successfully published to a queue over the last 15 minutes, but no messages have been received in the same region during that time."
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,"sum(increase(cvt_job_executor_jobs_processed_total{status=""Success""}[20m])) by (region) == 0","{'metric': 'cvt_job_executor_jobs_processed_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPipelinesNoJobsProcessedSuccessfully,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPipelinesNoJobsProcessedSuccessfully has fired on #$.labels.region#', 'IcM.Description': 'No jobs have been processed successfully in the last 15 minutes', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No jobs have been processed successfully in the last 15 minutes', 'summary': 'No jobs have been processed successfully in the last 15 minutes', 'dashboard': 'to-be-added'}","""In the last 20 minutes, there were no successfully processed jobs in any region."""
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,"absent(up{job=~""cvt-metric-receiver.*""}) == 1","{'metric': 'up', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtMetricReceiverMissing,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtMetricReceiverMissing has fired on prometheusmdmeastus-stage', 'IcM.Description': 'No replica has been found for deployment cvt-metric-receiver for more than 20 minutes', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No replica has been found for deployment cvt-metric-receiver for more than 20 minutes', 'summary': 'No replica has been found for deployment cvt-metric-receiver for more than 20 minutes', 'dashboard': 'to-be-added'}","The service with a job name matching the pattern ""cvt-metric-receiver.*"" is down or not reporting, and has been in this state for at least 1 minute (since Prometheus evaluates rules every minute by default), indicating that there is no metric receiver running or it's not sending metrics."
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,"absent(up{job=~""cvt-pipelines.*""}) == 1","{'metric': 'up', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPipelinesMissing,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPipelinesMissing has fired on prometheusmdmeastus-stage', 'IcM.Description': 'No replica has been found for deployment cvt-pipelines for more than 20 minutes', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No replica has been found for deployment cvt-pipelines for more than 20 minutes', 'summary': 'No replica has been found for deployment cvt-pipelines for more than 20 minutes', 'dashboard': 'to-be-added'}","There is no instance running with a job name that matches the pattern ""cvt-pipelines.*"" and has been down for at least one minute, indicating that all expected instances of this job are missing or not reporting their status."
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (region) (rate(cvt_job_executor_jobs_processed_total{status=""Available"",job_type=~""PoolBillingJob|.*AggregationJob""}[5m])) > 0","{'metric': 'cvt_job_executor_jobs_processed_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPipelinesAggregationJobFailure,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPipelinesAggregationJobFailure has fired on #$.labels.region# for job_type PoolBillingJob or AggregationJob', 'IcM.Description': 'Continuous failures have been detected for one or more aggregation jobs over the past 10 minutes. Aggregation jobs will be retried until they succeed.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Continuous failures have been detected for one or more aggregation jobs over the past 10 minutes. Aggregation jobs will be retried until they succeed.', 'summary': 'Continuous failures have been detected for one or more aggregation jobs over the past 10 minutes. Aggregation jobs will be retried until they succeed.', 'dashboard': 'to-be-added'}","Show the total number of ""Available"" jobs processed per region for Pool Billing and Aggregation jobs, over a 5-minute period, but only if there is at least one job being processed in that region."
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (region) (rate(cvt_job_executor_jobs_processed_total{status=""Available"",job_type=~""Cvi.*|Ontap.*|BackupFiveMinutesCollection""}[5m])) > 2","{'metric': 'cvt_job_executor_jobs_processed_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPipelinesSamplingJobFailure,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPipelinesSamplingJobFailure has fired on #$.labels.region#', 'IcM.Description': 'Continuous failures have been detected for one or more sampling jobs over the past 10 minutes. Sampling jobs will be retried until they expire.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Continuous failures have been detected for one or more sampling jobs over the past 10 minutes. Sampling jobs will be retried until they expire.', 'summary': 'Continuous failures have been detected for one or more sampling jobs over the past 10 minutes. Sampling jobs will be retried until they expire.', 'dashboard': 'to-be-added'}","The total rate of successfully processed jobs for available CVI, Ontap, or BackupFiveMinutesCollection jobs in each region over the last 5 minutes is greater than 2 per second."
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (region) (rate(cvt_job_executor_jobs_processed_total{status=""Ignored""}[5m])) > 0","{'metric': 'cvt_job_executor_jobs_processed_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPipelinesJobIgnored,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPipelinesJobIgnored has fired on #$.labels.region#', 'IcM.Description': 'A job has been ignored and no further attempts will be made to process it.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'A job has been ignored and no further attempts will be made to process it.', 'summary': 'A job has been ignored and no further attempts will be made to process it.', 'dashboard': 'to-be-added'}","There is at least one job being ignored in each region over a 5-minute period, as measured by the rate of increase in the total number of ignored jobs processed by the cvt job executor."
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum by (region) (increase(cvt_azure_push_agent_error_count[3m])) >= 1,"{'metric': 'cvt_azure_push_agent_error_count', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtAzurePushAgentErrorCount,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtAzurePushAgentErrorCount has fired on #$.labels.region#', 'IcM.Description': 'PAV2 has failed to process billing records.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/cvt-pusher-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'PAV2 has failed to process billing records.', 'summary': 'PAV2 has failed to process billing records.', 'dashboard': 'to-be-added'}","There has been at least one increase in Azure push agent errors in any region over the last 3 minutes, grouped by region and summed across all occurrences."
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum(increase(cvt_nfsaas_cvt_pusher_partition_push_success[2h])) by (region) == 0,"{'metric': 'cvt_nfsaas_cvt_pusher_partition_push_success', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtNfsaasCvtPusherPartitionPushNotSuccessful,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtNfsaasCvtPusherPartitionPushNotSuccessful has fired on #$.labels.region# for cvt-pipelines', 'IcM.Description': 'No billing notifications have been sent to Azure Queue Storage for the past 1 hour.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/cvt-pusher-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No billing notifications have been sent to Azure Queue Storage for the past 1 hour.', 'summary': 'No billing notifications are being sent to Azure Queue Storage.', 'dashboard': 'to-be-added'}",There have been no successful partition pushes from the cvt pusher in any region over the last 2 hours.
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum by (region) (deriv(cvt_nfsaas_cvt_pusher_partition_push_failure[1h])) > 0,"{'metric': 'cvt_nfsaas_cvt_pusher_partition_push_failure', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtNfsaasCvtPusherPartitionPushFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtNfsaasCvtPusherPartitionPushFailure has fired on #$.labels.region#', 'IcM.Description': 'Errors encountered when writing data to Azure Queue Storage.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Errors encountered when writing data to Azure Queue Storage.', 'summary': 'CVT Azure Pusher has failed to send billing notification to PAV2.', 'dashboard': 'to-be-added'}","The rate of increase in partition push failures for the CVT NFSaaS pusher is greater than zero over the last hour, grouped by region, indicating that there are recent partition push failures in one or more regions."
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum(increase(cvt_nfsaas_cvt_pusher_partition_usage_push_success[2h])) by (region) == 0,"{'metric': 'cvt_nfsaas_cvt_pusher_partition_usage_push_success', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtNfsaasCvtPusherUsagePushNotSuccessful,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtNfsaasCvtPusherUsagePushNotSuccessful has fired on #$.labels.region#', 'IcM.Description': 'No billing data has been sent to Azure Table Storage for the last 1 hour.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/cvt-pusher-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No billing data has been sent to Azure Table Storage for the last 1 hour.', 'summary': 'No billing data is being sent to Azure Table Storage.', 'dashboard': 'to-be-added'}","The total number of successful partition usage pushes in each region has not increased over the last 2 hours, resulting in no new successes for any region."
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum by (region) (deriv(cvt_nfsaas_cvt_pusher_partition_usage_push_failure[1h])) > 0,"{'metric': 'cvt_nfsaas_cvt_pusher_partition_usage_push_failure', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtNfsaasCvtPusherUsagePushFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtNfsaasCvtPusherUsagePushFailure has fired on #$.labels.region#', 'IcM.Description': 'CVT Azure Pusher has failed to send billing data to PAV2', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Errors encountered when writing data to Azure Table Storage', 'summary': 'CVT Azure Pusher has failed to send billing data to PAV2'}",The rate of change in partition usage push failures for the cvt_nfsaas_cvt_pusher over the last hour is increasing in at least one region.
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum(increase(cvt_geneva_hotpath_push_success_count[15m])) by (region) == 0,"{'metric': 'cvt_geneva_hotpath_push_success_count', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtGenevaHotpathPushNotSuccessful,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtGenevaHotpathPushNotSuccessful has fired on #$.labels.region#', 'IcM.Description': 'No message is being processed to hotpath by Geneva Sink', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/cvt-geneva-hotpath'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No message is being processed to hotpath by Geneva Sink for the past 10 minutes', 'summary': 'No message is being processed to hotpath by Geneva Sink'}",There have been no successful Geneva hotpath pushes in any region over the last 15 minutes.
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum by (region) (rate(cvt_geneva_hotpath_push_failure_count[5m])) > 0,"{'metric': 'cvt_geneva_hotpath_push_failure_count', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtGenevaHotpathPushFailure,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtGenevaHotpathPushFailure has fired on #$.labels.region#', 'IcM.Description': 'Geneva Sink has failed to process hotpath metrics to Geneva', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/cvt-geneva-hotpath'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Geneva Sink has failed to process hotpath metrics to Geneva in the last 5 minutes', 'summary': 'Geneva Sink has failed to process hotpath metrics to Geneva'}",There is at least one Geneva hot path push failure in any region over the last 5 minutes.
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum(increase(cvt_geneva_warmpath_push_success_count[20m])) by (region) == 0,"{'metric': 'cvt_geneva_warmpath_push_success_count', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtGenevaWarmpathPushNotSuccessful,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtGenevaWarmpathPushNotSuccessful has fired on #$.labels.region#', 'IcM.Description': 'No message is being processed to warmpath by Geneva Sink', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No message is being processed to warmpath by Geneva Sink for the past 10 minutes', 'summary': 'No message is being processed to warmpath by Geneva Sink'}",There have been no successful warm path pushes in Geneva in any region over the last 20 minutes.
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum by (region) (rate(cvt_geneva_warmpath_push_failure_count[5m])) > 5,"{'metric': 'cvt_geneva_warmpath_push_failure_count', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtGenevaWarmpathPushFailure,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtGenevaWarmpathPushFailure has fired on #$.labels.region#', 'IcM.Description': 'Geneva Sink has failed to process warmpath metrics to Geneva', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Geneva Sink has failed to process warmpath metrics to Geneva in the last 5 minutes', 'summary': 'Geneva Sink has failed to process warmpath metrics to Geneva'}",The average rate of warm path push failures in Geneva over the last 5 minutes is greater than 5 per second when aggregated by region.
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum by (region) (deriv(cvt_nfsaas_cvt_pusher_duplicate_usage_count[1h])) > 0,"{'metric': 'cvt_nfsaas_cvt_pusher_duplicate_usage_count', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtNfsaasCvtPusherDuplicateUsageCount,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtNfsaasCvtPusherDuplicateUsageCount has fired on #$.labels.region#', 'IcM.Description': 'CVT Azure Pusher has tried inserting duplicate rows into Azure Table Storage', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'CVT Azure Pusher has tried inserting duplicate rows into Azure Table Storage in the last 3 hours', 'summary': 'CVT Azure Pusher has tried inserting duplicate rows into Azure Table Storage'}",The rate of change in duplicate usage count for NFS pusher over the last hour is increasing in at least one region.
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,sum by (region) (deriv(cvt_nfsaas_cvt_failed_creating_azure_client_count[1h])) > 0,"{'metric': 'cvt_nfsaas_cvt_failed_creating_azure_client_count', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtNfsaasCvtFailedCreatingAzureClientCount,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtNfsaasCvtFailedCreatingAzureClientCount has fired on #$.labels.region#', 'IcM.Description': 'CVT Azure Pusher has failed connecting to Azure Storage Account', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Geneva Sink has failed to process warmpath metrics to Geneva in the last 10 minutes', 'summary': 'CVT Azure Pusher has failed connecting to Azure Storage Account'}",The rate of change over the last hour in the number of failed attempts to create an Azure client for NFS as a service is increasing in at least one region.
CvtPipelineRuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (app, region) (rate(cvt_cvi_client_errors_total[5m])) / sum by (app, region) (rate(cvt_cvi_client_requests_total[5m])) > 0.01","{'metric': 'cvt_cvi_client_errors_total, cvt_cvi_client_requests_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtCviClientErrorRate,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtCviClientErrorRate has fired on #$.labels.region#', 'IcM.Description': 'High CVI client error rate detected for #$.labels.app#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The CVI client error rate for #$.labels.app# has been continuously above 1% for the last 5 minutes', 'summary': 'High CVI client error rate detected for #$.labels.app#'}","The error rate for client requests is higher than 1% over the last 5 minutes, when grouped by application and region."
Gs2RuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_bandwidth_adjustment_jobapi_failure_counter{code=~""5..""}[10m])) > 0","{'metric': 'gs_bandwidth_adjustment_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerBandwidthAdjustmentJobAPIHttp500Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerBandwidthAdjustmentJobAPIHttp500Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for bandwidth adjustment in the last 10 minutes, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for bandwidth adjustment in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 500 Failures for bandwidth adjustment', 'dashboard': 'to-be-added'}",There has been at least one increase in the number of 'gs_bandwidth_adjustment_jobapi' failures with a status code between 500 and 599 in any region over the last 10 minutes.
Gs2RuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_bandwidth_adjustment_jobapi_failure_counter{code=~""4..""}[10m])) > 0","{'metric': 'gs_bandwidth_adjustment_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerBandwidthAdjustmentJobAPIHttp400Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerBandwidthAdjustmentJobAPIHttp400Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for bandwidth adjustment in the last 10 minutes, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for bandwidth adjustment in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 400 Failures for bandwidth adjustment', 'dashboard': 'to-be-added'}","There is an increase in the number of 'gs_bandwidth_adjustment_jobapi' failures with error codes between 400 and 499, over the last 10 minutes, when grouped by region, with at least one such failure occurring."
Gs2RuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_monitor_jobapi_failure_counter{code=~""5..""}[10m])) > 0","{'metric': 'gs_monitor_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerMonitorJobAPIHttp500Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerMonitorJobAPIHttp500Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for ontap monitoring job in the last 10 minutes, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for ontap monitoring job in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 500 Failures for ontap monitoring job', 'dashboard': 'to-be-added'}",Trigger an alert when there is at least one increase in the number of job API failures with a 5xx status code in any region over the last 10 minutes.
Gs2RuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_monitor_jobapi_failure_counter{code=~""4..""}[10m])) > 0","{'metric': 'gs_monitor_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerMonitorJobAPIHttp400Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerMonitorJobAPIHttp400Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for ontap monitoring job in the last 10 minutes, please investigate'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for ontap monitoring job in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 400 Failures for ontap monitoring job', 'dashboard': 'to-be-added'}",Trigger an alert when there is at least one increase in the number of job API failures with a 4xx status code in any region over the last 10 minutes.
GeneralRuleGroup,eastus,General Alerts,,"100 * (count(up == 0) BY (job,component,app) / count(up) BY (job,component,app)) > 10","{'metric': 'general_up', 'service': 'general', 'severity': 'critical'}",TargetDown,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert TargetDown has fired on prometheusmdmeastus-stage', 'IcM.Description': '#$.labels.value#% or more of #$.labels.job#/#$.labels.component#/#$.labels.app# targets are down.'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': '#$.labels.value#% or more of #$.labels.job#/#$.labels.component#/#$.labels.app# targets are down.', 'summary': 'Targets are down', 'dashboard': 'to-be-added'}","Trigger an alert when more than 10% of instances for a specific job, component, and application are down."
GeneralRuleGroup,eastus,General Alerts,,"100 * (count(up == 0) BY (job,component,app) / count(up) BY (job,component,app)) == 100","{'metric': 'general_all_up', 'service': 'general', 'severity': 'critical'}",AllTargetsDown,False,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert AllTargetsDown has fired on #$.labels.region#', 'IcM.Description': '#$.labels.value#% or more of #$.labels.job#/#$.labels.component#/#$.labels.app# targets are down.'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': '#$.labels.value#% or more of #$.labels.job#/#$.labels.component#/#$.labels.app# targets are down.', 'summary': 'All targets are down', 'dashboard': 'to-be-added'}","""Alert when all instances of a job, component, and app are down, meaning 100% of them are not reporting as 'up'."""
GeneralRuleGroup,eastus,General Alerts,,vector(1),"{'metric': 'vector', 'service': 'general', 'severity': 'critical'}",DeadMansSwitch,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert DeadMansSwitch has fired on #$.labels.region#', 'IcM.Description': 'This is a DeadMansSwitch meant to ensure that the entire Alerting pipeline is functional.'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'This is a DeadMansSwitch meant to ensure that the entire Alerting pipeline is functional.', 'summary': 'The Alerting pipeline is functional.', 'dashboard': 'to-be-added'}","Always return 1, regardless of any other conditions or metrics."
GeneralRuleGroup,eastus,General Alerts,,100 * (process_open_fds / process_max_fds) > 95,"{'metric': 'process_open_fds', 'service': 'general', 'severity': 'critical'}",TooManyOpenFileDescriptors,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert TooManyOpenFileDescriptors has fired on #$.labels.region#', 'IcM.Description': '#$.labels.job#: #$.labels.namespace#/#$.labels.pod# (#$.labels.instance#) is using #$.labels.value#% of the available file/socket descriptors.'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': '#$.labels.job#: #$.labels.namespace#/#$.labels.pod# (#$.labels.instance#) is using #$.labels.value#% of the available file/socket descriptors.', 'summary': 'too many open file descriptors', 'dashboard': 'to-be-added'}","The system is running low on available file descriptors, with more than 95% of the maximum allowed open file descriptors currently in use by a process."
GeneralRuleGroup,eastus,General Alerts,,"predict_linear(instance:fd_utilization[1h], 3600 * 4) > 1","{'metric': 'predict_linear', 'service': 'general', 'severity': 'critical'}",FdExhaustionClose,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FdExhaustionClose has fired on #$.labels.region#', 'IcM.Description': '#$.labels.job#: #$.labels.namespace#/#$.labels.pod# (#$.labels.instance#) instance will exhaust in file/socket descriptors soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': '#$.labels.job#: #$.labels.namespace#/#$.labels.pod# (#$.labels.instance#) instance will exhaust in file/socket descriptors soon', 'summary': 'file descriptors soon exhausted', 'dashboard': 'to-be-added'}","""In one hour, if the current rate of change of disk utilization for this instance continues, it is predicted to exceed 100% within the next 4 hours."""
GeneralRuleGroup,eastus,General Alerts,,"predict_linear(instance:fd_utilization[10m], 3600) > 1","{'metric': 'predict_linear', 'service': 'general', 'severity': 'critical'}",FdExhaustionCloseCritical,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FdExhaustionCloseCritical has fired on #$.labels.region#', 'IcM.Description': '#$.labels.job#: #$.labels.namespace#/#$.labels.pod# (#$.labels.instance#) instance will exhaust in file/socket descriptors soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': '#$.labels.job#: #$.labels.namespace#/#$.labels.pod# (#$.labels.instance#) instance will exhaust in file/socket descriptors soon', 'summary': 'file descriptors soon exhausted', 'dashboard': 'to-be-added'}",The disk utilization of this instance is predicted to exceed 100% within the next hour based on its usage over the last 10 minutes.
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (app,region) (rate(cvt_ontap_client_errors_total[20m])) / sum by (app,region) (rate(cvt_ontap_client_requests_total[20m])) > 0.55","{'metric': 'cvt_ontap_client_errors_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtOntapClientErrorRate,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtOntapClientErrorRate has fired on #$.labels.region# for app #$.labels.app#', 'IcM.Description': 'High ONTAP client error rate detected for #$.labels.app#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The ONTAP client error rate for #$.labels.app# has been continuously above 1% for the last 5 minutes', 'summary': 'High ONTAP client error rate detected for #$.labels.app#'}","The error rate for client requests to the Ontap system is higher than 55% over the last 20 minutes, when considering each application and region separately."
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum(rate(cvt_ontap_client_errors_total{path=""/api/cluster/counter/tables/wafl_comp_aggr_vol_bin/rows""}[5m])) / sum(rate(cvt_ontap_client_requests_total{path=""/api/cluster/counter/tables/wafl_comp_aggr_vol_bin/rows""}[5m])) > 0.55","{'metric': 'cvt_ontap_client_errors_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtOntapClientCoolTierErrors,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtOntapClientCoolTierErrors has fired on #$.labels.region#', 'IcM.Description': 'High ONTAP client error rate detected for cool-tier metrics', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The ONTAP client error rate for cool-tier metrics has been continuously above 1% for the last 5 minutes', 'summary': 'High ONTAP client error rate detected for cool-tier metrics'}","The error rate for client requests to the ""/api/cluster/counter/tables/wafl_comp_aggr_vol_bin/rows"" API endpoint over the last 5 minutes is more than 55%."
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (app,region) (rate(cvt_postgres_errors_total[5m])) / sum by (app,region) (rate(cvt_postgres_queries_total[5m])) > 0.01","{'metric': 'cvt_postgres_errors_total, cvt_postgres_queries_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPostgresQueryErrorRate,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPostgresQueryErrorRate has fired on #$.labels.region# for app #$.labels.app#', 'IcM.Description': 'High Postgres query error rate detected for #$.labels.app#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The Postgres query error rate for #$.labels.app# has been continuously above 1% for the last 5 minutes', 'summary': 'High Postgres query error rate detected for #$.labels.app#'}","""Trigger an alert when the average rate of PostgreSQL errors over the last 5 minutes is more than 1% of the average rate of total PostgreSQL queries over the same period, grouped by application and region."""
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (app,region) (rate(cvt_redis_client_errors_total[5m])) / sum by (app,region) (rate(cvt_redis_client_requests_total[5m])) > 0.01","{'metric': 'cvt_redis_client_errors_total, cvt_redis_client_requests_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtRedisClientErrorRate,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtRedisClientErrorRate has fired on #$.labels.region# for app #$.labels.app#', 'IcM.Description': 'High Redis client error rate detected for #$.labels.app#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The Redis client error rate for #$.labels.app# has been continuously above 1% for the last 5 minutes', 'summary': 'High Redis client error rate detected for #$.labels.app#'}","""Trigger an alert when the proportion of Redis client errors to total requests exceeds 1% over a 5-minute period for any application and region combination."""
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum(pgxpool_total_conns{db_name=""cloud_volumes_telemetry"", app=""cvt-metric-receiver""}) < 1 and sum(rate(cvt_metric_receiver_messages_received_total{processor=""persistedmetricreceiver""}[10m])) > 0","{'metric': 'pgxpool_total_conns, cvt_metric_receiver_messages_received_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPostgresNoCvtConnectionsMetricReceiver,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPostgresNoCvtConnectionsMetricReceiver has fired on #$.labels.region# for app #$.labels.app#', 'IcM.Description': 'No Postgres connections from Metric Receiver to the cloud_volumes_telemetry database'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No Postgres connections from Metric Receiver to the cloud_volumes_telemetry database in the last 10 minutes', 'summary': 'No Postgres connections from Metric Receiver to the cloud_volumes_telemetry database'}","There are less than 1 active database connections to ""cloud_volumes_telemetry"" from the ""cvt-metric-receiver"" application, and at the same time, the ""cvt-metric-receiver"" is successfully receiving messages, as indicated by an increasing rate of received messages over the last 10 minutes."
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (app,region) (pgxpool_total_conns{db_name=""cloud_volumes_telemetry"", app!=""cvt-metric-receiver""}) < 1","{'metric': 'pgxpool_total_conns', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPostgresNoCvtConnections,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPostgresNoCvtConnections has fired on #$.labels.region# for app #$.labels.app#', 'IcM.Description': 'No Postgres connections from #$.labels.app# to the cloud_volumes_telemetry database', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No Postgres connections from #$.labels.app# to the cloud_volumes_telemetry database in the last 10 minutes', 'summary': 'No Postgres connections from #$.labels.app# to the cloud_volumes_telemetry database'}","The total number of active connections to the ""cloud_volumes_telemetry"" database is less than 1 for at least one combination of application and region, excluding the ""cvt-metric-receiver"" application."
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (app,region) (pgxpool_total_conns{db_name=""metrics_collector""}) < 1","{'metric': 'pgxpool_total_conns', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPostgresNoMcConnections,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPostgresNoMcConnections has fired on #$.labels.region# for app #$.labels.app#', 'IcM.Description': 'No Postgres connections from #$.labels.app# to the metrics_collector database', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No Postgres connections from #$.labels.app# to the metrics_collector database in the last 40 minutes', 'summary': 'No Postgres connections from #$.labels.app# to the metrics_collector database'}","The total number of active connections to the ""metrics_collector"" database is less than 1 for at least one combination of application and region."
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (app,region) (pgxpool_total_conns{db_name=""cbs""}) < 1","{'metric': 'pgxpool_total_conns', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPostgresNoCbsConnections,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPostgresNoCbsConnections has fired on #$.labels.region# for app #$.labels.app#', 'IcM.Description': 'No Postgres connections from #$.labels.app# to the cbs database'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No Postgres connections from #$.labels.app# to the cbs database in the last 10 minutes', 'summary': 'No Postgres connections from #$.labels.app# to the cbs database'}","The total number of active connections to the ""cbs"" database is less than 1 for at least one combination of application and region."
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (app,region) (cvt_rabbitmq_client_open_connections{app!~""cvt-cvi-scraper|cvt-pipelines""}) < 1","{'metric': 'cvt_rabbitmq_client_open_connections', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtNoRabbitMqConnections,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtNoRabbitMqConnections has fired on #$.labels.region# for app #$.labels.app#', 'IcM.Description': 'No RabbitMQ connections open for #$.labels.app#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No RabbitMQ connections have been open for #$.labels.app# in the last 10 minutes', 'summary': 'No RabbitMQ connections open for #$.labels.app#'}","Trigger an alert when the total number of open RabbitMQ client connections is less than 1 for any combination of application and region, excluding applications named ""cvt-cvi-scraper"" and ""cvt-pipelines""."
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (app,region) (cvt_rabbitmq_client_open_channels{app!~""cvt-cvi-scraper|cvt-pipelines""}) < 1","{'metric': 'cvt_rabbitmq_client_open_channels', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtNoRabbitMqChannels,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtNoRabbitMqChannels has fired on #$.labels.region# for app #$.labels.app#', 'IcM.Description': 'No RabbitMQ channels open for #$.labels.app#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No RabbitMQ channels have been open for #$.labels.app# in the last 10 minutes', 'summary': 'No RabbitMQ channels open for #$.labels.app#'}",The number of open RabbitMQ channels is less than 1 for any application (excluding cvt-cvi-scraper and cvt-pipelines) in each region.
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (job_type,region) (rate(cvt_job_executor_jobs_processed_total{status=""Expired""}[10m])) > 0","{'metric': 'cvt_job_executor_jobs_processed_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPipelinesJobExpired,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPipelinesJobExpired has fired on #$.labels.region# for job type #$.labels.job_type#', 'IcM.Description': 'A job of type #$.labels.job_type# has expired', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'A job of type #$.labels.job_type# has expired in the last 10 minutes and no further attempts will be made to process it', 'summary': 'A job of type #$.labels.job_type# has expired'}","""Alert when there are any expired jobs processed in the last 10 minutes for any job type and region."""
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (job_type,region) (increase(cvt_job_source_jobs_found_total{job_type!~""PoolBillingJob|.*AggregationJob""}[20m])) == 0","{'metric': 'cvt_job_source_jobs_found_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPipelinesNoFrequentJobsFound,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPipelinesNoFrequentJobsFound has fired on #$.labels.region# for job type #$.labels.job_type#', 'IcM.Description': 'No jobs of type #$.labels.job_type# on region on #$.labels.region# have been found'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No jobs of type #$.labels.job_type# have been found by the Job Source in the last 10 minutes', 'summary': 'No jobs of type #$.labels.job_type# have been found'}","The total number of new jobs found in the last 20 minutes is zero for each combination of job type and region, excluding Pool Billing Jobs and Aggregation Jobs."
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (job_type,region) (increase(cvt_job_source_jobs_found_total{job_type=~""PoolBillingJob|.*AggregationJob""}[1h10m])) == 0","{'metric': 'cvt_job_source_jobs_found_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPipelinesNoHourlyJobsFound,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPipelinesNoHourlyJobsFound has fired on #$.labels.region# for job type #$.labels.job_type#', 'IcM.Description': 'No aggregation jobs of type #$.labels.job_type# have been found'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No aggregation jobs of type #$.labels.job_type# have been found by the Job Source in the last 70 minutes', 'summary': 'No jobs of type #$.labels.job_type# have been found'}",Alert when there are no new jobs found for Pool Billing or Aggregation Jobs in any region over the last 1 hour and 10 minutes.
CvtPipeline2RuleGroup,eastus,Alerts on Cvt Pipeline,,"sum by (error_type,region) (increase(cvt_azure_billing_mapper_deleted_subscription_client_error_total[2h])) > 0","{'metric': 'cvt_azure_billing_mapper_deleted_subscription_client_error_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtPipelinesDeletedSubscriptionsClientError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtPipelinesDeletedSubscriptionsClientError has fired in region #.labels.region# for error_type #.labels.error_type#. Error accessing deleted subscription list', 'IcM.Description': 'Error of type #$.labels.error_type# has occured while trying to get deleted subscription list. This means that billing has been submitted for resources belonging to deleted subscriptions.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-pipelines'}}]",{'autoResolved': False},"{'description': 'Error accessing deleted subscription list. Error type: #$.labels.error_type#', 'summary': 'Error of type #$.labels.error_type# has occured while trying to get deleted subscription list.'}",There has been at least one increase in deleted subscription client errors for any error type and region over the last 2 hours.
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,sum by (region) (exporter_successful != 1 or increase(exporter_empty[1d1h]) != 0 or increase(exporter_failed[1d1h]) != 0),"{'metric': 'exporter_successful', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",ExporterNoDataExportedFor25h,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert ExporterNoDataExportedFor25h has fired on prometheusmdmeastus-stage', 'IcM.Description': 'No cloud-volumes-telemetry exporter jobs scheduled in the last 25 hours', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/observability/monitors/cloudvolumetelemetry/exporternodataexportedfor25h'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': ""No cloud-volumes-telemetry exporter jobs in the last 25 hours. Job has not been scheduled by Kubernetes and bizops don't have their report. Manual trigger required."", 'summary': 'No cloud-volumes-telemetry exporter jobs scheduled in the last 25 hours', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","Trigger an alert when, for each region, any of the following conditions are met: 
- The exporter was not fully successful, 
- There was an increase in empty exports over the last 37 hours, 
- There was an increase in failed exports over the last 37 hours."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,sum by (region) (exporter_empty > 0),"{'metric': 'exporter_empty', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",ExporterEmpty,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert ExporterEmpty has fired on #$.labels.region#', 'IcM.Description': 'Exporter #$.labels.pod_name# ran successfully but produced no data', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/observability/monitors/cloudvolumetelemetry/exporterempty'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': ""Exporter #$.labels.pod_name# ran successfully but produced no data. This is an abnormal state because the exporter runs on older records in qstack_usage.nfsaas_aggregated_usage. It's possible the metrics delivery pipeline might be disrupted."", 'summary': 'Exporter #$.labels.pod_name# ran successfully but produced no data', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","Show the total number of exporters with no data, grouped by region."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,sum by (region) (exporter_failed > 0),"{'metric': 'exporter_failed', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",ExporterFailed,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert ExporterFailed has fired on prometheusmdmeastus-stage', 'IcM.Description': 'The cloud-volumes-telemetry exporter has failed to export data for 3 consecutive attempts.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/observability/monitors/cloudvolumetelemetry/exporterfailed'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Export job has failed. Kubernetes system will automatically attempt 5 more times with exponential backoff until giving up.', 'summary': 'The cloud-volumes-telemetry exporter has failed to export data for 3 consecutive attempts.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","Show the total number of failed exporters, grouped by region, where at least one exporter has failed."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"sum by (region) (redis_errors_total{service=""cvt-collector""}) >= 1","{'metric': 'redis_errors_total', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CVTcollectorRedisConnection,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVTcollectorRedisConnection has fired on #$.labels.region#', 'IcM.Description': 'Connection between cvt-collector and redis is down', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'cvt-collector is having issues connecting to redis. Please investigate', 'summary': 'Connection between cvt-collector and redis is down', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}",The total number of Redis errors in each region for the cvt-collector service has reached or exceeded 1.
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"postgress_connection_status{service=""cvt-collector-supervisor""} >= 5","{'metric': 'postgress_connection_status', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CVTcollectorPostgresConnection,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVTcollectorPostgresConnection has fired on #$.labels.region#', 'IcM.Description': 'Connection between cvt collector supervisor and postgres is down', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'cvt collector supervisor is having issues connecting to postgres. Please investigate', 'summary': 'Connection between cvt collector supervisor and postgres is down', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","The PostgreSQL connection status for the cvt-collector-supervisor service has been at or above 5 for an extended period of time, indicating a potential issue with the database connection."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"sum by (region) (rate(cvt_collector_job_failure_count{type=""process_cool_tier_job""}[5m])) > 0","{'metric': 'cvt_collector_job_failure_count', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtCollectorCoolTierJobFailures,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtCollectorCoolTierJobFailures has fired on #$.labels.region#', 'IcM.Description': 'CVT collector cool tier job failed', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-collector-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Unable to process cvt-collector cool tier job', 'summary': 'CVT collector cool tier job failed', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","The rate of job failures for the process cool tier job is increasing in any region over the last 5 minutes, with at least one failure occurring."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"sum by (region) (rate(cvt_collector_job_failure_count{type=""process_pool_usage_job""}[5m])) > 0","{'metric': 'cvt_collector_job_failure_count', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",CvtCollectorPoolUsageJobFailures,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvtCollectorPoolUsageJobFailures has fired on #$.labels.region#', 'IcM.Description': 'CVT collector pool usage job failed', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvt-collector-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Unable to process cvt-collector pool usage job', 'summary': 'CVT collector pool usage job failed', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","The rate of job failures for the process pool usage job is increasing in any region over the last 5 minutes, with at least one failure occurring."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"cvt_ems_handler_ems_cache_creation_error{code=""ems_cache_creation_error""} >= 1","{'metric': 'cvt_ems_handler_ems_cache_creation_error', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",EmsCacheCreationError,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert EmsCacheCreationError has fired on #$.labels.region#', 'IcM.Description': 'Error creating Redis cache', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The cvt-ems-handler pod is not able to fetch cache data from redis. Check if the redis pod is running and cvt-ems-handler is able to connect to it.', 'summary': 'Error creating Redis cache', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","Trigger an alert when there is at least one error occurring in the EMS cache creation process, as indicated by the ems_cache_creation_error code in the cvt_ems_handler metric."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"cvt_ems_handler_ems_certificate_not_found{code=""ems_certificate_not_found""} >= 1","{'metric': 'cvt_ems_handler_ems_certificate_not_found', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",EmsCertificateNotFound,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert EmsCertificateNotFound has fired on #$.labels.region#', 'IcM.Description': 'EMS certificate not found', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The cvt-ems-handler pod did not get the EMS certificate from AKV. Make sure the EMS certificate exists in AKV and cvt-ems-handler is able to fetch it.', 'summary': 'EMS certificate not found', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","Trigger an alert when at least one EMS certificate is not found, as indicated by the 'cvt_ems_handler_ems_certificate_not_found' metric with a code of ""ems_certificate_not_found""."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"cvt_ems_handler_ems_fluentd_connection_error{code=""ems_fluentd_connection_error""} >= 1","{'metric': 'cvt_ems_handler_ems_fluentd_connection_error', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",EmsFluentdConnectionError,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert EmsFluentdConnectionError has fired on #$.labels.region#', 'IcM.Description': 'Error creating Fluentd client, geneva logs', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The cvt-ems-handler pod to able to connect to fluentd pods. Make sure they are running and reachable.', 'summary': 'Error creating Fluentd client, geneva logs', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","Trigger an alert when there is at least one error in the EMS Fluentd connection, as indicated by the cvt_ems_handler metric reporting an ""ems_fluentd_connection_error""."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"cvt_ems_handler_ems_statsd_connection_error{code=""ems_statsd_connection_error""} >= 1","{'metric': 'cvt_ems_handler_ems_statsd_connection_error', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",EmsStatsdConnectionError,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert EmsStatsdConnectionError has fired on #$.labels.region#', 'IcM.Description': 'Error creating Statsd client, geneva metrics', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The cvt-ems-handler pod is not able to connect to the geneva-hotpath and/or geneva-hotpath-public pods. Make sure they are up and running and reachable.', 'summary': 'Error creating Statsd client, geneva metrics', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","Trigger an alert when there is at least one error in connecting to the EMS statsd, as indicated by the ems_statsd_connection_error code."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"cvt_ems_handler_ems_metadata_cache_empty{code=""ems_metadata_cache_empty""} >= 1","{'metric': 'cvt_ems_handler_ems_metadata_cache_empty', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",EmsMetadataCacheEmpty,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert EmsMetadataCacheEmpty has fired on #$.labels.region#', 'IcM.Description': 'ONTAP metadata cache received from redis is empty', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The cvt-ems-handler pod received empty metadata from the redis cache. Make sure the metadata for ONTAP is being generated correctly in redis. If not make sure that the ONTAP host exists in the storage db', 'summary': 'ONTAP metadata cache received from redis is empty', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","The ""cvt_ems_handler_ems_metadata_cache_empty"" metric has a value greater than or equal to 1 when the code is ""ems_metadata_cache_empty"", indicating that the EMS metadata cache is empty."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"cvt_syslog_handler_syslog_cache_creation_error{code=""syslog_cache_creation_error""} >= 1","{'metric': 'cvt_syslog_handler_syslog_cache_creation_error', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",SyslogCacheCreationError,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert SyslogCacheCreationError has fired on #$.labels.region#', 'IcM.Description': 'Error creating Redis cache', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The cvt-syslog-handler pod is not able to fetch cache data from redis. Check if the redis pod is running and cvt-syslog-handler is able to connect to it.', 'summary': 'Error creating Redis cache', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","Trigger an alert when there is at least one error occurring in the creation of the syslog cache, as indicated by the syslog handler."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"cvt_syslog_handler_syslog_certificate_not_found{code=""syslog_certificate_not_found""} >= 1","{'metric': 'cvt_syslog_handler_syslog_certificate_not_found', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",SyslogCertificateNotFound,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert SyslogCertificateNotFound has fired on #$.labels.region#', 'IcM.Description': 'Syslog certificate not found', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The cvt-syslog-handler pod did not get the EMS certificate from AKV. Make sure the EMS certificate exists in AKV and cvt-syslog-handler is able to fetch it.', 'summary': 'Syslog certificate not found', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","Trigger an alert when there is at least one occurrence of a syslog certificate not being found, as indicated by the cvt_syslog_handler_syslog_certificate_not_found metric with the code ""syslog_certificate_not_found""."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"cvt_syslog_handler_syslog_fluentd_connection_error{code=""syslog_fluentd_connection_error""} >= 1","{'metric': 'cvt_syslog_handler_syslog_fluentd_connection_error', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",SyslogFluentdConnectionError,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert SyslogFluentdConnectionError has fired on #$.labels.region#', 'IcM.Description': 'Error creating Fluentd client, geneva logs', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The cvt-syslog-handler pod to able to connect to fluentd pods. Make sure they are running and reachable.', 'summary': 'Error creating Fluentd client, geneva logs', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","Trigger an alert when there is at least one error in the connection to Fluentd from the syslog handler, indicating a problem with sending logs to Fluentd."
CvtExporterEmsRuleGroup,eastus,Alerts on CvtExporterEms,,"cvt_syslog_handler_syslog_metadata_cache_empty{code=""syslog_metadata_cache_empty""} >= 1","{'metric': 'cvt_syslog_handler_syslog_metadata_cache_empty', 'service': 'cloud-volumes-telemetry', 'pod': 'cvt'}",SyslogMetadataCacheEmpty,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert SyslogMetadataCacheEmpty has fired on #$.labels.region#', 'IcM.Description': 'ONTAP metadata cache received from redis is empty', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The cvt-syslog-handler pod received empty metadata from the redis cache. Make sure the metadata for ONTAP is being generated correctly in redis. If not make sure that the ONTAP host exists in the storage db', 'summary': 'ONTAP metadata cache received from redis is empty', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/L5XmgF34k/cloud-volumes-telemetry'}","The syslog metadata cache is empty, as indicated by at least one occurrence of the ""syslog_metadata_cache_empty"" error code in the cvt_syslog_handler_syslog system."
VrkmRuleGroup,eastus,Alerts on Vrkm,,"vrkm_scheduler_processqueue_jobs{job_type=""error""} > 0","{'metric': 'vrkm_scheduler_processqueue_jobs', 'service': 'volume-rekey-manager', 'pod': 'vrkm'}",VRKMJobError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VRKMJobError has fired on #$.labels.region#', 'IcM.Description': 'There is a rekey job on the ONTAP node #$.labels.node# that has an error'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There is a rekey job on the ONTAP node #$.labels.node# that has an error', 'summary': 'Rekey jobs in error state'}","There are more jobs in the error process queue of the VRKM scheduler than expected, specifically more than zero."
VrkmRuleGroup,eastus,Alerts on Vrkm,,increase(vrkm_scheduler_add_volumes_to_queue_errors[24h]) > 0,"{'metric': 'vrkm_scheduler_add_volumes_to_queue_errors', 'service': 'volume-rekey-manager', 'pod': 'vrkm'}",VRKMAddVolumeToJobQueueError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VRKMAddVolumeToJobQueueError has fired on #$.labels.region#', 'IcM.Description': 'There is an increase in error type #$.labels.error# when adding volumes to job queue'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There is an increase in error type #$.labels.error# when adding volumes to job queue', 'summary': 'Error while adding volumes to job queue'}",There has been an increase in the number of errors when adding volumes to the scheduler queue over the last 24 hours.
VrkmRuleGroup,eastus,Alerts on Vrkm,,increase(vrkm_scheduler_handle_messages_errors[24h]) > 0,"{'metric': 'vrkm_scheduler_handle_messages_errors', 'service': 'volume-rekey-manager', 'pod': 'vrkm'}",VRKMHandleMessagesError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VRKMHandleMessagesError has fired on #$.labels.region#', 'IcM.Description': 'There is an increase in error type #$.labels.error# when handling ONTAP messages'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There is an increase in error type #$.labels.error# when handling ONTAP messages', 'summary': 'Error while handling ONTAP message'}",There has been an increase in the number of errors handling messages by the vrkm scheduler over the last 24 hours.
VrkmRuleGroup,eastus,Alerts on Vrkm,,increase(vrkm_scheduler_processqueue_errors[24h]) > 0,"{'metric': 'vrkm_scheduler_processqueue_errors', 'service': 'volume-rekey-manager', 'pod': 'vrkm'}",VRKMProcessQueueError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VRKMProcessQueueError has fired on #$.labels.region#', 'IcM.Description': 'There is an increase in error type #$.labels.error# when processing the job queue'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There is an increase in error type #$.labels.error# when processing the job queue', 'summary': 'Error while processing the job queue'}",There has been an increase in scheduler process queue errors over the last 24 hours.
Vp2RuleGroup,eastus,Alerts on Volume Placement,,sum(increase(vps_failed_http_requests_total[5m])) > 0 or max(vps_failed_http_requests_total != 0 unless vps_failed_http_requests_total offset 5m),"{'severity': 'critical', 'metric': 'vps_failed_http_requests_total', 'service': 'cloud-volumes-service', 'pod': 'cvs'}",VPSHttpRequestError,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VPSHttpRequestError has fired on #$.labels.region# | #$.labels.service# | #$.labels.pod#', 'IcM.Description': 'There was an increase of HTTP errors for #$.labels.pod#, HTTP requests for CVS to VPS have failed', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvs-prometheus-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'VPSHttpRequestError', 'description': 'There was an increase of HTTP errors for #$.labels.pod#, HTTP requests for CVS to VPS have failed', 'summary': 'HTTP requests for CVS to VPS have failed', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/e6278aad-0b5e-43b4-b78f-c1bc2651a5a1/cloud-volumes'}","""Trigger an alert if there has been at least one increase in the total number of failed HTTP requests on any VPS within the last 5 minutes, or if there is currently a non-zero number of failed HTTP requests and there were no failed requests 5 minutes ago."""
Vp2RuleGroup,eastus,Alerts on Volume Placement,,sum(hyperscaler_cluster_stale_document) by (region) > 0,"{'metric': 'hyperscaler_cluster_stale_document', 'service': 'vps', 'pod': 'vps'}",VpsStaleHyperscalerDocuments,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VpsStaleHyperscalerDocuments has fired on #$.labels.region#', 'IcM.Description': 'The VPS service has stale hyperscaler resource usage information', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The VPS service has stale hyperscaler resource usage information', 'summary': 'VPS hyperscaler resource usage information is stale', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}",There are stale documents in at least one region across all hyperscaler clusters.
Vp2RuleGroup,eastus,Alerts on Volume Placement,,sum(rate(vps_invalid_vds[10m])) by (region) > 0,"{'metric': 'vps_invalid_vds', 'service': 'vps', 'pod': 'vps'}",VpsBadFormat,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VpsBadFormat has fired on #$.labels.region#', 'IcM.Description': 'The VPS service has received poorly formed volume allocation requests', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The VPS service has received poorly formed volume allocation requests', 'summary': 'This VPS service has received poorly formed volume allocation requests. This is a coding error.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","Show regions where there has been at least one invalid VDS event in the last 10 minutes, grouped by region."
Vp2RuleGroup,eastus,Alerts on Volume Placement,,100*(sum(rate(vps_allocate_volume_failure[10m])) by (region) / (sum(rate(vps_allocate_volume_failure[10m])) by (region) + sum(rate(vps_allocate_volume_success[10m])) by (region))) > 30,"{'metric': 'vps_allocate_volume_failure', 'service': 'vps', 'pod': 'vps'}",VolumePlacementFailureRate,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VolumePlacementFailureRate has fired on #$.labels.region#', 'IcM.Description': 'Volume placement failure rate is #$.labels.value# for a 10 minute period', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Volume placement failure rate is #$.labels.value# for a 10 minute period', 'summary': 'Volume placement failure rate is greater than 30% for a 10 minute period', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","The percentage of failed volume allocations in each region over the last 10 minutes is greater than 30%, calculated as the number of failed allocations divided by the total number of allocations (failed and successful)."
Vp2RuleGroup,eastus,Alerts on Volume Placement,,100 * (sum(rate(vps_reallocate_volume_failure[10m])) by (region) / (sum(rate(vps_reallocate_volume_failure[10m])) by (region) + sum(rate(vps_reallocate_volume_success[10m])) by (region))) > 10,"{'metric': 'vps_reallocate_volume_failure', 'service': 'vps', 'pod': 'vps'}",VolumeReallocationFailureRate,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VolumeReallocationFailureRate has fired on #$.labels.region#', 'IcM.Description': 'Volume resizes are failing #$.labels.value# times in a 10 minute period', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Volume resizes are failing #$.labels.value# times in a 10 minute period', 'summary': 'Volume resizes are failing in a 10 minute period', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","The percentage of failed volume reallocations in each region over the last 10 minutes is greater than 10%, where failure rate is calculated as the number of failed reallocations divided by the total number of reallocation attempts."
Vp2RuleGroup,eastus,Alerts on Volume Placement,,absent_over_time(ontap_cluster_svm_count_usage[20m]),"{'metric': 'ontap_cluster_svm_count_usage', 'service': 'vps', 'pod': 'vps'}",NoUsageStats,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NoUsageStats has fired on prometheusmdmeastus-stage', 'IcM.Description': 'No ONTAP usage metrics reported in the last 10 minutes.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No ONTAP usage metrics reported in the last 10 minutes.', 'summary': 'This alert is triggered if there have been no ONTAP usage metrics reported in the last 10 minutes.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}",There has been no data for the number of SVMs (Storage Virtual Machines) in use on an ONTAP cluster for the last 20 minutes.
Vp2RuleGroup,eastus,Alerts on Volume Placement,,absent_over_time(ontap_cluster_svm_count_usage[10m:5m]),"{'metric': 'ontap_cluster_svm_count_usage', 'service': 'vps', 'pod': 'vps'}",NoUsageStats-New,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NoUsageStats-New has fired on prometheusmdmeastus-stage', 'IcM.Description': 'No ONTAP usage metrics reported in the last 10 minutes.', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No ONTAP usage metrics reported in the last 10 minutes.', 'summary': 'This alert is triggered if there have been no ONTAP usage metrics reported in the last 10 minutes.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","There has been no data for the number of SVMs (Storage Virtual Machines) in use on an ONTAP cluster for at least 10 minutes, with a 5-minute tolerance for missing data."
Vp2RuleGroup,eastus,Alerts on Volume Placement,,sum(increase(periodic_job_success[10m])) by (region) == 0,"{'metric': 'periodic_job_success', 'service': 'vps', 'pod': 'vps'}",NoSuccessfulUsageRefreshes,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NoSuccessfulUsageRefreshes has fired on #$.labels.region#', 'IcM.Description': 'No successful refreshes, either DRP or ONTAP in that last 10 minutes. Indicates rt-workers are down or otherwise not functioning', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'No successful refreshes, either DRP or ONTAP in that last 10 minutes. Indicates rt-workers are down or otherwise not functioning', 'summary': 'Triggered when there are no successful refreshes, either DRP or ONTAP in that last 10 minutes. Indicates rt-workers are down or otherwise not functioning', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""The total number of successful periodic jobs in each region has not increased at all over the last 10 minutes."""
LinkerdRuleGroup,eastus,Alerts on Linkerd,,sum(rate(request_total[10m])) by (region) < 100,"{'severity': '3', 'metric': 'request_total', 'service': 'linkerd', 'pod': 'linkerd'}",RequestTotal,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Linkerd Degradation has fired on #$.labels.region#', 'IcM.Description': 'Linkerd is Degraded'}}]","{'autoResolved': True, 'timeToResolve': 'PT5M'}","{'title': 'Linkerd Degradation', 'summary': 'Linkerd is Degraded'}","The total rate of requests over the last 10 minutes, grouped by region, is less than 100 per second."
LinkerdRuleGroup,eastus,Alerts on Linkerd,,sum(rate(response_total[10m])) by (region) < 100,"{'severity': '3', 'metric': 'response_total', 'service': 'linkerd', 'pod': 'linkerd'}",ResponseTotal,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Linkerd Degradation has fired on #$.labels.region#', 'IcM.Description': 'Linkerd is Degraded on response requests\n'}}]","{'autoResolved': True, 'timeToResolve': 'PT5M'}","{'title': 'Linkerd Degradation', 'summary': 'Linkerd is Degraded', 'description': 'Linkerd is Degraded on response requests\n'}",The total number of responses per region over the last 10 minutes is less than 100 on average.
LinkerdRuleGroup,eastus,Alerts on Linkerd,,sum(rate(tcp_read_bytes_total[2m])) by (region) < 0,"{'severity': '3', 'metric': 'tcp_read_bytes_total', 'service': 'linkerd', 'pod': 'linkerd'}",tcp_read_bytes_total,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Linkerd is Down has fired on #$.labels.region#', 'IcM.Description': 'Linkerd is Down as tcp_read_bytes_total is empty  \n '}}]","{'autoResolved': True, 'timeToResolve': 'PT5M'}","{'title': 'Linkerd is Down', 'summary': 'Linkerd is Down', 'description': 'Linkerd is Down as tcp_read_bytes_total is empty  \n '}","The total rate of TCP read bytes is decreasing over the last 2 minutes, across all regions, and this decrease is being detected for each region individually."
LinkerdRuleGroup,eastus,Alerts on Linkerd,,sum(rate(tcp_write_bytes_total[2m])) by (region) > 0,"{'severity': '3', 'metric': 'tcp_write_bytes_total', 'service': 'linkerd', 'pod': 'linkerd'}",tcp_write_bytes_total,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Linkerd is Down has fired on #$.labels.region#', 'IcM.Description': 'Linkerd is Down as tcp_write_bytes_total is empty  \n '}}]","{'autoResolved': True, 'timeToResolve': 'PT5M'}","{'title': 'Linkerd is Down', 'summary': 'Linkerd is Down', 'description': 'Linkerd is Down as tcp_write_bytes_total is empty  \n '}",The total rate of TCP write bytes over the last 2 minutes is greater than 0 for each region.
LinkerdRuleGroup,eastus,Alerts on Linkerd,,sum(rate(inbound_http_authz_allow_total[1m])) by (region) < 0,"{'severity': '3', 'metric': 'inbound_http_authz_allow_total', 'service': 'linkerd', 'pod': 'linkerd'}",inbound_http_authz_allow_total,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Linkerd is degraded has fired on #$.labels.region#', 'IcM.Description': 'Linkerd is degraded inbound_http_authz_allow_total is empty \n '}}]","{'autoResolved': True, 'timeToResolve': 'PT5M'}","{'title': 'Linkerd is degraded', 'summary': 'Linkerd is degraded', 'description': 'Linkerd is degraded inbound_http_authz_allow_total is empty \n '}",The total number of allowed inbound HTTP authorization requests per minute is decreasing in at least one region.
LinkerdRuleGroup,eastus,Alerts on Linkerd,,sum(rate(outbound_http_authz_allow_total[1m])) by (region) < 0,"{'severity': '3', 'metric': 'outbound_http_authz_allow_total', 'service': 'linkerd', 'pod': 'linkerd'}",outbound_http_authz_allow_total,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Linkerd is degraded has fired on #$.labels.region#', 'IcM.Description': 'Linkerd is degraded outbound_http_authz_allow_total is empty \n '}}]","{'autoResolved': True, 'timeToResolve': 'PT5M'}","{'title': 'Linkerd is degraded', 'summary': 'Linkerd is degraded', 'description': 'Linkerd is degraded outbound_http_authz_allow_total is empty \n '}",The total number of allowed outbound HTTP authorization requests per minute is decreasing in at least one region.
LinkerdRuleGroup,eastus,Alerts on Linkerd,,sum(rate(outbound_tcp_errors_total[1m])) by (region) < 0,"{'severity': '3', 'metric': 'outbound_tcp_errors_total', 'service': 'linkerd', 'pod': 'linkerd'}",outbound_tcp_errors_total,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert Linkerd is down has fired on #$.labels.region#', 'IcM.Description': 'Linkerd is down outbound_tcp_errors_total is empty \n '}}]","{'autoResolved': True, 'timeToResolve': 'PT5M'}","{'title': 'Linkerd is down', 'summary': 'Linkerd is down', 'description': 'Linkerd is down outbound_tcp_errors_total is empty \n '}","The total rate of outbound TCP errors over the last minute, summed and grouped by region, should never be less than zero, but since the rate is always non-negative, this rule will never trigger as the condition is always false."
GsRuleGroup,eastus,Alerts on Global Scheduler,,sum by (region) (increase(gs_sm_failed_http_requests_total[15m])) > 5 or max(gs_sm_failed_http_requests_total != 0 unless gs_sm_failed_http_requests_total offset 5m),"{'metric': 'gs_sm_failed_http_requests_total', 'service': 'global-scheduler', 'severity': 'sev2', 'pod': 'gs'}",GlobalSchedulerSessionManagerHTTPRequestError,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerSessionManagerHTTPRequestError has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There was an increase of HTTP errors for #$.labels.pod#, HTTP requests for Session manager pod', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There was an increase of HTTP errors for #$.labels.pod#, HTTP requests for Session manager pod', 'summary': 'HTTP requests to Session Manager pod have failed', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","""Alert when the total number of failed HTTP requests in the last 15 minutes, grouped by region, exceeds 5; or if there are any non-zero failed HTTP requests now that weren't present 5 minutes ago."""
GsRuleGroup,eastus,Alerts on Global Scheduler,,sum by (region) (increase(gs_db_failure_counter[10m])) > 0 or max(gs_db_failure_counter != 0 unless gs_db_failure_counter offset 10m),"{'metric': 'gs_db_failure_counter', 'service': 'global-scheduler', 'severity': 'sev2', 'pod': 'gs'}",GlobalSchedulerDatabaseRefreshError,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerDatabaseRefreshError has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There was an increase of Database errors for #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There was an increase of Database errors for #$.labels.pod#', 'summary': 'Database refresh error has occurred in Global Scheduler', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","""Alert when there is an increase in database failures in any region over the last 10 minutes, or if a database failure has occurred and it wasn't already failing 10 minutes ago."""
GsRuleGroup,eastus,Alerts on Global Scheduler,,sum by (region) (increase(gs_monitor_scan_failure_counter[10m])) > 0 or max(gs_monitor_scan_failure_counter != 0 unless gs_monitor_scan_failure_counter offset 10m),"{'metric': 'gs_monitor_scan_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerMonitoringScanFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerMonitoringScanFailure has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There was an increase of monitor scan failures for #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There was an increase of monitor scan failures for #$.labels.pod#', 'summary': 'Monitoring scanning of cluster has failed in Global Scheduler', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","""Alert when there is an increase in scan failures in any region over the last 10 minutes, or if a scan failure occurred and it wasn't already failing 10 minutes ago."""
GsRuleGroup,eastus,Alerts on Global Scheduler,,sum by (region) (increase(gs_on_demand_jobs_failure_counter[10m])) > 0 or max(gs_on_demand_jobs_failure_counter != 0 unless gs_on_demand_jobs_failure_counter offset 10m),"{'metric': 'gs_on_demand_jobs_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerOnDemandJobFailure,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerOnDemandJobFailure has fired on prometheusmdmeastus-stage', 'IcM.Description': 'There was an increase of on demand jobs failure for #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There was an increase of on demand jobs failure for #$.labels.pod#', 'summary': 'On Demand job has failed in Global Scheduler', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","""Alert when there is an increase in on-demand job failures in any region over the last 10 minutes, or if there are currently failed jobs and there weren't any 10 minutes ago."""
GsRuleGroup,eastus,Alerts on Global Scheduler,,sum by (region) (increase(gs_job_missed_schedule_counter[10m])) > 0 or max(gs_job_missed_schedule_counter != 0 unless gs_job_missed_schedule_counter offset 10m),"{'metric': 'gs_job_missed_schedule_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerScheduledJobMissedScheduleFailure,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerScheduledJobMissedScheduleFailure has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There was an increase of jobs missing its schedule for #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There was an increase of jobs missing its schedule for #$.labels.pod#', 'summary': 'Scheduled Job has missed its schedule in Global Scheduler', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","""Alert when there is an increase in missed schedules for any job in a region over the last 10 minutes, or if a job has missed a schedule and wasn't missing one 10 minutes ago."""
GsRuleGroup,eastus,Alerts on Global Scheduler,,sum by (region) (increase(gs_schedule_job_missed_sla_counter[10m])) > 0 or max(gs_schedule_job_missed_sla_counter != 0 unless gs_schedule_job_missed_sla_counter offset 10m),"{'metric': 'gs_schedule_job_missed_sla_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerScheduledJobMissedSLAFailure,False,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerScheduledJobMissedSLAFailure has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There was an increase of jobs missing its SLA/RPO for #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There was an increase of jobs missing its SLA/RPO for #$.labels.pod#', 'summary': 'Scheduled Job has missed its SAL/RPO in Global Scheduler', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","""Alert when there is at least one missed SLA for scheduled jobs in any region within the last 10 minutes, either by a significant increase in missed SLAs or if there are currently missed SLAs that weren't present 10 minutes ago."""
GsRuleGroup,eastus,Alerts on Global Scheduler,,sum by (region) (increase(gs_ontap_connectivity_failure_counter[5m])) > 15 or max(gs_ontap_connectivity_failure_counter != 0 unless gs_ontap_connectivity_failure_counter offset 5m),"{'metric': 'gs_ontap_connectivity_failure_counter', 'service': 'global-scheduler', 'severity': 'sev2', 'pod': 'gs'}",GlobalSchedulerOntapConnectionFailure,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerOntapConnectionFailure has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There was an increase in ontap connection for #$.labels.pod#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There was an increase in ontap connection for #$.labels.pod#', 'summary': 'Ontap connection error occurred in global scheduler', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","""Trigger an alert when either of the following conditions occur in any region: 
1) there are more than 15 increases in connectivity failures within a 5-minute window, 
or 
2) there is at least one connectivity failure and it's not a repeat of a previous failure that occurred exactly 5 minutes ago."""
GsRuleGroup,eastus,Alerts on Global Scheduler,,sum by (region) (increase(gs_on_demand_job_limit_counter[5m])) > 5 or max(gs_on_demand_job_limit_counter != 0 unless gs_on_demand_job_limit_counter offset 5m),"{'metric': 'gs_on_demand_job_limit_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerOnDemandJobLimitAlert,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerOnDemandJobLimitAlert has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There was an increase in on-demand job limit for #$.labels.pod# for replication-api pod', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There was an increase in on-demand job limit for #$.labels.pod# for replication-api pod', 'summary': 'Alert occurred while there is increase in on-demand job by a customer in global scheduler', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","""Trigger an alert when, in any 5-minute period, either the total number of times the on-demand job limit was reached across all regions is more than 5, or there are regions where the on-demand job limit was reached and it wasn't already reached 5 minutes ago."""
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_http_requests_total{job=""gs-replication-api"",code!~""2..|4..""}[10m])) > 5","{'metric': 'gs_http_requests_total', 'service': 'global-scheduler', 'severity': 'sev2', 'pod': 'gs'}",GSReplicationAPIHttp500ErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GSReplicationAPIHttp500ErrorCritical has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for in gs-replication-api in the last 30 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for in gs-replication-api in the last 30 minutes, please investigate', 'summary': 'High amount of GS Replication API HTTP 500 Failures', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","Trigger an alert when the total number of non-successful HTTP requests (status codes other than 200 or 400) to the gs-replication-api job increases by more than 5 within a 10-minute window, grouped by region."
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_http_requests_total{job=""gs-backup-api"",code!~""2..|4..""}[10m])) > 5","{'metric': 'gs_http_requests_total', 'service': 'global-scheduler', 'severity': 'sev2', 'pod': 'gs'}",GSBackupAPIHttp500ErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GSBackupAPIHttp500ErrorCritical has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for in gs-backup-api in the last 30 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for in gs-backup-api in the last 30 minutes, please investigate', 'summary': 'High amount of GS Backup API HTTP 500 Failures', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","Trigger an alert when, in any 10-minute period, the total number of HTTP requests to the ""gs-backup-api"" job that return non-200 or non-400 error codes increases by more than 5 requests across all regions."
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_http_requests_total{job=""gs-config"",code!~""2..|4..""}[10m])) > 5","{'metric': 'gs_http_requests_total', 'service': 'global-scheduler', 'severity': 'sev2', 'pod': 'gs'}",GSConfigHttp500ErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GSConfigHttp500ErrorCritical has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for in gs-config in the last 30 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for in gs-config in the last 30 minutes, please investigate', 'summary': 'High amount of GS Config HTTP 500 Failures', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","Trigger an alert when the total number of non-200 and non-400 HTTP request errors from the ""gs-config"" job, grouped by region, increases by more than 5 within a 10-minute window."
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region,hostUUID) (increase(gs_smc_token_rotation_failure_counter[24h:1h])) > 0","{'metric': 'gs_smc_token_rotation_failure_counter', 'service': 'global-scheduler', 'severity': 'sev2', 'pod': 'gs'}",GlobalSchedulerSmcTokenRotationFailure,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerSmcTokenRotationFailure has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There was #$.labels.value# smc token rotation failure increasing in last 24 hours for hostUUID: #$.labels.hostUUID#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There was #$.labels.value# smc token rotation failure increasing in last 24 hours for hostUUID: #$.labels.hostUUID#', 'summary': 'Smc token rotation failure occurred in global scheduler', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}",There has been at least one failure in rotating SMC tokens in the last 24 hours for each region and host.
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_scheduled_jobapi_failure_counter{code=~""5..""}[10m])) > 0","{'metric': 'gs_scheduled_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerScheduledJobAPIHttp500Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerScheduledJobAPIHttp500Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for scheduled job in the last 10 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for scheduled job in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 500 Failures for scheduled job', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","Trigger an alert when there is at least one increase in the number of 'gs_scheduled_jobapi_failure_counter' errors with a status code between 500 and 599 within any 10-minute window, grouped by region."
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_scheduled_jobapi_failure_counter{code=~""4..""}[10m])) > 0","{'metric': 'gs_scheduled_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerScheduledJobAPIHttp400Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerScheduledJobAPIHttp400Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for scheduled job in the last 10 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for scheduled job in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 400 Failures for scheduled job', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}",Trigger an alert when there is at least one increase in the number of failures with a 4xx status code for scheduled job API requests in any region over the last 10 minutes.
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_adhoc_jobapi_failure_counter{code=~""5..""}[10m])) > 0","{'metric': 'gs_adhoc_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerAdhocJobAPIHttp500Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerAdhocJobAPIHttp500Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for adhoc job in the last 10 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for adhoc job in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 500 Failures for adhoc job', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","Trigger an alert when there is at least one increase in the number of failures with a 5xx status code from the gs_adhoc_jobapi_failure_counter metric within a 10-minute window, grouped by region."
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_adhoc_jobapi_failure_counter{code=~""4..""}[10m])) > 0","{'metric': 'gs_adhoc_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerAdhocJobAPIHttp400Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerAdhocJobAPIHttp400Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for adhoc job in the last 10 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for adhoc job in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 400 Failures for adhoc job', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","Trigger an alert when there is at least one increase in the number of failures with a 4xx status code from the gs_adhoc_jobapi_failure_counter metric within a 10-minute window, grouped by region."
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_backup_rotation_jobapi_failure_counter{code=~""5..""}[10m])) > 0","{'metric': 'gs_backup_rotation_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerBackupRotationJobAPIHttp500Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerBackupRotationJobAPIHttp500Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for backup rotation in the last 10 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for backup rotation in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 500 Failures for backup rotation', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","Trigger an alert when there is at least one increase in the number of 'gs_backup_rotation_jobapi_failure_counter' errors with a code between 500 and 599 within any 10-minute window, grouped by region."
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_backup_rotation_jobapi_failure_counter{code=~""4..""}[10m])) > 0","{'metric': 'gs_backup_rotation_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerBackupRotationJobAPIHttp400Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerBackupRotationJobAPIHttp400Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for backup rotation in the last 10 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for backup rotation in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 400 Failures for backup rotation job', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}","There has been at least one increase in the number of times the backup rotation job API failed with a 4xx error code in any region over the last 10 minutes, when grouped by region and summed together."
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_smc_token_rotation_jobapi_failure_counter{code=~""5..""}[6h])) > 0","{'metric': 'gs_smc_token_rotation_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerSMCTokenRotationJobAPIHttp500Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerSMCTokenRotationJobAPIHttp500Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for SMC token rotation in the last 10 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for GS internal api for SMC token rotation in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 500 Failures for SMC token rotation', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}",There has been at least one failure with a 500-level error code in the token rotation job API for each region over the last 6 hours.
GsRuleGroup,eastus,Alerts on Global Scheduler,,"sum by (region) (increase(gs_smc_token_rotation_jobapi_failure_counter{code=~""4..""}[6h])) > 0","{'metric': 'gs_smc_token_rotation_jobapi_failure_counter', 'service': 'global-scheduler', 'severity': 'critical', 'pod': 'gs'}",GlobalSchedulerSMCTokenRotationJobAPIHttp400Error,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert GlobalSchedulerSMCTokenRotationJobAPIHttp400Error has fired on #$.labels.region# | pod #$.labels.pod#', 'IcM.Description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for SMC token rotation in the last 10 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/global-scheduler-alerts'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 400 errors for GS internal api for SMC token rotation in the last 10 minutes, please investigate', 'summary': 'High amount of GS internal api HTTP 400 Failures for SMC token rotation', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/p8qMIBX4z/global-scheduler'}",There has been at least one API failure with a 4xx error code in the token rotation job for any region within the last 6 hours.
RedisRuleGroup,eastus,Alerts for Redis,,"sum by (region) (count(redis_instance_info{role=""master""}) or vector(0)) < 1","{'metric': 'redis_instance_info', 'service': 'redis', 'severity': 'medium', 'pod': 'redis'}",RedisMissingMaster,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RedisMissingMaster has fired on #$.labels.region#', 'IcM.Description': 'Redis cluster has no node marked as master.'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Redis cluster has no node marked as master.', 'summary': 'Redis missing master (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}",There is at least one region where there are no Redis master instances.
RedisRuleGroup,eastus,Alerts for Redis,,sum by (region) (redis_up) == 0,"{'metric': 'redis_up', 'service': 'redis', 'severity': 'high', 'pod': 'redis'}",RedisDown,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RedisDown has fired on #$.labels.region#', 'IcM.Description': 'Redis instance is down', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/redis-down'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Redis instance is down', 'summary': 'Redis down (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}",There is at least one region where no Redis instances are up.
RedisRuleGroup,eastus,Alerts for Redis,,sum by (region) (time() - redis_rdb_last_save_timestamp_seconds) > 60 * 60 * 24,"{'metric': 'redis_rdb_last_save_timestamp_seconds', 'service': 'redis', 'severity': 'medium', 'pod': 'redis'}",RedisMissingBackup,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RedisMissingBackup has fired on #$.labels.region#', 'IcM.Description': 'Redis has not been backuped for 24 hours'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Redis has not been backuped for 24 hours', 'summary': 'Redis missing backup (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}","Send an alert when the Redis database has not been saved in the last 24 hours, grouped by region."
RedisRuleGroup,eastus,Alerts for Redis,,sum by (region) (redis_memory_used_bytes / redis_memory_max_bytes * 100) > 90,"{'metric': 'redis_memory_max_bytes', 'service': 'redis', 'severity': 'medium', 'pod': 'redis'}",RedisOutOfSystemMemory,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RedisOutOfSystemMemory has fired on #$.labels.region#', 'IcM.Description': 'Redis is running out of system memory (> 90%)'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Redis is running out of system memory (> 90%)', 'summary': 'Redis out of system memory (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}",The average percentage of used Redis memory across all instances in each region is greater than 90%.
RedisRuleGroup,eastus,Alerts for Redis,,sum by (region) (redis_memory_used_bytes / redis_memory_max_bytes * 100) > 90,"{'metric': 'redis_memory_max_bytes', 'service': 'redis', 'severity': 'high', 'pod': 'redis'}",RedisOutOfConfiguredMaxmemory,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RedisOutOfConfiguredMaxmemory has fired on #$.labels.region#', 'IcM.Description': 'Redis is running out of configured maxmemory (> 90%)', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/redis-out-of-configured-max-memory'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Redis is running out of configured maxmemory (> 90%)', 'summary': 'Redis out of configured maxmemory (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}",The average percentage of used Redis memory across all instances in each region is greater than 90%.
RedisRuleGroup,eastus,Alerts for Redis,,sum by (region) (redis_connected_clients > 10000) or (redis_connection_errors > 100),"{'metric': 'redis_connected_clients', 'service': 'redis', 'severity': 'medium', 'pod': 'redis'}",RedisTooManyConnections,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RedisTooManyConnections has fired on #$.labels.region#', 'IcM.Description': 'Redis instance has too many connections', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/redis-out-of-configured-max-memory'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Redis instance has too many connections', 'summary': 'Redis too many connections (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}","Trigger an alert when, in any region, either the number of connected clients to Redis exceeds 10,000 or the number of Redis connection errors exceeds 100."
RedisRuleGroup,eastus,Alerts for Redis,,sum by (region) (redis_connected_clients <= 0),"{'metric': 'redis_connected_clients', 'service': 'redis', 'severity': 'high', 'pod': 'redis'}",RedisNotEnoughConnections,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RedisNotEnoughConnections has fired on #$.labels.region#', 'IcM.Description': 'Redis instance should have more connections (> 5)', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/redis-not-enough-connections'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Redis instance should have more connections (> 5)', 'summary': 'Redis not enough connections (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}",Show the total number of regions where there are no connected Redis clients.
RedisRuleGroup,eastus,Alerts for Redis,,sum by (region) (increase(redis_rejected_connections_total[1m])) > 0,"{'metric': 'redis_rejected_connections_total', 'service': 'redis', 'severity': 'medium', 'pod': 'redis'}",RedisRejectedConnections,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RedisRejectedConnections has fired on #$.labels.region#', 'IcM.Description': 'Some connections to Redis has been rejected'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Some connections to Redis has been rejected', 'summary': 'Redis rejected connections (instance #$.labels.instance#)', 'dashboard': 'to-be-added'}","There is an increase in the total number of rejected Redis connections over the last minute, grouped by region, with at least one rejection occurring."
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(hyperscaler_cluster_drp_route_count_usage / hyperscaler_cluster_drp_route_count_volume_allocation_limit) > 0.8,"{'metric': 'hyperscaler_cluster_drp_route_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",DrpRouteCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert DrpRouteCount has fired on #$.labels.region#', 'IcM.Description': 'Drp route count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Drp route count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Drp route count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The proportion of used route counts in a hyperscaler cluster's Disaster Recovery Plan is more than 80% of its allocated limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(hyperscaler_cluster_drp_vrf_count_usage / hyperscaler_cluster_drp_vrf_count_volume_allocation_limit) > 0.8,"{'metric': 'hyperscaler_cluster_drp_vrf_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",DrpVrfCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert DrpVrfCount has fired on #$.labels.region#', 'IcM.Description': 'Drp vrf count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Drp vrf count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Drp vrf count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The usage of VRFs in a hyperscaler cluster is more than 80% of its allocated volume limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(hyperscaler_cluster_cvn_tenancy_count_usage / hyperscaler_cluster_cvn_tenancy_count_volume_allocation_limit) > 0.8,"{'metric': 'hyperscaler_cluster_cvn_tenancy_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",CvnTenancyCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvnTenancyCount has fired on #$.labels.region#', 'IcM.Description': 'Cvn tenancy count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Cvn tenancy count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Cvn tenancy count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The usage of CVN tenancies in a hyperscaler cluster is more than 80% of the allocated volume limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(ontap_cluster_svm_count_usage / ontap_cluster_svm_count_volume_allocation_limit) > 0.8,"{'metric': 'ontap_cluster_svm_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",OntapSvmCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert OntapSvmCount has fired on #$.labels.region#', 'IcM.Description': 'Ontap cluster svm count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Ontap cluster svm count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Ontap cluster svm count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The storage usage of SVMs in the cluster is more than 80% of their allocated volume limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(ontap_cluster_flexgroup_volume_count_usage / ontap_cluster_flexgroup_volume_count_volume_allocation_limit) > 0.8,"{'metric': 'ontap_cluster_flexgroup_volume_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",OntapFlexgroupVolumeCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert OntapFlexgroupVolumeCount has fired on #$.labels.region#', 'IcM.Description': 'Ontap cluster flexgroup volume count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Ontap cluster flexgroup volume count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Ontap cluster flexgroup volume count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The usage of flexgroup volumes in the cluster is more than 80% of their allocated limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(ontap_cluster_vlan_count_usage / ontap_cluster_vlan_count_volume_allocation_limit) > 0.8,"{'metric': 'ontap_cluster_vlan_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",OntapVlanCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert OntapVlanCount has fired on #$.labels.region#', 'IcM.Description': 'Ontap cluster vlan count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Ontap cluster vlan count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Ontap cluster vlan count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The VLAN usage in the ONTAP cluster is more than 80% of its allocated volume limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(ontap_cluster_ipspace_count_usage / ontap_cluster_ipspace_count_volume_allocation_limit) > 0.8,"{'metric': 'ontap_cluster_ipspace_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",OntapIpspaceCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert OntapIpspaceCount has fired on #$.labels.region#', 'IcM.Description': 'Ontap cluster ipspace count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Ontap cluster ipspace count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Ontap cluster ipspace count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The storage usage of an Ontap cluster's IP space is more than 80% of its allocated volume limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(aggregate_logical_size_usage / aggregate_logical_size_volume_allocation_limit) > 0.8,"{'metric': 'aggregate_logical_size_usage', 'service': 'vps-usage', 'pod': 'vps'}",AggregateLogicalSize,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert AggregateLogicalSize has fired on #$.labels.region#', 'IcM.Description': 'Aggregate logical size usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Aggregate logical size usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Aggregate logical size usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The volume is more than 80% full based on its logical size usage compared to its allocated limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(aggregate_physical_size_usage / aggregate_physical_size_volume_allocation_limit) > 0.8,"{'metric': 'aggregate_physical_size_usage', 'service': 'vps-usage', 'pod': 'vps'}",AggregatePhysicalSize,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert AggregatePhysicalSize has fired on #$.labels.region#', 'IcM.Description': 'Aggregate physical size usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Aggregate physical size usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Aggregate physical size usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The used physical size is more than 80% of the total allocated volume size.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(node_iops_throughput_usage / node_iops_throughput_volume_allocation_limit) > 0.8,"{'metric': 'node_iops_throughput_usage', 'service': 'vps-usage', 'pod': 'vps'}",NodeIopsThroughput,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NodeIopsThroughput has fired on #$.labels.region#', 'IcM.Description': 'Node iops throughput usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Node iops throughput usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Node iops throughput usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The disk usage for IOPS (input/output operations per second) is more than 80% of its allocated limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(node_mibps_throughput_usage / node_mibps_throughput_volume_allocation_limit) > 0.8,"{'metric': 'node_mibps_throughput_usage', 'service': 'vps-usage', 'pod': 'vps'}",NodeMibpsThroughput,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NodeMibpsThroughput has fired on #$.labels.region#', 'IcM.Description': 'Node mibps throughput usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Node mibps throughput usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Node mibps throughput usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The usage of throughput on a node is more than 80% of its allocated limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(node_data_lif_count_usage / node_data_lif_count_volume_allocation_limit) > 0.8,"{'metric': 'node_data_lif_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",NodeDataLifCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NodeDataLifCount has fired on #$.labels.region#', 'IcM.Description': 'Node data lif count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Node data lif count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Node data lif count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The data LIF usage on a node is more than 80% of its allocated volume limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(node_dp_volume_count_usage / node_dp_volume_count_volume_allocation_limit) > 0.8,"{'metric': 'node_dp_volume_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",NodeDpVolumeCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NodeDpVolumeCount has fired on #$.labels.region#', 'IcM.Description': 'Node dp volume count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Node dp volume count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Node dp volume count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The disk usage of a volume is more than 80% of its allocated limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(node_volume_count_usage / node_volume_count_volume_allocation_limit) > 0.8,"{'metric': 'node_volume_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",NodeVolumeCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NodeVolumeCount has fired on #$.labels.region#', 'IcM.Description': 'Node volume count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Node volume count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Node volume count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The disk usage of a node is more than 80% of its total allocated volume space.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(node_volume_tag_count_SAPHANA_log_usage / node_volume_tag_count_SAPHANA_log_volume_allocation_limit) > 0.8,"{'metric': 'node_volume_tag_count_SAPHANA_log_usage', 'service': 'vps-usage', 'pod': 'vps'}",NodeVolumeCountSAPHANALog,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NodeVolumeCountSAPHANALog has fired on #$.labels.region#', 'IcM.Description': 'Node volume count usage for SAPHANA Log volumes exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Node volume count usage for SAPHANA Log volumes exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Node volume count usage for SAPHANA Log volumes exceeded', 'dashboard': 'vps-usage-dashboard'}",The log usage of SAPHANA volumes is more than 80% of their allocated limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(node_volume_tag_count_SAPHANA_data_usage / node_volume_tag_count_SAPHANA_data_volume_allocation_limit) > 0.8,"{'metric': 'node_volume_tag_count_SAPHANA_data_usage', 'service': 'vps-usage', 'pod': 'vps'}",NodeVolumeCountSAPHANAData,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NodeVolumeCountSAPHANAData has fired on #$.labels.region#', 'IcM.Description': 'Node volume count usage for SAPHANA Data volumes exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Node volume count usage for SAPHANA Data volumes exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Node volume count usage for SAPHANA Data volumes exceeded', 'dashboard': 'vps-usage-dashboard'}",The used storage space of the SAPHANA data volume exceeds 80% of its allocated limit.
VpsUsageRuleGroup,eastus,Alerts on VPS Usage,,(node_ca_share_lock_count_usage / node_ca_share_lock_count_volume_allocation_limit) > 0.8,"{'metric': 'node_ca_share_lock_count_usage', 'service': 'vps-usage', 'pod': 'vps'}",NodeCAShareLockCount,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert NodeCAShareLockCount has fired on #$.labels.region#', 'IcM.Description': 'Node CA share lock count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Node CA share lock count usage exceeded 80 percent of the limit configured for #$.labels.name# with uuid: #$.labels.uuid#', 'summary': 'Node CA share lock count usage exceeded', 'dashboard': 'vps-usage-dashboard'}",The usage of shared locks on a node is more than 80% of its allocated limit.
VpRuleGroup,eastus,Alerts on VVolume Placementp,,"sum by (region) (increase(http_requests_total{job=""vps"",code!~""2..|4..""}[5m])) > 1","{'metric': 'http_requests_total', 'service': 'vps', 'pod': 'vps'}",VPSHttp500ErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VPSHttp500ErrorCritical has fired on #$.labels.region#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for in VPS in the last 5 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for in VPS in the last 5 minutes, please investigate', 'summary': 'High amount of VPS HTTP 500 Failures.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}",Trigger an alert when the total number of non-200 and non-400 HTTP request errors from VPS servers in each region increases by more than 1 within a 5-minute window.
VpRuleGroup,eastus,Alerts on VVolume Placementp,,"sum by (region) (increase(http_requests_total{job=""rt-api-server"",code!~""2..|4..""}[5m])) > 1","{'metric': 'http_requests_total', 'service': 'cloud-volumes-service', 'pod': 'vps'}",RtApiServerHttp500ErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RtApiServerHttp500ErrorCritical has fired on #$.labels.region#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for in RT Api Server in the last 5 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for in RT Api Server in the last 5 minutes, please investigate', 'summary': 'High amount of RT Api Server HTTP 500 Failures.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","Trigger an alert when the total number of non-200 and non-400 HTTP requests from the ""rt-api-server"" job increases by more than 1 request within a 5-minute window, grouped by region."
VpRuleGroup,eastus,Alerts on VVolume Placementp,,"sum by (region) (increase(http_requests_total{job=""rt-supervisor"",code!~""2..|4..""}[5m])) > 1","{'metric': 'http_requests_total', 'service': 'cloud-volumes-service', 'pod': 'vps'}",RtSupervisorHttp500ErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RtSupervisorHttp500ErrorCritical has fired on #$.labels.region#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for in RT Supervisor in the last 5 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for in RT Supervisor in the last 5 minutes, please investigate', 'summary': 'High amount of RT Supervisor HTTP 500 Failures.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","Trigger an alert when the total number of non-200 and non-400 HTTP request errors from the ""rt-supervisor"" job increases by more than 1 in any 5-minute window, grouped by region."
VpRuleGroup,eastus,Alerts on VVolume Placementp,,"sum by (region) (increase(http_requests_total{job=""rt-worker"",code!~""2..|4..""}[5m])) > 1","{'metric': 'http_requests_total', 'service': 'cloud-volumes-service', 'pod': 'vps'}",RtWorkerHttp500ErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert RtWorkerHttp500ErrorCritical has fired on #$.labels.region#', 'IcM.Description': 'There were #$.labels.value# HTTP 500 errors for in RT Worker in the last 5 minutes, please investigate', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'There were #$.labels.value# HTTP 500 errors for in RT Worker in the last 5 minutes, please investigate', 'summary': 'High amount of RT Worker HTTP 500 Failures.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","Trigger an alert when, in any 5-minute period, the total number of non-successful HTTP requests (those with status codes not starting with 2 or 4) from the ""rt-worker"" job increases by more than 1 request in any region."
VpRuleGroup,eastus,Alerts on VVolume Placementp,,sum by (region) (increase(mongodb_failure[10m])) > 0 or max(mongodb_failure != 0 unless mongodb_failure offset 10m),"{'metric': 'mongodb_failure', 'service': 'vps', 'pod': 'vps'}",MongoDBFailure,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert MongoDBFailure has fired on #$.labels.region#', 'IcM.Description': 'MongoDB failure occurred #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'MongoDB failure occurred #$.labels.value# times', 'summary': 'MongoDB failure occurred', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""Trigger an alert if, in any 10-minute period, there is an increase in MongoDB failures in a region, or if there are any current MongoDB failures that didn't exist 10 minutes ago."""
VpRuleGroup,eastus,Alerts on VVolume Placementp,,sum by (region) (increase(cvi_failure[10m])) > 0 or max(cvi_failure != 0 unless cvi_failure offset 10m),"{'metric': 'cvi_failure', 'service': 'vps', 'pod': 'vps'}",CVIFailure,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVIFailure has fired on #$.labels.region#', 'IcM.Description': 'CVI failure occurred #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'CVI failure occurred #$.labels.value# times', 'summary': 'CVI failure occurred', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""Trigger an alert if, in any 10-minute period, there is an increase in CVI failures in at least one region, or if there are any current CVI failures that didn't exist 10 minutes ago."""
VpRuleGroup,eastus,Alerts on VVolume Placementp,,sum by (region) (increase(drp_failure[10m])) > 2 or max(drp_failure != 0 unless drp_failure offset 10m),"{'metric': 'drp_failure', 'service': 'vps', 'pod': 'vps'}",DRPFailure,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert DRPFailure has fired on #$.labels.region#', 'IcM.Description': 'DRP failure occurred #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'DRP failure occurred #$.labels.value# times', 'summary': 'DRP failure occurred', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""Trigger an alert when, in any region, there are more than 2 increases in DRP failures over the last 10 minutes, or if there is at least one DRP failure that wasn't present 10 minutes ago."""
VpRuleGroup,eastus,Alerts on VVolume Placementp,,sum by (region) (increase(default_limits_missing[10m])) > 0 or max(default_limits_missing != 0 unless default_limits_missing offset 10m),"{'metric': 'default_limits_missing', 'service': 'vps', 'pod': 'vps'}",DefaultLimitsMissing,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert DefaultLimitsMissing has fired on #$.labels.region#', 'IcM.Description': 'Missing default limit configuration encountered #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Missing default limit configuration encountered #$.labels.value# times', 'summary': 'Default limit configuration for one or more ONTAP models/versions is missing', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""Alert when there is an increase in missing default limits in any region over the last 10 minutes, or if there are currently missing default limits and there weren't any 10 minutes ago."""
VpRuleGroup,eastus,Alerts on VVolume Placementp,,sum by (region) (increase(fallback_limit_use[10m])) > 0 or max(fallback_limit_use != 0 unless fallback_limit_use offset 10m),"{'metric': 'fallback_limit_use', 'service': 'vps', 'pod': 'vps'}",FallbackLimitUse,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert FallbackLimitUse has fired on #$.labels.region#', 'IcM.Description': 'Fallback limits were used #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'Fallback limits were used #$.labels.value# times', 'summary': 'On ore more default limit configurations have missing limit values', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""Trigger an alert when, for any region, there has been an increase in the use of the fallback limit within the last 10 minutes, or if the fallback limit is currently being used and it wasn't being used 10 minutes ago."""
VpRuleGroup,eastus,Alerts on VVolume Placementp,,sum by (region) (increase(msg_drop_total[10m])) > 0 or max(msg_drop_total != 0 unless msg_drop_total offset 10m),"{'metric': 'msg_drop_total', 'service': 'vps', 'pod': 'vps'}",MQMessageDropped,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert MQMessageDropped has fired on #$.labels.region#', 'IcM.Description': 'mq message dropped occurred #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'mq message dropped occurred #$.labels.value# times', 'summary': 'mq message dropped occurred, can be because of validate, parse and timeout', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""Alert when, in any 10-minute period, there is an increase in the total number of dropped messages in a region, or if there are currently dropped messages and there weren't any 10 minutes ago."""
VpRuleGroup,eastus,Alerts on VVolume Placementp,,sum by (region) (rate(malformed_message[10m])) > 0,"{'metric': 'malformed_message', 'service': 'vps', 'pagerduty': 'true', 'pod': 'vps'}",VpsMalformedMessage,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VpsMalformedMessage has fired on #$.labels.region#', 'IcM.Description': 'The VPS service rt-worker pod has received a poorly formed refresh task message', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The VPS service rt-worker pod has received a poorly formed refresh task message', 'summary': 'This VPS service rt-worker pod has received a poorly formed refresh task message. This is an internal coding error.', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","The rate of malformed messages over the last 10 minutes is greater than 0 in at least one region, when calculated as a total for each region."
VpRuleGroup,eastus,Alerts on VVolume Placementp,,sum by (region) (increase(ontap_failure[10m])) > 0 or max(ontap_failure != 0 unless ontap_failure offset 10m),"{'metric': 'ontap_failure', 'service': 'vps', 'pod': 'vps'}",OntapFailure,False,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert OntapFailure has fired on #$.labels.region#', 'IcM.Description': 'ontap failure occurred #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'ontap failure occurred #$.labels.value# times', 'summary': 'ontap failure occurred', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""Trigger an alert if, in any 10-minute period, there is at least one increase in ONTAP failures in each region, or if there are currently ONTAP failures and there weren't any 10 minutes ago."""
VpRuleGroup,eastus,Alerts on VVolume Placementp,,sum by (region) (rate(periodic_job_failure[15m])) > 0,"{'metric': 'periodic_job_failure', 'service': 'vps', 'pod': 'vps'}",PeriodicJobFailure,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert PeriodicJobFailure has fired on #$.labels.region#', 'IcM.Description': 'An rt-worker periodic job failure occurred', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'An rt-worker periodic job failure occurred', 'summary': 'An rt-worker periodic job failure occurred', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","The rate of periodic job failures over the last 15 minutes is greater than zero in at least one region, indicating that there have been recent failures in some areas."
VpRuleGroup,eastus,Alerts on VVolume Placementp,,"(((sum by (region) (increase(vps_allocate_volume_failure{context=""algorithm-phase""}[5m])) > 0) or (max(vps_allocate_volume_failure{context=""algorithm-phase""} != 0 unless vps_allocate_volume_failure{context=""algorithm-phase""} offset 5m))) / (((sum by (region) (increase(vps_allocate_volume_failure{context=""algorithm-phase""}[5m])) > 0) or (max(vps_allocate_volume_failure{context=""algorithm-phase""} != 0 unless vps_allocate_volume_failure{context=""algorithm-phase""} offset 5m))) + ((sum by (region) (increase(vps_allocate_volume_success{context=""algorithm-phase""}[5m])) > 0) or (max(vps_allocate_volume_success{context=""algorithm-phase""} != 0 unless vps_allocate_volume_success{context=""algorithm-phase""} offset 5m)))) * 100 > 30)","{'metric': 'vps_allocate_volume_failure', 'service': 'vps', 'pod': 'vps'}",VpsAllocateVolumeFailureAlgorithmPhase,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VpsAllocateVolumeFailureAlgorithmPhase has fired on #$.labels.region#', 'IcM.Description': 'VPS allocate volume failure algorithm phase occurred #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'VPS allocate volume failure algorithm phase occurred #$.labels.value# times', 'summary': 'VPS allocate volume failure algorithm phase occurred', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","The percentage of failed volume allocations in the ""algorithm-phase"" context is higher than 30% over the last 5 minutes, where a failure is considered when there's at least one increase in allocation failures or if there are current failures and none were present 5 minutes ago, compared to the total number of both successful and failed allocations."
VpRuleGroup,eastus,Alerts on VVolume Placementp,,"((absent(vps_allocate_volume_success{context=""algorithm-phase""}) == 1) * ((sum by (region) (increase(vps_allocate_volume_failure{context=""algorithm-phase""}[5m])) > 0) or (max(vps_allocate_volume_failure{context=""algorithm-phase""} != 0 unless vps_allocate_volume_failure{context=""algorithm-phase""} offset 5m))) / ((sum by (region) (increase(vps_allocate_volume_failure{context=""algorithm-phase""}[5m])) > 0) or (max(vps_allocate_volume_failure{context=""algorithm-phase""} != 0 unless vps_allocate_volume_failure{context=""algorithm-phase""} offset 5m))) * 100)","{'metric': 'vps_allocate_volume_failure', 'service': 'vps', 'pod': 'vps'}",VpsAllocateVolumeFailureAlgorithmPhase2,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VpsAllocateVolumeFailureAlgorithmPhase2 has fired on prometheusmdmeastus-stage', 'IcM.Description': 'VPS allocate volume failure algorithm phase occurred #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'VPS allocate volume failure algorithm phase occurred #$.labels.value# times', 'summary': 'VPS allocate volume failure algorithm phase occurred', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""When there are no successful volume allocations in the 'algorithm-phase' context, and either there has been at least one failed allocation in each region within the last 5 minutes or there is currently a failure that wasn't present 5 minutes ago, then calculate the percentage of regions with failures, otherwise return NaN."""
VpRuleGroup,eastus,Alerts on VVolume Placementp,,"((sum by (region) (increase(vps_allocate_volume_success{context=""algorithm-phase""}[5m])) == 0) + ((sum by (region) (increase(vps_allocate_volume_failure{context=""algorithm-phase""}[5m])) > 0) or (max(vps_allocate_volume_failure{context=""algorithm-phase""} != 0 unless vps_allocate_volume_failure{context=""algorithm-phase""} offset 5m))) / ((sum by (region) (increase(vps_allocate_volume_failure{context=""algorithm-phase""}[5m])) > 0) or (max(vps_allocate_volume_failure{context=""algorithm-phase""} != 0 unless vps_allocate_volume_failure{context=""algorithm-phase""} offset 5m))) * 100)","{'metric': 'vps_allocate_volume_success', 'service': 'vps', 'pod': 'vps'}",VpsAllocateVolumeFailureAlgorithmPhase3,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VpsAllocateVolumeFailureAlgorithmPhase3 has fired on #$.labels.region#', 'IcM.Description': 'VPS allocate volume failure algorithm phase occurred #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'VPS allocate volume failure algorithm phase occurred #$.labels.value# times', 'summary': 'VPS allocate volume failure algorithm phase occurred', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""The percentage of regions where no successful volume allocations occurred in the last 5 minutes, and either a failure occurred in the last 5 minutes or there was a previous failure that hasn't been resolved yet, compared to all regions with failures in the last 5 minutes or unresolved previous failures."""
VpRuleGroup,eastus,Alerts on VVolume Placementp,,sum by (region) (increase(vps_reallocate_volume_failure[5m])) > 0 or max(vps_reallocate_volume_failure != 0 unless vps_reallocate_volume_failure offset 5m),"{'metric': 'vps_reallocate_volume_failure', 'service': 'vps', 'pod': 'vps'}",VpsReallocateVolumeFailure,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VpsReallocateVolumeFailure has fired on #$.labels.region#', 'IcM.Description': 'VPS reallocate volume failure occurred #$.labels.value# times', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'VPS reallocate volume failure occurred #$.labels.value# times', 'summary': 'VPS reallocate volume failure occurred', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}","""Trigger an alert if, in any region, there has been at least one failure to reallocate a volume for a virtual private server within the last 5 minutes, or if there is currently a failure to reallocate a volume that wasn't present 5 minutes ago."""
VpRuleGroup,eastus,Alerts on VVolume Placementp,,avg_over_time(mq_depth[10m]) > 6,"{'metric': 'mq_depth', 'service': 'vps', 'pod': 'vps'}",VpsMQDepth,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VpsMQDepth has fired on #$.labels.region#', 'IcM.Description': 'The VPS rt-worker message queue has a high average depth #$.labels.value#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The VPS rt-worker message queue has a high average depth #$.labels.value#', 'summary': 'VPS rt-worker message queue has a high average depth', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}",The average message queue depth over the last 10 minutes is greater than 6.
VpRuleGroup,eastus,Alerts on VVolume Placementp,,ontap_cluster_stale_document > 0,"{'metric': 'ontap_cluster_stale_document', 'service': 'vps', 'pod': 'vps'}",VpsStaleOntapDocuments,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert VpsStaleOntapDocuments has fired on #$.labels.region#', 'IcM.Description': 'The VPS service has stale ONTAP resource usage information', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/vps-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'description': 'The VPS service has stale ONTAP resource usage information', 'summary': 'VPS ONTAP resource usage information is stale', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/c26ce363-a99f-4ece-96a6-440876b83d9f/volume-placement-service-vps'}",There is at least one stale document in an ONTAP cluster.
PgbouncerRuleGroup,eastus,Alerts on Pgbouncer,,sum by (region) (pgbouncer_up) == 0,"{'metric': 'pgbouncer_up', 'service': 'pgbouncer', 'pod': 'pgbouncer'}",PgbouncerDown,False,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert PgbouncerDown has fired on #$.labels.region#', 'IcM.Description': 'Pgbouncer is not responding. Investigate and resolve the issue', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT5M'}","{'title': 'PgbouncerDown', 'description': 'Pgbouncer is not responding. Investigate and resolve the issue', 'summary': 'Pgbouncer service is down', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/b65a429f-88e5-4da8-8f8a-98783f24c436/pgbouncer'}",There are no PGBouncer instances running in any region.
PgbouncerRuleGroup,eastus,Alerts on Pgbouncer,,sum by (region) (pgbouncer_pools_client_waiting_connections) > 10,"{'metric': 'pgbouncer_pools_client_waiting_connections', 'service': 'pgbouncer', 'pod': 'pgbouncer'}",PgbouncerClientWaitingConnections,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert PgbouncerClientWaitingConnections has fired on #$.labels.region#', 'IcM.Description': 'The number of client waiting connections in pgbouncer has exceeded 10. Investigate and take action', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'PgbouncerClientWaitingConnections', 'description': 'The number of client waiting connections in pgbouncer has exceeded 10. Investigate and take action', 'summary': 'High client waiting connections in Pgbouncer', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/b65a429f-88e5-4da8-8f8a-98783f24c436/pgbouncer'}",The total number of client connections waiting in PGBouncer pools is greater than 10 in at least one region.
PgbouncerRuleGroup,eastus,Alerts on Pgbouncer,,sum by (region) (pgbouncer_pools_server_login_connections) > 500,"{'metric': 'pgbouncer_pools_server_login_connections', 'service': 'pgbouncer', 'pod': 'pgbouncer'}",PgbouncerPoolsServerLoginConnections,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert PgbouncerPoolsServerLoginConnections has fired on #$.labels.region#', 'IcM.Description': 'The number of server connections currently in the process of logging in is more than 500. Investigate and take action', 'IcM.TsgId': 'coming-soon'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'PgbouncerPoolsServerLoginConnections', 'description': 'The number of server connections currently in the process of logging in is more than 500. Investigate and take action', 'summary': 'High number of server connections are currently in the Process of logging in', 'dashboard': 'https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/b65a429f-88e5-4da8-8f8a-98783f24c436/pgbouncer'}",The total number of server login connections for PGBouncer pools is higher than 500 in at least one region.
CviRuleGroup,eastus,Cloud Volumes Infrastructure Alerts,,increase(qstack_storage_http_requests_total_user_errors[1m]) >= 1,"{'severity': 'critical', 'metric': 'qstack_storage_http_requests_total_user_errors', 'service': 'cloud_volumes_infrastructure', 'pod': 'cvi'}",StorageHttpRequestError,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert StorageHttpRequestError has fired on #$.labels.region# | #$.labels.pod# ', 'IcM.Description': 'There was an increase of HTTP errors for #$.labels.pod#, The UI is incorrectly interfacing with the admin objects or someone is attempting to configure SDE-Admin objects without the use of the UI'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'StorageHttpRequestError', 'description': 'There was an increase of HTTP errors for #$.labels.pod#, The UI is incorrectly interfacing with the admin objects or someone is attempting to configure SDE-Admin objects without the use of the UI', 'summary': 'The UI is incorrectly interfacing with the admin objects or someone is attempting to configure SDE-Admin objects without the use of the UI', 'dashboard': 'to-be-added'}",There has been at least one increase in the total number of HTTP request errors from users within the last minute.
CviRuleGroup,eastus,Cloud Volumes Infrastructure Alerts,,"sum by (region) (rate(http_requests_total{job='cloud-volumes-infrastructure',code!~'2..|4..'}[10m])) > 0.1","{'severity': 'critical', 'metric': 'http_requests_total', 'service': 'cloud_volumes_infrastructure', 'pod': 'cvi'}",CVIHttp5XXErrorCritical,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CVIHttp5XXErrorCritical has fired on #$.labels.region# | #$.labels.pod# ', 'IcM.Description': 'There were #$.labels.value# HTTP 5XX errors for in CVI in the last 10 minutes, please investigate', 'IcM.TsgId': 'NA'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CVIHttp5XXErrorCritical', 'description': 'There were #$.labels.value# HTTP 5XX errors for in CVI in the last 10 minutes, please investigate', 'summary': 'High amount of CVI HTTP 5XX Failures', 'dashboard': 'to-be-added'}","Trigger an alert when the average rate of non-200 and non-400 HTTP requests per minute from the cloud-volumes-infrastructure job exceeds 0.1 requests per second over a 10-minute period, grouped by region."
CvetRuleGroup,eastus,CVEL Alerts,,increase(database_conflict[1h]) > 5,"{'severity': 'critical', 'metric': 'cvel_database_conflict', 'service': 'cvel', 'pod': 'cvel'}",CvelDatabaseConflict,True,1.0,PT15M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'CvelDatabaseConflict - {{ $labels.service }} - {{ $labels.pod }}', 'IcM.Description': 'CVEL Database conflict detected'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CvelDatabaseConflict', 'description': 'CVEL Database conflict detected', 'summary': 'The cvel_database_conflict metric has been greater than 5 for more than 1 hour.', 'dashboard': 'to-be-added'}",There has been an increase of more than 5 database conflicts within the last hour.
SdsSloRuleGroup,eastus,SLO Alerts on SDS,,"(sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", code=~""5.."", url=""/v1/hosts/<uuid>"", method=""get""}[1h])) by (region) / sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", url=""/v1/hosts/<uuid>"", method=""get""}[1h])) by (region)) > (14.4 * 0.0001) and (sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", code=~""5..""}[5m])) by (region) / sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", url=""/v1/hosts/<uuid>"", method=""get""}[5m])) by (region)) > (14.4 * 0.0001)","{'metric': 'SDS_http_requests_total', 'service': 'SDS'}",SDSServerErrorRateOnGetHost,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert SDSServerErrorRateOnGetHost has fired on #$.labels.region#', 'IcM.Description': 'SDS is experiencing high server error rate while getting Host, please investigate', 'IcM.Tags': 'anf-atom-blocking-icm'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'SDS is experiencing high server error rate while getting Host, please investigate'}","""Trigger an alert when, for each region, the proportion of HTTP GET requests to '/v1/hosts/<uuid>' that return a 5xx error code is more than 0.00144 (or 0.144%) over the last hour, and also when the proportion of all 5xx errors across all endpoints is more than 0.00144 over the last 5 minutes."""
SdsSloRuleGroup,eastus,SLO Alerts on SDS,,"(sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", code=~""5.."", url=""/v1/hosts/<uuid>"", method=""put""}[1h])) by (region) / sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", url=""/v1/hosts/<uuid>"", method=""put""}[1h])) by (region)) > (14.4 * 0.0001) and (sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", code=~""5..""}[5m])) by (region) / sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", url=""/v1/hosts/<uuid>"", method=""put""}[5m])) by (region)) > (14.4 * 0.0001)","{'metric': 'SDS_http_requests_total', 'service': 'SDS'}",SDSServerErrorRateOnCreateHost,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert SDSServerErrorRateOnCreateHost has fired on #$.labels.region#', 'IcM.Description': 'SDS is experiencing high server error rate while creating Host, please investigate', 'IcM.Tags': 'anf-atom-blocking-icm'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'SDS is experiencing high server error rate while creating Host, please investigate'}","""Trigger an alert when, for each region, the proportion of HTTP requests to '/v1/hosts/<uuid>' with a 'PUT' method and a 5xx status code over the last hour exceeds 0.144% of all 'PUT' requests to '/v1/hosts/<uuid>', AND the proportion of all 5xx status code requests over the last 5 minutes exceeds 0.144% of all 'PUT' requests to '/v1/hosts/<uuid>'""."
SdsSloRuleGroup,eastus,SLO Alerts on SDS,,"(sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", code=~""5.."", url=""/v1/hosts/<uuid>"", method=""delete""}[1h])) by (region) / sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", url=""/v1/hosts/<uuid>"", method=""delete""}[1h])) by (region)) > (14.4 * 0.0001) and (sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", code=~""5..""}[5m])) by (region) / sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", url=""/v1/hosts/<uuid>"", method=""delete""}[5m])) by (region)) > (14.4 * 0.0001)","{'metric': 'SDS_http_requests_total', 'service': 'SDS'}",SDSServerErrorRateOnDeleteHost,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert SDSServerErrorRateOnDeleteHost has fired on #$.labels.region#', 'IcM.Description': 'SDS is experiencing high server error rate while deleting Host, please investigate', 'IcM.Tags': 'anf-atom-blocking-icm'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'SDS is experiencing high server error rate while creating Host, please investigate'}","""Trigger an alert when, for each region, the ratio of HTTP delete requests to a specific URL that return 5xx status codes to all HTTP delete requests to that URL exceeds 0.00144 over the last hour, and also when the ratio of all 5xx status code requests to all HTTP delete requests to that URL exceeds 0.00144 over the last 5 minutes."""
SdsSloRuleGroup,eastus,SLO Alerts on SDS,,"(sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", code=~""5.."", url=""/v1/hosts/<uuid>"", method=""post""}[1h])) by (region) / sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", url=""/v1/hosts/<uuid>"", method=""post""}[1h])) by (region)) > (14.4 * 0.0001) and (sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", code=~""5..""}[5m])) by (region) / sum(rate(SDS_http_requests_total{job=""cloud-volumes-sds"", url=""/v1/hosts/<uuid>"", method=""post""}[5m])) by (region)) > (14.4 * 0.0001)","{'metric': 'SDS_http_requests_total', 'service': 'SDS'}",SDSServerErrorRateOnUpdateHost,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert SDSServerErrorRateOnUpdateHost has fired on #$.labels.region#', 'IcM.Description': 'SDS is experiencing high server error rate while updating Host, please investigate', 'IcM.Tags': 'anf-atom-blocking-icm'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'SDS is experiencing high server error rate while updating Host, please investigate'}","""Trigger an alert when, for each region, the proportion of HTTP POST requests to '/v1/hosts/<uuid>' that return a 5xx error code is more than 0.00144 (or 0.144%) over the last hour, and also when the proportion of all 5xx errors across all endpoints is more than 0.00144 over the last 5 minutes."""
SdsSloRuleGroup,eastus,SLO Alerts on SDS,,"histogram_quantile(0.90, rate(SDS_http_request_duration_seconds_bucket{job=""cloud-volumes-sds"",method=""get"",url=""/v1/hosts/<uuid>""}[5m])) > 60","{'metric': 'SDS_http_request_duration_seconds_bucket', 'service': 'SDS'}",HighP90LatenyOnGetHost,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnGetHost has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on Get Host operation is above 1 minute, please investigate', 'IcM.Tags': 'anf-atom-blocking-icm'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on Get Host operation is above 1 minute, please investigate'}","""Trigger an alert when the 90th percentile of the time it takes to handle GET requests to /v1/hosts/<uuid> on cloud-volumes-sds is greater than 60 seconds over a 5-minute period."""
SdsSloRuleGroup,eastus,SLO Alerts on SDS,,"histogram_quantile(0.90, rate(SDS_http_request_duration_seconds_bucket{job=""cloud-volumes-sds"",method=""put"",url=""/v1/hosts/<uuid>""}[5m])) > 60","{'metric': 'SDS_http_request_duration_seconds_bucket', 'service': 'SDS'}",HighP90LatenyOnCreateHost,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnCreateHost has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on Create Host operation is above 1 minute, please investigate', 'IcM.Tags': 'anf-atom-blocking-icm'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on Create Host operation is above 1 minute, please investigate'}",The 90th percentile of the average time it takes to process 'PUT' requests to '/v1/hosts/<uuid>' on the cloud-volumes-sds job is greater than 60 seconds over a 5-minute period.
SdsSloRuleGroup,eastus,SLO Alerts on SDS,,"histogram_quantile(0.90, rate(SDS_http_request_duration_seconds_bucket{job=""cloud-volumes-sds"",method=""delete"",url=""/v1/hosts/<uuid>""}[5m])) > 60","{'metric': 'SDS_http_request_duration_seconds_bucket', 'service': 'SDS'}",HighP90LatenyOnDeleteHost,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnDeleteHost has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on Delete Host operation is above 1 minute, please investigate', 'IcM.Tags': 'anf-atom-blocking-icm'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on Delete Host operation is above 1 minute, please investigate'}",The 90th percentile of the average time it takes to delete a host on the cloud volumes SDS service over the last 5 minutes is greater than 60 seconds.
SdsSloRuleGroup,eastus,SLO Alerts on SDS,,"histogram_quantile(0.90, rate(SDS_http_request_duration_seconds_bucket{job=""cloud-volumes-sds"",method=""post"",url=""/v1/hosts/<uuid>""}[5m])) > 60","{'metric': 'SDS_http_request_duration_seconds_bucket', 'service': 'SDS'}",HighP90LatenyOnUpdateHost,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert HighP90LatenyOnUpdateHost has fired on #$.labels.region#', 'IcM.Description': 'P90 latency on Update Host operation is above 1 minute, please investigate', 'IcM.Tags': 'anf-atom-blocking-icm'}}]","{'autoResolved': False, 'timeToResolve': 'PT10M'}","{'description': 'P90 latency on Update Host operation is above 1 minute, please investigate'}","""Trigger an alert when the 90th percentile of the time it takes to handle POST requests to /v1/hosts/<uuid> on cloud-volumes-sds is greater than 60 seconds over a 5-minute period."""
CvelRuleGroup,eastus,CVEL Alerts,,sum by (region) (increase(cvel_database_conflict[1h])) > 5,"{'severity': '2', 'metric': 'cvel_database_conflict', 'service': 'cvel', 'pod': 'cvel'}",CvelDatabaseConflict,True,2.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvelDatabaseConflict has fired on #$.labels.region# | #$.labels.pod#', 'IcM.Description': 'CVEL Database conflict detected', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvel-prometheus-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CvelDatabaseConflict', 'description': 'CVEL Database conflict detected', 'summary': 'The cvel_database_conflict metric has been greater than 5 for more than 1 hour.', 'dashboard': 'NA'}",The total number of times a database conflict occurred in each region over the last hour is greater than 5.
CvelRuleGroup,eastus,CVEL Alerts,,sum by (region) (increase(cvel_message_handling_error[1d])) > 10,"{'severity': '3', 'metric': 'cvel_message_handling_error', 'service': 'cvel', 'pod': 'cvel'}",CvelMessageHandlingError,True,3.0,,"[{'actionGroupId': '/subscriptions/19998cf7-a1b7-4c69-8daf-1f026e397d66/resourcegroups/anf.automation-stage.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Stage', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert CvelMessageHandlingError has fired on #$.labels.region# | #$.labels.pod#', 'IcM.Description': 'CVEL Message handling error detected', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-storage-dev-mansah/azure-netapp-files/azure-netapp-files/runbooks/cvel-prometheus-alerts-tsg'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}","{'title': 'CvelMessageHandlingError', 'description': 'CVEL Message handling error detected', 'summary': 'The cvel_message_handling_error metric has been greater than 10 for more than 1 day.', 'dashboard': 'NA'}",The total number of times there was an error handling a CVEL message in each region over the last day is greater than 10.
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:cpu_headroom:ok,"clamp_min(clamp_max(headroom_cpu_optimal_point_utilization - ignoring(pod, pod_template_hash, container, aadpodidbinding, app, app_kubernetes_io_instance, app_kubernetes_io_managed_by, app_kubernetes_io_name, app_kubernetes_io_version, azure_workload_identity_use, cluster, cvs_netapp_com_application, cvs_netapp_com_hyperscaler, cvs_netapp_com_maintainers, helm_sh_chart, instance, job, kubernetes_namespace, kubernetes_pod_name) headroom_cpu_current_utilization + 5, 1), 0)",,,True,,,,,,"""The CPU headroom is considered okay if the current utilization plus a 5% buffer is below the optimal point utilization, with a value between 0 and 1, ignoring various Kubernetes pod and application labels."""
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:cpu_headroom:percent_good_30d,(sum_over_time(slo:cpu_headroom:ok[30d:]) / count_over_time(slo:cpu_headroom:ok[30d:])) * 100,,,True,,,,,,"""The percentage of time the CPU headroom was within the allowed limits over the last 30 days, calculated by summing up all the times it was okay and dividing by the total number of measurements, then multiplying by 100."""
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:nfs_giveups:ok,"max by(metric, node, ontap_cluster, stamp_name, region)(1 - max_over_time(clamp_min(clamp_max(rw_ctx_nfs_giveups, 1), 0)[5m:]))",,,True,,,,,,"The percentage of time that NFS operations did not give up (i.e., were successful) over the last 5 minutes, split by metric, node, cluster, stamp name, and region, with a maximum value of 100% (since it's 1 minus the max giveups)."
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:nfs_giveups:fail,"max by(node, ontap_cluster, stamp_name, region) (slo:nfs_giveups:ok == bool 0)",,,True,,,,,,"Show the maximum number of times NFS give-ups failed for each unique combination of node, cluster, stamp name, and region, but only when there was a failure (i.e., slo:nfs_giveups:ok is false)."
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:cifs_giveups:fail,"max by(node, ontap_cluster, stamp_name, region) (slo:cifs_giveups:ok == bool 0)",,,True,,,,,,"Show the maximum number of CIFS give-up failures for each node, cluster, stamp name, and region where a failure occurred (i.e., where slo:cifs_giveups:ok is False)."
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:nfs_giveups:percent_good_30d,(sum_over_time(slo:nfs_giveups:ok[30d:]) / count_over_time(slo:nfs_giveups:ok[30d:])) * 100,,,True,,,,,,"""Calculate the percentage of successful NFS give-ups over the last 30 days, by dividing the total number of successful give-ups in that time period by the total count of give-ups and multiplying by 100."""
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:cifs_giveups:ok,"max by(metric, node, ontap_cluster, stamp_name, region)(1 - max_over_time(clamp_min(clamp_max(rw_ctx_cifs_giveups, 1), 0)[5m:]))",,,True,,,,,,"The percentage of CIFS give-up requests that were successful over the last 5 minutes is above the threshold for each metric, node, cluster, stamp name, and region, where success is defined as having no more than 1 give-up request."
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:cifs_giveups:percent_good_30d,(sum_over_time(slo:cifs_giveups:ok[30d:]) / count_over_time(slo:cifs_giveups:ok[30d:])) * 100,,,True,,,,,,"""Calculate the percentage of successful CIFS give-ups over the last 30 days, by dividing the total number of successful give-ups in that time period by the total count of give-up attempts and multiplying by 100."""
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:nfs_giveups:volume_avg_latency:ok,"clamp_max(1 - (((volume_avg_latency{volume!~""vol0|_root""} >= 200000) > 0) * ((volume_total_ops{volume!~""vol0|_root""} > 20) > 0) * on(node, ontap_cluster, region) max by(node, ontap_cluster, region)(slo:nfs_giveups:fail)), 1)",,,True,,,,,,"The rule calculates the percentage of volumes that meet a certain performance threshold and have sufficient operations, and then subtracts the percentage of failing volumes from 100% to get the success rate. 

More specifically, it checks if:
- The average latency for a volume is less than 200 milliseconds.
- The total number of operations for a volume is more than 20.
If both conditions are met, the volume is considered successful. 

The rule then calculates the maximum failure rate across all nodes, clusters, and regions, and subtracts this from 1 (or 100%) to get the overall success rate, capping it at 1 (or 100%). The result is a metric that represents the Service Level Objective (SLO) for NFS give-ups based on volume average latency."
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:cifs_giveups:volume_avg_latency:ok,"clamp_max(1 - (((volume_avg_latency{volume!~""vol0|_root""} >= 200000) > 0) * ((volume_total_ops{volume!~""vol0|_root""} > 20) > 0) * on(node, ontap_cluster, region) max by(node, ontap_cluster, region)(slo:cifs_giveups:fail)), 1)",,,True,,,,,,"The rule calculates the percentage of volumes that meet a certain performance threshold and have sufficient operations, and then subtracts the percentage of failing volumes from 100% to get the success rate. 

More specifically, it checks if:
- The average latency for a volume is greater than or equal to 200 milliseconds.
- The total number of operations on a volume is more than 20.
If both conditions are met for any volume (excluding ""vol0"" and ""_root""), it's considered a failure.

The success rate is then calculated as 1 minus the maximum failure rate across all nodes, clusters, and regions. The result is capped at 1 (100% success)."
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:nfs_giveups_with_latency:ok,"clamp_min(clamp_max(((((volume_avg_latency <= 200000) > 0) or on(volume, svm, node, aggr, ontap_cluster, stamp_name, region) vector(0)) * (((volume_total_ops > 20) > 0) or on(volume, svm, node, aggr, ontap_cluster, stamp_name, region) vector(0)) * on(node, ontap_cluster, region) group_left() ((max by(node, ontap_cluster, region)(slo:nfs_giveups:ok)) or on(node, ontap_cluster, region) vector(1))),1),0)",,,True,,,,,,"The rule ""slo:nfs_giveups_with_latency:ok"" is triggered when all of the following conditions are met for a specific node, cluster, and region:

- The average latency of a volume is less than or equal to 200 milliseconds.
- The total operations on a volume are more than 20.
- There are no NFS give-ups (or at least one give-up) reported for that node, cluster, and region.

The result is clamped between 0 and 1, meaning it will be either 0 (false) or 1 (true), indicating whether the conditions are met. If any of these conditions are not met, the rule will not trigger."
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:cifs_giveups_with_latency:ok,"clamp_min(clamp_max(((((volume_avg_latency <= 200000) > 0) or on(volume, svm, node, aggr, ontap_cluster, stamp_name, region) vector(0)) * (((volume_total_ops > 20) > 0) or on(volume, svm, node, aggr, ontap_cluster, stamp_name, region) vector(0)) * on(node, ontap_cluster, region) group_left() ((max by(node, ontap_cluster, region)(slo:cifs_giveups:ok)) or on(node, ontap_cluster, region) vector(1))),1),0)",,,True,,,,,,"The rule ""slo:cifs_giveups_with_latency:ok"" is true when two conditions are met for a volume: 
1) The average latency of the volume is less than or equal to 200 milliseconds, and 
2) The total operations on the volume are more than 20. 
If either condition is not met, it defaults to being okay (returns 1). 
The result is then clamped between 0 and 1, meaning any value outside this range will be set to the nearest boundary (0 or 1), effectively ensuring the output is a binary ""okay"" status.
This rule also considers the SLO (Service Level Objective) for CIFS give-ups, which must be okay for the node, ontap cluster, and region. If no SLO data is available, it defaults to being okay (returns 1)."
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:nfs_giveups_with_latency:fail,"clamp_min(clamp_max(((((volume_avg_latency > 200000) > 0) or on(volume, svm, node, aggr, ontap_cluster, stamp_name, region) vector(0)) * (((volume_total_ops > 20) > 0) or on(volume, svm, node, aggr, ontap_cluster, stamp_name, region) vector(0)) * on(node, ontap_cluster, region) group_left() ((max by(node, ontap_cluster, region)(slo:nfs_giveups:ok)) or on(node, ontap_cluster, region) vector(0))),1),0)",,,True,,,,,,"Trigger an alert for 'nfs giveups with latency failure' when the average latency of a volume is greater than 200 milliseconds and the total operations are more than 20, considering the SLO for NFS giveups is not met for any node in the same cluster and region, unless no data is available."
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:cifs_giveups_with_latency:fail,"clamp_min(clamp_max(((((volume_avg_latency > 200000) > 0) or on(volume, svm, node, aggr, ontap_cluster, stamp_name, region) vector(0)) * (((volume_total_ops > 20) > 0) or on(volume, svm, node, aggr, ontap_cluster, stamp_name, region) vector(0)) * on(node, ontap_cluster, region) group_left() ((max by(node, ontap_cluster, region)(slo:cifs_giveups:ok)) or on(node, ontap_cluster, region) vector(0))),1),0)",,,True,,,,,,"Trigger an alert for 'cifs_giveups_with_latency:fail' when the average latency of a volume is greater than 200 milliseconds and the total operations are more than 20, considering the SLO for CIFS give-ups is not met for any node in the same cluster and region."
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:nfs_giveups_with_latency:availability,sum_over_time(slo:nfs_giveups_with_latency:ok[1h]) / (sum_over_time(slo:nfs_giveups_with_latency:ok[1h]) + sum_over_time(slo:nfs_giveups_with_latency:fail[1h])),,,True,,,,,,"""The percentage of successful NFS requests with acceptable latency over the last hour, calculated by dividing the number of successful requests by the total number of requests (both successful and failed) within that time frame."""
SloRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,slo:cifs_giveups_with_latency:availability,sum_over_time(slo:cifs_giveups_with_latency:ok[1h]) / (sum_over_time(slo:cifs_giveups_with_latency:ok[1h]) + sum_over_time(slo:cifs_giveups_with_latency:fail[1h])),,,True,,,,,,"""The percentage of successful CIFS operations with acceptable latency over the last hour, calculated by dividing the number of successful operations by the total number of operations (both successful and failed) within that time frame."""
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_total_latency_bucket,"sum by (ontap_cluster,node,le) (rate(node_total_latency[1m]))",,,True,,,,,,"Show the total latency for each storage cluster and node over the last minute, grouped by the cluster, node, and latency limit."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_total_latency_bucket,"sum by (ontap_cluster,node,le) (rate(node_total_latency[1m]) and node_total_latency < 100000)",{'le': '100000'},,True,,,,,,"Show the total latency for each storage cluster node over the last minute, but only if the latency is less than 100 seconds, grouped by cluster and node."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_total_latency_bucket,"sum by (ontap_cluster,node,le) (rate(node_total_latency[1m]) and node_total_latency < 200000)",{'le': '200000'},,True,,,,,,"Show the total latency for each storage cluster node over the last minute, but only if the latency is less than 200 milliseconds, grouped by cluster and node."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_total_latency_bucket,"sum by (ontap_cluster,node,le) (rate(node_total_latency[1m]) and node_total_latency < 300000)",{'le': '300000'},,True,,,,,,"Show the total latency for each storage cluster node over the last minute, but only if the latency is less than 5 minutes (300,000 milliseconds), and group the results by cluster and node."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_total_latency_bucket,"sum by (ontap_cluster,node,le) (rate(node_total_latency[1m]) and node_total_latency < 400000)",{'le': '400000'},,True,,,,,,"Show the total latency for each storage cluster node over the last minute, but only if the latency is less than 400 milliseconds, grouped by cluster and node."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_total_latency_bucket,"sum by (ontap_cluster,node,le) (rate(node_total_latency[1m]) and node_total_latency < 500000)",{'le': '500000'},,True,,,,,,"Show the total latency for each storage cluster node over the last minute, but only if the total latency is less than 500 milliseconds, grouped by cluster and node."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_total_latency_bucket,"sum by (ontap_cluster,node,le) (rate(node_total_latency[1m]) and node_total_latency >= 500000)",{'le': '+Inf'},,True,,,,,,"Show the total latency for each storage cluster node over the last minute, but only if the total latency is greater than or equal to 500 milliseconds, and group the results by cluster, node, and latency threshold."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_throughput_bucket,"sum(rate(node_total_data[1m])) by (ontap_cluster,node) <= 1000",{'le': '1000'},,True,,,,,,The total data throughput for a specific NetApp ONTAP cluster and node is less than or equal to 1000 over a 1-minute period.
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_throughput_bucket,"sum(rate(node_total_data[1m])) by (ontap_cluster,node) > 1000 and sum(rate(node_total_data[1m])) by (ontap_cluster,node) <= 10000",{'le': '10000'},,True,,,,,,"The total data throughput for a node in an ONTAP cluster is greater than 1000 but less than or equal to 10,000 over a 1-minute period."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_throughput_bucket,"sum(rate(node_total_data[1m])) by (ontap_cluster,node) > 10000 and sum(rate(node_total_data[1m])) by (ontap_cluster,node) <= 100000",{'le': '100000'},,True,,,,,,"The total data throughput for a node in an ONTAP cluster is greater than 10,000 but less than or equal to 100,000 over a 1-minute period."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,node_throughput_bucket,"sum(rate(node_total_data[1m])) by (ontap_cluster,node) > 100000",{'le': '+Inf'},,True,,,,,,"The total data throughput for a specific NetApp ONTAP cluster and node is exceeding 100,000 over a 1-minute period."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,volume_qos_latency_bucket,"sum by (ontap_cluster,volume,le) (rate(qos_latency[1m]))",,,True,,,,,,"Show the total rate of QOS latency for each combination of ONTAP cluster and volume over the last minute, grouped by the cluster, volume, and latency limit (le)."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,volume_qos_latency_bucket,"sum by (ontap_cluster,volume,le) (rate(qos_latency[1m]) and qos_latency < 100000)",{'le': '100000'},,True,,,,,,"Show the total rate of QOS latency for each ONTAP cluster and volume over the last minute, but only include values where the latency is less than 100 milliseconds, grouped by cluster, volume, and latency limit (le)."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,volume_qos_latency_bucket,"sum by (ontap_cluster,volume,le) (rate(qos_latency[1m]) and qos_latency < 200000)",{'le': '200000'},,True,,,,,,"Show the total rate of QOS latency for each ONTAP cluster and volume over the last minute, but only include values where the latency is less than 200 milliseconds, grouped by cluster, volume, and latency limit (le)."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,volume_qos_latency_bucket,"sum by (ontap_cluster,volume,le) (rate(qos_latency[1m]) and qos_latency < 300000)",{'le': '300000'},,True,,,,,,"Show the total rate of QOS latency for each ONTAP cluster and volume over the last minute, but only include values where the latency is less than 300 milliseconds, grouped by cluster, volume, and latency limit (le)."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,volume_qos_latency_bucket,"sum by (ontap_cluster,volume,le) (rate(qos_latency[1m]) and qos_latency < 400000)",{'le': '400000'},,True,,,,,,"Show the total rate of QOS latency for each ONTAP cluster and volume over the last minute, but only include values where the latency is less than 400 milliseconds, grouped by cluster, volume, and latency limit (le)."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,volume_qos_latency_bucket,"sum by (ontap_cluster,volume,le) (rate(qos_latency[1m]) and qos_latency < 500000)",{'le': '500000'},,True,,,,,,"Show the total rate of QOS latency for each ONTAP cluster and volume over the last minute, but only include values where the latency is less than 500 milliseconds, grouped by cluster, volume, and latency limit (le)."
SloLatencyRecordingRulesGroup,eastus,SLO recording rules for ANF data plane,volume_qos_latency_bucket,"sum by (ontap_cluster,volume,le) (rate(qos_latency[1m]) and qos_latency >= 500000)",{'le': '+Inf'},,True,,,,,,"Show the total rate of QOS latency for each ONTAP cluster and volume over the last minute, but only if the latency is greater than or equal to 500 milliseconds, grouped by cluster, volume, and latency limit."
RPRuleGroup,eastus,Resource Provider alerts,,increase(rp_cvs_unavailable_total[5m]) > 3,"{'metric': 'rp_cvs_unavailable_total', 'service': 'anf-rp'}",RPCVSUnavailable,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP is unable to communicate with CVS in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is unable to communicate with CVS in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is unable to communicate with CVS in region #$.labels.region#'},There has been an increase of more than 3 instances of CVS being unavailable in the last 5 minutes.
RPRuleGroup,eastus,Resource Provider alerts,,increase(rp_cvs_unexpected_404_total[5m]) > 0,"{'metric': 'rp_cvs_unexpected_404_total', 'service': 'anf-rp'}",RPUnexpected404,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP unexpected 404 encountered in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is receiving unexpected 404 not found errors in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is receiving unexpected 404 not found errors in region #$.labels.region#'},There has been an increase in the total number of unexpected 404 errors from the CVS repository within the last 5 minutes.
RPRuleGroup,eastus,Resource Provider alerts,,increase(rp_cvs_unexpected_error_state_total[5m]) > 0,"{'metric': 'rp_cvs_unexpected_error_state_total', 'service': 'anf-rp'}",RPUnexpectedErrorState,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP unexpected error state encountered in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is receiving unexpected unexpected errors in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is receiving unexpected errors in region #$.labels.region#'},There has been an increase in unexpected error states for CVS operations within the last 5 minutes.
RPRuleGroup,eastus,Resource Provider alerts,,increase(rp_cvs_internal_error_response_total[5m]) > 1,"{'metric': 'rp_cvs_internal_error_response_total', 'service': 'anf-rp'}",RPInternalErrors,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP internal errors encountered in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is encountering internal errors in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is encountering internal errors in region #$.labels.region#'},There has been more than one increase in the total number of internal error responses from CVS within the last 5 minutes.
RPRuleGroup,eastus,Resource Provider alerts,,increase(rp_failed_notify_AVS_servicelevel_change[10m]) > 0,"{'metric': 'rp_failed_notify_AVS_servicelevel_change', 'service': 'anf-rp'}",RPFailNotifyAVS,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP failing to notify AVS of service level change in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is failing to notify AVS of service level change in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is failing to notify AVS of service level change in region #$.labels.region#'},There has been an increase in the number of failed notifications for AVS service level changes over the last 10 minutes.
RPRuleGroup,eastus,Resource Provider alerts,,increase(rp_nrp_nic_bmt_cleanup_failed_job_count[10m]) > 0,"{'metric': 'rp_nrp_nic_bmt_cleanup_failed_job_count', 'service': 'anf-rp'}",RPNICCleanupFailed,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP nic cleanup failing in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is reporting failure trying to cleanup NIC for BMTs in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is reporting failure trying to cleanup NIC for BMTs in region #$.labels.region#'},There has been an increase in the number of failed BMT cleanup jobs for NICs in the last 10 minutes.
RPRuleGroup,eastus,Resource Provider alerts,,rp_nrp_nic_bmt_cleanup_unresolved_job_count > 20,"{'metric': 'rp_nrp_nic_bmt_cleanup_unresolved_job_count', 'service': 'anf-rp'}",RPUnresolvedNICCleanupGrowing,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP unresolved cleanups growing in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# unresolved cleanups growing in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# unresolved cleanups growing in region #$.labels.region#'},The number of unresolved cleanup jobs for NIC BMT is greater than 20.
RPRuleGroup,eastus,Resource Provider alerts,,increase(rp_lock_timeout_counter[10m]) > 5,"{'metric': 'rp_nrp_nic_bmt_cleanup_unresolved_job_count', 'service': 'anf-rp'}",RPLockTimeout,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP lock timeouts are occuring in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is encountering lock timeouts in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is encountering lock timeouts in region #$.labels.region#'},The number of times the database lock timed out in the last 10 minutes has increased by more than 5 occurrences.
RPRuleGroup,eastus,Resource Provider alerts,,increase(rp_unexepected_code_paths[10m]) > 0,"{'metric': 'rp_unexepected_code_paths', 'service': 'anf-rp'}",RPUnexpectedCodepaths,False,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP unexpected codepaths being reported in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is reporting unexpected code paths in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is reporting unexpected code paths in region #$.labels.region#'},There has been an increase in the number of unexpected code paths executed over the last 10 minutes.
RPRuleGroup,eastus,Resource Provider alerts,,"increase(rp_faulttolerance_job_processed_count{status=""Failed""}[10m]) > 0","{'metric': 'rp_faulttolerance_job_processed_count', 'service': 'anf-rp'}",RPUnableToCommunicateWithCVS,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP jobs are failing in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is reporting failing jobs in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is reporting failing jobs in region #$.labels.region#'},There has been an increase in the number of failed job processes within the last 10 minutes.
RPRuleGroup,eastus,Resource Provider alerts,,increase(rp_tor_down[10m]) > 0,"{'metric': 'rp_tor_down', 'service': 'anf-rp'}",RPTORDown,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP TOR down in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is reporting a TOR down in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is reporting a TOR down in region #$.labels.region#'},There has been an increase in the number of times the Tor connection went down in the last 10 minutes.
RPRuleGroup,eastus,Resource Provider alerts,,"rate(rp_external_rp_request_reponses_bucket{le=""+Inf""}[5m]) < 1","{'metric': 'rp_external_rp_request_reponses_bucket', 'service': 'anf-rp'}",RPUnableToCommunicateWithCVS,True,4.0,,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'RP unable to communicate externally in region #$.labels.region#', 'IcM.Description': 'RP pod #$.labels.pod# is unable to communicate externally in region #$.labels.region#'}}]","{'autoResolved': True, 'timeToResolve': 'PT10M'}",{'description': 'RP pod #$.labels.pod# is unable to communicate externally in region #$.labels.region#'},The average rate of external RP requests that received a response over the last 5 minutes is less than 1 request per second.
ONTAPRuleGroup-SRE,eastus,ONTAP alerts,,(count(node_failed_power > 0) by (ontap_cluster)) > 2,"{'metric': 'node_failed_power', 'service': 'Ontap'}",OntapPSUFailure,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert OntapPSUFailure Multiple PSU failures on #$.labels.ontap_cluster# has fired in #$.labels.region#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/storage-node-psu-failure', 'IcM.Description': ""Harvest:Multiple nodes with PSU failure in cluster #$.labels.ontap_cluster# has fired on #$.labels.region#. HarvestDashboard: https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/cduqgwkkaidq8f/ontap3a-power?orgId=1&var-ds=',#$.labels.region#,'&var-Datacenter=All&var-Cluster='#$.labels.ontap_cluster#'&var-TopResources=5&from=now-1h&to=now""}}]","{'autoResolved': True, 'timeToResolve': 'PT1M'}",{'description': 'Ontap:Multiple nodes with PSU failure on cluster #$.labels.ontap_cluster# has fired in #$.labels.region#'},Alert when more than 2 nodes in the same ONTAP cluster have reported a power failure.
ONTAP2RuleGroup,eastus,ONTAP alerts,,(count(node_failed_power > 0) by (ontap_cluster)) > 2,"{'metric': 'node_failed_power', 'service': 'Ontap'}",OntapPSUFailure,True,4.0,PT10M,"[{'actionGroupId': '/subscriptions/2f495c46-73b1-463c-ae90-dae28e3880ef/resourcegroups/anf.automation.rg/providers/microsoft.insights/actionGroups/Azure-Monitor-IcM-Atlas', 'actionProperties': {'IcM.Title': 'Azure Monitor Alert OntapPSUFailure Multiple PSU failures on #$.labels.ontap_cluster# has fired in #$.labels.region#', 'IcM.TsgId': 'https://eng.ms/docs/cloud-ai-platform/azure-core/azure-storage/azure-file-storage/azure-netapp-files/azure-netapp-files/runbooks/storage-node-psu-failure', 'IcM.Description': ""Harvest:Multiple nodes with PSU failure in cluster #$.labels.ontap_cluster# has fired on #$.labels.region#. HarvestDashboard: https://anftelemetry-asapfja3btgvhhff.scus.grafana.azure.com/d/cduqgwkkaidq8f/ontap3a-power?orgId=1&var-ds=',#$.labels.region#,'&var-Datacenter=All&var-Cluster='#$.labels.ontap_cluster#'&var-TopResources=5&from=now-1h&to=now""}}]","{'autoResolved': True, 'timeToResolve': 'PT1M'}",{'description': 'Ontap:Multiple nodes with PSU failure on cluster #$.labels.ontap_cluster# has fired in #$.labels.region#'},Alert when more than 2 nodes in the same ONTAP cluster have reported a power failure.
