{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the llm proxy\n",
    "# import os\n",
    "# # pem_path = \"/opt/homebrew/etc/openssl@3/certs/../cert.pem\"\n",
    "# pem_path = \"/opt/homebrew/etc/openssl@3/cert.pem\"\n",
    "# # Ensure the environment variables are set before making the API call\n",
    "# os.environ['REQUESTS_CA_BUNDLE'] = pem_path\n",
    "# os.environ['SSL_CERT_FILE'] = pem_path\n",
    "\n",
    "# # Proceed with the API call\n",
    "# import getpass\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url = os.getenv('proxyllmendpoint'),\n",
    "#     api_key  = os.getenv('proxyllmuserkey'), \n",
    "# )\n",
    "# completion =  client.chat.completions.create(\n",
    "#                     model    = \"gpt-4o-mini\",\n",
    "#                     messages = [{ \"role\"   : \"user\",\n",
    "#                                   \"content\": \"Write a function that prints n primes in python\"}],\n",
    "#                     user     = getpass.getuser() # DO NOT HARDCODE A USER HERE\n",
    "#                 )\n",
    "\n",
    "# import os\n",
    "# import getpass\n",
    "# pem_path = \"/opt/homebrew/etc/openssl@3/certs/../cert.pem\"\n",
    "# # Ensure the environment variables are set before making the API call\n",
    "# os.environ['REQUESTS_CA_BUNDLE'] = pem_path\n",
    "# # os.environ['SSL_CERT_FILE']      = pem_path\n",
    "\n",
    "# # Proceed with the API call\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url = os.getenv('proxyllmendpoint'),\n",
    "#     api_key  = os.getenv('proxyllmappkey'), # DO NOT HARDCODE YOUR KEY\n",
    "# )\n",
    "# completion =  client.chat.completions.create(\n",
    "#                     model    = \"gpt-4o-mini\",\n",
    "#                     messages = [{ \"role\"   : \"user\",\n",
    "#                                   \"content\": \"Write a function that prints n primes in python\"}],\n",
    "#                     # user     = \"prragenticworkflow\" # DO NOT HARDCODE A USER HERE\n",
    "#                     user = getpass.getuser()\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhilred/Library/Python/3.13/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass # to get the current user\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain.chains import ConversationChain\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from typing import Optional, TypedDict, Annotated\n",
    "import operator\n",
    "from semantic_router.utils.function_call import FunctionSchema\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining file read tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_python_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"Error: File not found.\"\n",
    "    \n",
    "class codepath(BaseModel):\n",
    "  path: str = Field(description=\"code path to execute\")\n",
    "\n",
    "@tool(args_schema = codepath)\n",
    "def execute_query(path: str) -> str:\n",
    "  \"\"\"Returns the result of code path execution\"\"\"\n",
    "  return read_python_file(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "   messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'read_python_file',\n",
       "  'description': 'None',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'file_path': {'description': None, 'type': 'object'},\n",
       "    'description': None},\n",
       "   'required': []}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create the function calling schema for ollama\n",
    "execute_query_schema = FunctionSchema(read_python_file).to_ollama()\n",
    "# execute_query_schema[\"function\"][\"parameters\"][\"properties\"][\"description\"] = None\n",
    "execute_query_schema[\"function\"][\"parameters\"][\"properties\"][\"description\"] = None\n",
    "execute_query_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:04:13 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST https://llm-proxy-api.ai.openeng.netapp.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 15:04:16 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST https://llm-proxy-api.ai.openeng.netapp.com/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "class codeAgent:\n",
    "  # initialising the object\n",
    "  def __init__(self, model, tools, system_prompt = \"\"):\n",
    "    self.system_prompt = system_prompt\n",
    "\n",
    "    # initialising graph with a state \n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    # adding nodes \n",
    "    graph.add_node(\"llm\", self.call_llm)\n",
    "    graph.add_node(\"function\", self.execute_function)\n",
    "    graph.add_conditional_edges(\n",
    "      \"llm\",\n",
    "      self.exists_function_calling,\n",
    "      {True: \"function\", False: END}\n",
    "    )\n",
    "    graph.add_edge(\"function\", \"llm\")\n",
    "\n",
    "    # setting starting point\n",
    "    graph.set_entry_point(\"llm\")\n",
    "\n",
    "    self.graph = graph.compile()\n",
    "    self.tools = {t.name: t for t in tools}\n",
    "    self.model = model.bind_tools(tools)\n",
    "\n",
    "  def call_llm(self, state: AgentState):\n",
    "    messages = state['messages']\n",
    "    # adding system prompt if it's defined\n",
    "    if self.system_prompt:\n",
    "        messages = [SystemMessage(content=self.system_prompt)] + messages\n",
    "\n",
    "    # calling LLM\n",
    "    message = self.model.invoke(messages)\n",
    "\n",
    "    return {'messages': [message]}\n",
    "  \n",
    "  def execute_function(self, state: AgentState):\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    results = []\n",
    "    for tool in tool_calls:\n",
    "      # checking whether tool name is correct\n",
    "      if not tool['name'] in self.tools:\n",
    "        # returning error to the agent \n",
    "        result = \"Error: There's no such tool, please, try again\" \n",
    "      else:\n",
    "        # getting result from the tool\n",
    "        result = self.tools[tool['name']].invoke(tool['args'])\n",
    "\n",
    "      results.append(\n",
    "        ToolMessage(\n",
    "          tool_call_id=tool['id'], \n",
    "          name=tool['name'], \n",
    "          content=str(result)\n",
    "        )\n",
    "    )\n",
    "    return {'messages': results}\n",
    "  \n",
    "  def exists_function_calling(self, state: AgentState):\n",
    "    result = state['messages'][-1]\n",
    "    return len(result.tool_calls) > 0\n",
    "\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "# model = ChatOllama(model=\"llama3.2:1b\")\n",
    "# # model = ChatOllama(model=\"codellama:latest\")\n",
    "\n",
    "# system prompt\n",
    "# prompt = '''You are a senior expert in reviewing python code. \n",
    "# So, you can help the team to review the code and provide feedback. \n",
    "# You are very accurate and take into account all the nuances in code.\n",
    "# Your goal is to provide the detailed documentation for any security issues in the code that will help users.'''\n",
    "\n",
    "# prompt = '''You are a senior expert in reviewing code for observability. The best one that exists.\n",
    "# Your goal is to analyze the code on the following questions \n",
    "# 1. Are there actionable alerts identified for the feature? Are there Runbooks for the actionable alerts? Do we have TSGs attached to the alert?\n",
    "# 2. Add metrics to monitor dependencies and exception handling on components, infrastructure and features so that SRE can create alerts to reduce TTD?\n",
    "# 3. Are there CorrelationIDs established in logs to derive error lineage across various components?\n",
    "# 4. Can the feature/service support changing log levels for troubleshooting purposes?\n",
    "# 5. Are there critical log lines that we need to get alerted upon?\n",
    "# Provide response in the format as follows: {question: response}\n",
    "# '''\n",
    "\n",
    "prompt = '''You are a senior expert in reviewing code for resiliency. The best one that exists.\n",
    "Your goal is to analyze the code on the following questions \n",
    "1. Can the service/feature sustain a single node/pod failure?\n",
    "2. Can the feature/service recover gracefully without requiring a restart?\n",
    "3. Are there conditions that will require a human intervention to start the pods? if so, what are they?\n",
    "4. What will be the customer impact(if any) due to lack of high availability?\n",
    "Provide response in the format as follows: {question: response}\n",
    "'''\n",
    "\n",
    "#  getting the required ssl certificates\n",
    "pem_path = \"/opt/homebrew/etc/openssl@3/cert.pem\"\n",
    "# Ensure the environment variables are set before making the API call\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = pem_path\n",
    "os.environ['SSL_CERT_FILE'] = pem_path\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model_name      = \"gpt-4o-mini\",\n",
    "                 openai_api_base = os.getenv('proxyllmendpoint'),\n",
    "                 openai_api_key  = os.getenv('proxyllmuserkey'),\n",
    "                 model_kwargs    = {'user': getpass.getuser() })\n",
    "\n",
    "doc_agent = codeAgent(model, [execute_query], system_prompt=prompt)\n",
    "\n",
    "messages = [HumanMessage(content=\"the code is in the path '/Users/akhilred/Desktop/Billing Pyton Script/Billing_Usage_Extraction.py'. Analyze the code for resiliency requirements mentioned in the prompt\")]\n",
    "result = doc_agent.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"1\": \"The service can sustain a single node/pod failure to some extent, as it utilizes Azure Kusto's querying capabilities which distribute load across multiple nodes. However, if the entire service instance fails (i.e., the instance where the script is running), or if there are issues with the authentication mechanism or data source configuration, data extraction will be halted.\",\n",
      "  \"2\": \"The feature is designed to recover gracefully in terms of handling exceptions during Kusto queries, specifically with try-except blocks in the get_data_for_usage_app_auth and get_data_for_usage_gov_app_auth functions. However, if the entire service or script is terminated, it would require a restart to resume functioning, as there is no implemented retry logic or fallback mechanisms in place.\",\n",
      "  \"3\": \"Conditions that will require human intervention include: \\n- Authentication errors where a user needs to input secrets manually. \\n- Kusto query failures as logged in the exceptions, which will require investigation of the conditions that caused the queries to fail. \\n- Issues related to path validation for downloading CSV files, as incorrect paths will hinder file creation.\",\n",
      "  \"4\": \"Customer impact due to lack of high availability may include data access delays or complete unavailability of the queried billing and usage data. This may disrupt operational processes that rely on timely data extraction for analytics and decision-making, potentially leading to financial and operational inefficiencies.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
