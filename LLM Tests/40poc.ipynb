{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhilred/Library/Python/3.13/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "import ollama\n",
    "\n",
    "from semantic_router.utils.function_call import FunctionSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_python_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"Error: File not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class codepath(BaseModel):\n",
    "  path: str = Field(description=\"code path to execute\")\n",
    "\n",
    "@tool(args_schema = codepath)\n",
    "def execute_query(path: str) -> str:\n",
    "  \"\"\"Returns the result of code path execution\"\"\"\n",
    "  return read_python_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# defining agent state\n",
    "class AgentState(TypedDict):\n",
    "   messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'read_python_file',\n",
       "  'description': 'None',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'file_path': {'description': None, 'type': 'object'},\n",
       "    'description': None},\n",
       "   'required': []}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create the function calling schema for ollama\n",
    "execute_query_schema = FunctionSchema(read_python_file).to_ollama()\n",
    "# execute_query_schema[\"function\"][\"parameters\"][\"properties\"][\"description\"] = None\n",
    "execute_query_schema[\"function\"][\"parameters\"][\"properties\"][\"description\"] = None\n",
    "execute_query_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 15:12:55 - azure.core.pipeline.policies.http_logging_policy - INFO - _universal.py:509 - on_request() - Request URL: 'https://agenticaieastu6274613629.openai.azure.com/openai/deployments/gpt-4o-mini/info?api-version=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'Accept': 'application/json'\n",
      "    'x-ms-client-request-id': '15d349cc-0a87-11f0-81d4-52304e248077'\n",
      "    'api-key': 'REDACTED'\n",
      "    'User-Agent': 'langchain-azure-ai azsdk-python-ai-inference/1.0.0b9 Python/3.13.0 (macOS-15.3.2-arm64-arm-64bit-Mach-O)'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n",
      "2025-03-26 15:12:55 - azure.core.pipeline.policies.http_logging_policy - INFO - _universal.py:545 - on_response() - Response status: 404\n",
      "Response headers:\n",
      "    'Content-Length': '56'\n",
      "    'Content-Type': 'application/json'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'Date': 'Wed, 26 Mar 2025 21:12:54 GMT'\n",
      "2025-03-26 15:12:55 - langchain_azure_ai.chat_models.inference - WARNING - inference.py:498 - initialize_client() - Endpoint 'https://agenticaieastu6274613629.openai.azure.com/openai/deployments/gpt-4o-mini' does not support model metadata retrieval. Unable to populate model attributes. If this endpoint supports multiple models, you may be forgetting to indicate `model_name` parameter.\n",
      "2025-03-26 15:12:55 - azure.core.pipeline.policies.http_logging_policy - INFO - _universal.py:506 - on_request() - Request URL: 'https://agenticaieastu6274613629.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '1288'\n",
      "    'Accept': 'application/json'\n",
      "    'x-ms-client-request-id': '16163e58-0a87-11f0-81d4-52304e248077'\n",
      "    'api-key': 'REDACTED'\n",
      "    'User-Agent': 'langchain-azure-ai azsdk-python-ai-inference/1.0.0b9 Python/3.13.0 (macOS-15.3.2-arm64-arm-64bit-Mach-O)'\n",
      "    'Authorization': 'REDACTED'\n",
      "A body is sent with the request\n",
      "2025-03-26 15:12:56 - azure.core.pipeline.policies.http_logging_policy - INFO - _universal.py:545 - on_response() - Response status: 200\n",
      "Response headers:\n",
      "    'Content-Length': '1104'\n",
      "    'Content-Type': 'application/json'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'x-ratelimit-remaining-requests': 'REDACTED'\n",
      "    'x-ratelimit-limit-requests': 'REDACTED'\n",
      "    'x-ratelimit-remaining-tokens': 'REDACTED'\n",
      "    'x-ratelimit-limit-tokens': 'REDACTED'\n",
      "    'cmp-upstream-response-duration': 'REDACTED'\n",
      "    'x-accel-buffering': 'REDACTED'\n",
      "    'x-aml-cluster': 'REDACTED'\n",
      "    'x-envoy-upstream-service-time': 'REDACTED'\n",
      "    'x-ms-rai-invoked': 'REDACTED'\n",
      "    'x-request-id': 'REDACTED'\n",
      "    'ms-azureml-model-time': 'REDACTED'\n",
      "    'x-ms-client-request-id': '16163e58-0a87-11f0-81d4-52304e248077'\n",
      "    'azureml-model-session': 'REDACTED'\n",
      "    'Date': 'Wed, 26 Mar 2025 21:12:55 GMT'\n",
      "2025-03-26 15:12:56 - azure.core.pipeline.policies.http_logging_policy - INFO - _universal.py:506 - on_request() - Request URL: 'https://agenticaieastu6274613629.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '23958'\n",
      "    'Accept': 'application/json'\n",
      "    'x-ms-client-request-id': '1689acd0-0a87-11f0-81d4-52304e248077'\n",
      "    'api-key': 'REDACTED'\n",
      "    'User-Agent': 'langchain-azure-ai azsdk-python-ai-inference/1.0.0b9 Python/3.13.0 (macOS-15.3.2-arm64-arm-64bit-Mach-O)'\n",
      "    'Authorization': 'REDACTED'\n",
      "A body is sent with the request\n",
      "2025-03-26 15:13:03 - azure.core.pipeline.policies.http_logging_policy - INFO - _universal.py:545 - on_response() - Response status: 200\n",
      "Response headers:\n",
      "    'Content-Length': '3841'\n",
      "    'Content-Type': 'application/json'\n",
      "    'apim-request-id': 'REDACTED'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'x-content-type-options': 'REDACTED'\n",
      "    'x-ms-region': 'REDACTED'\n",
      "    'x-ratelimit-remaining-requests': 'REDACTED'\n",
      "    'x-ratelimit-limit-requests': 'REDACTED'\n",
      "    'x-ratelimit-remaining-tokens': 'REDACTED'\n",
      "    'x-ratelimit-limit-tokens': 'REDACTED'\n",
      "    'cmp-upstream-response-duration': 'REDACTED'\n",
      "    'x-accel-buffering': 'REDACTED'\n",
      "    'x-aml-cluster': 'REDACTED'\n",
      "    'x-envoy-upstream-service-time': 'REDACTED'\n",
      "    'x-ms-rai-invoked': 'REDACTED'\n",
      "    'x-request-id': 'REDACTED'\n",
      "    'ms-azureml-model-time': 'REDACTED'\n",
      "    'x-ms-client-request-id': '1689acd0-0a87-11f0-81d4-52304e248077'\n",
      "    'azureml-model-session': 'REDACTED'\n",
      "    'Date': 'Wed, 26 Mar 2025 21:13:02 GMT'\n"
     ]
    }
   ],
   "source": [
    "class codeAgent:\n",
    "  # initialising the object\n",
    "  def __init__(self, model, tools, system_prompt = \"\"):\n",
    "    self.system_prompt = system_prompt\n",
    "\n",
    "    # initialising graph with a state \n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    # adding nodes \n",
    "    graph.add_node(\"llm\", self.call_llm)\n",
    "    graph.add_node(\"function\", self.execute_function)\n",
    "    graph.add_conditional_edges(\n",
    "      \"llm\",\n",
    "      self.exists_function_calling,\n",
    "      {True: \"function\", False: END}\n",
    "    )\n",
    "    graph.add_edge(\"function\", \"llm\")\n",
    "\n",
    "    # setting starting point\n",
    "    graph.set_entry_point(\"llm\")\n",
    "\n",
    "    self.graph = graph.compile()\n",
    "    self.tools = {t.name: t for t in tools}\n",
    "    self.model = model.bind_tools(tools)\n",
    "\n",
    "  def call_llm(self, state: AgentState):\n",
    "    messages = state['messages']\n",
    "    # adding system prompt if it's defined\n",
    "    if self.system_prompt:\n",
    "        messages = [SystemMessage(content=self.system_prompt)] + messages\n",
    "\n",
    "    # calling LLM\n",
    "    message = self.model.invoke(messages)\n",
    "\n",
    "    return {'messages': [message]}\n",
    "  \n",
    "  def execute_function(self, state: AgentState):\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    results = []\n",
    "    for tool in tool_calls:\n",
    "      # checking whether tool name is correct\n",
    "      if not tool['name'] in self.tools:\n",
    "        # returning error to the agent \n",
    "        result = \"Error: There's no such tool, please, try again\" \n",
    "      else:\n",
    "        # getting result from the tool\n",
    "        result = self.tools[tool['name']].invoke(tool['args'])\n",
    "\n",
    "      results.append(\n",
    "        ToolMessage(\n",
    "          tool_call_id=tool['id'], \n",
    "          name=tool['name'], \n",
    "          content=str(result)\n",
    "        )\n",
    "    )\n",
    "    return {'messages': results}\n",
    "  \n",
    "  def exists_function_calling(self, state: AgentState):\n",
    "    result = state['messages'][-1]\n",
    "    return len(result.tool_calls) > 0\n",
    "\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "# model = ChatOllama(model=\"llama3.2:1b\")\n",
    "# # model = ChatOllama(model=\"codellama:latest\")\n",
    "\n",
    "# system prompt\n",
    "# prompt = '''You are a senior expert in reviewing python code. \n",
    "# So, you can help the team to review the code and provide feedback. \n",
    "# You are very accurate and take into account all the nuances in code.\n",
    "# Your goal is to provide the detailed documentation for any security issues in the code that will help users.'''\n",
    "\n",
    "prompt = '''You are a senior expert in reviewing code for observability. The best one that exists.\n",
    "Your goal is to analyze the code on the following questions \n",
    "1. Are there actionable alerts identified for the feature? Are there Runbooks for the actionable alerts? Do we have TSGs attached to the alert?\n",
    "2. Add metrics to monitor dependencies and exception handling on components, infrastructure and features so that SRE can create alerts to reduce TTD?\n",
    "3. Are there CorrelationIDs established in logs to derive error lineage across various components?\n",
    "4. Can the feature/service support changing log levels for troubleshooting purposes?\n",
    "5. Are there critical log lines that we need to get alerted upon?\n",
    "Provide response in the format as follows: {question: response}\n",
    "'''\n",
    "\n",
    "import os\n",
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "\n",
    "model = AzureAIChatCompletionsModel(\n",
    "    endpoint= os.getenv('azureaifoundry4oendpoint'),\n",
    "    credential=os.getenv('azureaifoundry4okey'),\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "doc_agent = codeAgent(model, [execute_query], system_prompt=prompt)\n",
    "\n",
    "messages = [HumanMessage(content=\"the code is in the path '/Users/akhilred/Desktop/Billing Pyton Script/Billing_Usage_Extraction.py'. Analyze the code for observability requirements mentioned in the prompt\")]\n",
    "result = doc_agent.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the analysis of the provided code regarding the observability requirements:\n",
      "\n",
      "1. **Are there actionable alerts identified for the feature? Are there Runbooks for the actionable alerts? Do we have TSGs attached to the alert?**\n",
      "   - **Response:** The code does not include any built-in alerts, runbooks, or TSG (trouble shooting guides) frameworks. There is no logic in place to generate alerts related to failure conditions, such as unsuccessful data extraction or exceptions. It is recommended to implement appropriate monitoring and alerting mechanisms based on expected failure points.\n",
      "\n",
      "2. **Add metrics to monitor dependencies and exception handling on components, infrastructure and features so that SRE can create alerts to reduce TTD?**\n",
      "   - **Response:** The code currently lacks metrics that would allow monitoring of dependencies or exception handling. It only catches `KustoServiceError` exceptions in the data extraction methods. Adding logging for successes, failures, execution time, and request statistics would significantly enhance observability. Consider using a metrics service or logging framework to capture these metrics.\n",
      "\n",
      "3. **Are there CorrelationIDs established in logs to derive error lineage across various components?**\n",
      "   - **Response:** There is no use of CorrelationIDs in the logging mechanism to track the lineage of errors across components. Implementing CorrelationIDs in log statements would help trace the flow of requests and errors through the system, aiding debugging and enhancing traceability.\n",
      "\n",
      "4. **Can the feature/service support changing log levels for troubleshooting purposes?**\n",
      "   - **Response:** The code does not appear to support changing log levels dynamically. To facilitate troubleshooting, consider integrating a logging framework (like Python's built-in `logging` module), which allows for log level configuration (e.g., DEBUG, INFO, WARNING, ERROR) that can be adjusted at runtime.\n",
      "\n",
      "5. **Are there critical log lines that we need to get alerted upon?**\n",
      "   - **Response:** The code currently has no alerts set for critical log lines. While it does include exception handling, there is no logging of these exceptions nor any mechanism to alert on critical failures. It is advisable to add structured logs for critical actions and failure points, as well as set up alerts for these logs based on their severity.\n",
      "\n",
      "In summary, while the code performs its intended functionality, it lacks robust observability features. Implementing alerting, logging, metrics collection, and correlation tracking will greatly enhance the maintainability and reliability of the system.\n"
     ]
    }
   ],
   "source": [
    "print(result['messages'][3].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
