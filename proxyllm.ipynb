{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the llm proxy\n",
    "# import os\n",
    "# # pem_path = \"/opt/homebrew/etc/openssl@3/certs/../cert.pem\"\n",
    "# pem_path = \"/opt/homebrew/etc/openssl@3/cert.pem\"\n",
    "# # Ensure the environment variables are set before making the API call\n",
    "# os.environ['REQUESTS_CA_BUNDLE'] = pem_path\n",
    "# os.environ['SSL_CERT_FILE'] = pem_path\n",
    "\n",
    "# # Proceed with the API call\n",
    "# import getpass\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url = os.getenv('proxyllmendpoint'),\n",
    "#     api_key  = os.getenv('proxyllmuserkey'), \n",
    "# )\n",
    "# completion =  client.chat.completions.create(\n",
    "#                     model    = \"gpt-4o-mini\",\n",
    "#                     messages = [{ \"role\"   : \"user\",\n",
    "#                                   \"content\": \"Write a function that prints n primes in python\"}],\n",
    "#                     user     = getpass.getuser() # DO NOT HARDCODE A USER HERE\n",
    "#                 )\n",
    "\n",
    "# import os\n",
    "# import getpass\n",
    "# pem_path = \"/opt/homebrew/etc/openssl@3/certs/../cert.pem\"\n",
    "# # Ensure the environment variables are set before making the API call\n",
    "# os.environ['REQUESTS_CA_BUNDLE'] = pem_path\n",
    "# # os.environ['SSL_CERT_FILE']      = pem_path\n",
    "\n",
    "# # Proceed with the API call\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url = os.getenv('proxyllmendpoint'),\n",
    "#     api_key  = os.getenv('proxyllmappkey'), # DO NOT HARDCODE YOUR KEY\n",
    "# )\n",
    "# completion =  client.chat.completions.create(\n",
    "#                     model    = \"gpt-4o-mini\",\n",
    "#                     messages = [{ \"role\"   : \"user\",\n",
    "#                                   \"content\": \"Write a function that prints n primes in python\"}],\n",
    "#                     # user     = \"prragenticworkflow\" # DO NOT HARDCODE A USER HERE\n",
    "#                     user = getpass.getuser()\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhilred/Library/Python/3.13/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass # to get the current user\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain.chains import ConversationChain\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from typing import Optional, TypedDict, Annotated\n",
    "import operator\n",
    "from semantic_router.utils.function_call import FunctionSchema\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining file read tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_python_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"Error: File not found.\"\n",
    "    \n",
    "class codepath(BaseModel):\n",
    "  path: str = Field(description=\"code path to execute\")\n",
    "\n",
    "@tool(args_schema = codepath)\n",
    "def execute_query(path: str) -> str:\n",
    "  \"\"\"Returns the result of code path execution\"\"\"\n",
    "  return read_python_file(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "   messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'read_python_file',\n",
       "  'description': 'None',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'file_path': {'description': None, 'type': 'object'},\n",
       "    'description': None},\n",
       "   'required': []}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create the function calling schema for ollama\n",
    "execute_query_schema = FunctionSchema(read_python_file).to_ollama()\n",
    "# execute_query_schema[\"function\"][\"parameters\"][\"properties\"][\"description\"] = None\n",
    "execute_query_schema[\"function\"][\"parameters\"][\"properties\"][\"description\"] = None\n",
    "execute_query_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:28:21 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST https://llm-proxy-api.ai.openeng.netapp.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-28 16:28:26 - httpx - INFO - _client.py:1025 - _send_single_request() - HTTP Request: POST https://llm-proxy-api.ai.openeng.netapp.com/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "class codeAgent:\n",
    "  # initialising the object\n",
    "  def __init__(self, model, tools, system_prompt = \"\"):\n",
    "    self.system_prompt = system_prompt\n",
    "\n",
    "    # initialising graph with a state \n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    # adding nodes \n",
    "    graph.add_node(\"llm\", self.call_llm)\n",
    "    graph.add_node(\"function\", self.execute_function)\n",
    "    graph.add_conditional_edges(\n",
    "      \"llm\",\n",
    "      self.exists_function_calling,\n",
    "      {True: \"function\", False: END}\n",
    "    )\n",
    "    graph.add_edge(\"function\", \"llm\")\n",
    "\n",
    "    # setting starting point\n",
    "    graph.set_entry_point(\"llm\")\n",
    "\n",
    "    self.graph = graph.compile()\n",
    "    self.tools = {t.name: t for t in tools}\n",
    "    self.model = model.bind_tools(tools)\n",
    "\n",
    "  def call_llm(self, state: AgentState):\n",
    "    messages = state['messages']\n",
    "    # adding system prompt if it's defined\n",
    "    if self.system_prompt:\n",
    "        messages = [SystemMessage(content=self.system_prompt)] + messages\n",
    "\n",
    "    # calling LLM\n",
    "    message = self.model.invoke(messages)\n",
    "\n",
    "    return {'messages': [message]}\n",
    "  \n",
    "  def execute_function(self, state: AgentState):\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "\n",
    "    results = []\n",
    "    for tool in tool_calls:\n",
    "      # checking whether tool name is correct\n",
    "      if not tool['name'] in self.tools:\n",
    "        # returning error to the agent \n",
    "        result = \"Error: There's no such tool, please, try again\" \n",
    "      else:\n",
    "        # getting result from the tool\n",
    "        result = self.tools[tool['name']].invoke(tool['args'])\n",
    "\n",
    "      results.append(\n",
    "        ToolMessage(\n",
    "          tool_call_id=tool['id'], \n",
    "          name=tool['name'], \n",
    "          content=str(result)\n",
    "        )\n",
    "    )\n",
    "    return {'messages': results}\n",
    "  \n",
    "  def exists_function_calling(self, state: AgentState):\n",
    "    result = state['messages'][-1]\n",
    "    return len(result.tool_calls) > 0\n",
    "\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "# model = ChatOllama(model=\"llama3.2:1b\")\n",
    "# # model = ChatOllama(model=\"codellama:latest\")\n",
    "\n",
    "# system prompt\n",
    "# prompt = '''You are a senior expert in reviewing python code. \n",
    "# So, you can help the team to review the code and provide feedback. \n",
    "# You are very accurate and take into account all the nuances in code.\n",
    "# Your goal is to provide the detailed documentation for any security issues in the code that will help users.'''\n",
    "\n",
    "prompt = '''You are a senior expert in reviewing code for observability. The best one that exists.\n",
    "Your goal is to analyze the code on the following questions \n",
    "1. Are there actionable alerts identified for the feature? Are there Runbooks for the actionable alerts? Do we have TSGs attached to the alert?\n",
    "2. Add metrics to monitor dependencies and exception handling on components, infrastructure and features so that SRE can create alerts to reduce TTD?\n",
    "3. Are there CorrelationIDs established in logs to derive error lineage across various components?\n",
    "4. Can the feature/service support changing log levels for troubleshooting purposes?\n",
    "5. Are there critical log lines that we need to get alerted upon?\n",
    "Provide response in the format as follows: {question: response}\n",
    "'''\n",
    "\n",
    "#  getting the required ssl certificates\n",
    "pem_path = \"/opt/homebrew/etc/openssl@3/cert.pem\"\n",
    "# Ensure the environment variables are set before making the API call\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = pem_path\n",
    "os.environ['SSL_CERT_FILE'] = pem_path\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model_name      = \"gpt-4o-mini\",\n",
    "                 openai_api_base = os.getenv('proxyllmendpoint'),\n",
    "                 openai_api_key  = os.getenv('proxyllmuserkey'),\n",
    "                 model_kwargs    = {'user': getpass.getuser() })\n",
    "\n",
    "doc_agent = codeAgent(model, [execute_query], system_prompt=prompt)\n",
    "\n",
    "messages = [HumanMessage(content=\"the code is in the path '/Users/akhilred/Desktop/Billing Pyton Script/Billing_Usage_Extraction.py'. Analyze the code for observability requirements mentioned in the prompt\")]\n",
    "result = doc_agent.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the responses to the observability requirements based on the code provided:\n",
      "\n",
      "1. **Are there actionable alerts identified for the feature? Are there Runbooks for the actionable alerts? Do we have TSGs attached to the alert?**\n",
      "   - **response**: The provided code does not include any specific configuration for actionable alerts or Runbooks for handling such alerts. There are no TSGs (Troubleshooting Guides) mentioned in the code.\n",
      "\n",
      "2. **Add metrics to monitor dependencies and exception handling on components, infrastructure, and features so that SRE can create alerts to reduce TTD?**\n",
      "   - **response**: The code includes some exception handling via `try...except` blocks specifically around Kusto queries, but there are no metrics or monitoring dependencies included. Adding metrics for monitoring the performance of Kusto queries and their results would be beneficial for SRE to generate alerts.\n",
      "\n",
      "3. **Are there CorrelationIDs established in logs to derive error lineage across various components?**\n",
      "   - **response**: The code does not implement Correlation IDs for tracking requests or logging activities. Incorporating Correlation IDs would help trace errors back through various components, enabling better debugging.\n",
      "\n",
      "4. **Can the feature/service support changing log levels for troubleshooting purposes?**\n",
      "   - **response**: The code does not provide any mechanisms for adjusting log levels dynamically. Implementing a logging framework that supports different log levels (such as DEBUG, INFO, WARN, ERROR) that can be adjusted at runtime would enhance troubleshooting capabilities.\n",
      "\n",
      "5. **Are there critical log lines that we need to get alerted upon?**\n",
      "   - **response**: There are no specific critical log lines or alert conditions defined in the provided code. Setting up alerts for certain exceptions (like `KustoServiceError`) or monitoring significant events during data extraction would be essential to ensure observability.\n",
      "\n",
      "To enhance the observability of the feature, it is recommended to add structured logging, metrics for performance and health, and a more comprehensive error handling with alerts.\n"
     ]
    }
   ],
   "source": [
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
